{"title":"Week 5 Workbook","markdown":{"yaml":{"title":"Week 5 Workbook","author":"Emorie D Beck","format":{"html":{"code-tools":true,"code-copy":true,"code-line-numbers":true,"code-link":true,"theme":"united","highlight-style":"tango","df-print":"paged","code-fold":"show","toc":true,"toc-float":true,"self-contained":true}}},"headingText":"Week 5: Functions, Iteration, and `purrr`","containsRefs":false,"markdown":"\n\n```{r, echo = F}\noptions(htmltools.dir.version = FALSE\n        , knitr.kable.NA = \"\")\n```\n\n```{r, echo = T}\nlibrary(knitr)\nlibrary(psych)\nlibrary(lme4)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(kableExtra)\nlibrary(plyr)\nlibrary(tidyverse)\n```\n\n\n# Outline\n\n1.  Questions on Homework\n2.  Functions\n3.  `purrr`\n4.  Problem Set and Question Time\n\n# Functions\n\n-   A **`function`** or subroutine is a sequence of program instructions that performs a specific task, packaged as a unit.\n-   Functions typically take some input and return some specified output\n\n## Why Write Your Own Functions?\n\n-   Automate your workflow\n-   Prevent copy-paste errors (only need to update the function, not update the same task in a bunch of areas)\n-   Saves you time by providing tools that port to new projects\n-   Functions make you *think*, which improves the quality of our work\n\n## When Should You Write Your Own Functions?\n\n-   Hadley Wickham's rule of thumb (which is common in a lot of CS and DS circles) is that if you have to copy-paste something more than twice, you should write a function\n-   But really you can write one whenever you'd like\n-   I often write functions when a link of code starts getting because it's usually a good signal that I'm trying to do too much at once.\n\n## Types of Functions\n\n-   Vector functions take one or more vectors as input and return a vector as output.\n-   Data frame functions take a data frame as input and return a data frame as output.\n-   Plot functions that take a data frame as input and return a plot as output.\n-   and so on...\n-   We'll talk extensively about this again when we talk about building figures and tables into your workflow in Week 8\n\n## Vector Functions\n\n-   In my work, I often want to POMP (**P**ercentage **O**f **M**aximum **P**ossible) score or z-score (standardize) variables, which allows me to get data on a standard and interpretable scale (percentage or standard deviations)\n-   I often want to do this with lots of variables, so to do each one separately would take forever and introduce chances for error.\n\n```{r}\nbfi %>%\n  select(matches(\"1$\")) %>%\n  mutate(\n    E1 = (E1 - min(E1, na.rm = T))/(max(E1, na.rm = T) - min(E1, na.rm = T))*100\n    , A1 = (A1 - min(A1, na.rm = T))/(max(A1, na.rm = T) - min(A1, na.rm = T))*100\n    , C1 = (C1 - min(C1, na.rm = T))/(max(C1, na.rm = T) - min(C1, na.rm = T))*100\n    , N1 = (N1 - min(N1, na.rm = T))/(max(N1, na.rm = T) - min(N1, na.rm = T))*100\n    , O1 = (O1 - min(O1, na.rm = T))/(max(O1, na.rm = T) - min(O1, na.rm = T))*100\n  ) %>%\n  head(10)\n```\n\n-   Instead we could make this a function because I don't want to talk about how long it took me to do that copy-pasting ðŸ˜­\n\n```{r}\npomp <- function(x, na) (x - min(x, na.rm = na))/(max(x, na.rm = na) - min(x, na.rm = na))*100\nbfi %>%\n  select(matches(\"1$\")) %>%\n  mutate(\n    E1 = pomp(E1, T)\n    , A1 =  pomp(A1, T)\n    , C1 =  pomp(C1, T)\n    , N1 =  pomp(N1, T)\n    , O1 =  pomp(O1, T)\n  ) %>%\n  head(10)\n```\n\n-   And remember, we could simplify this even more with `mutate_at()`!\n-   And I know in this example it saved us about four lines of code, but having a function like this that you can just reference, especially in conjunction with `mutate_at()` saves you ***time*** in the short and long run.[^1]\n\n[^1]: Fun fact. I wrote this example and used POMP scores because I like to make them my personality and then noticed it's the same example used by R4DS\n\n```{r}\npomp <- function(x, na) (x - min(x, na.rm = na))/(max(x, na.rm = na) - min(x, na.rm = na))*100\nbfi %>%\n  select(matches(\"1$\")) %>%\n  mutate_at(vars(matches(\"1$\")), pomp, na = T) %>%\n  head(10)\n```\n\n## Writing Functions\n\n-   The first step in writing a function is to find the *pattern*:\n-   Let's look back at the code above:\n\n```{r, eval = F}\nE1 = (E1 - min(E1, na.rm = T))/(max(E1, na.rm = T) - min(E1, na.rm = T))*100\nA1 = (A1 - min(A1, na.rm = T))/(max(A1, na.rm = T) - min(A1, na.rm = T))*100\nC1 = (C1 - min(C1, na.rm = T))/(max(C1, na.rm = T) - min(C1, na.rm = T))*100\nN1 = (N1 - min(N1, na.rm = T))/(max(N1, na.rm = T) - min(N1, na.rm = T))*100\nO1 = (O1 - min(O1, na.rm = T))/(max(O1, na.rm = T) - min(O1, na.rm = T))*100\n```\n\n-   Do you see the pattern?\n\n```{r, eval = F}\n(â–ˆ - min(â–ˆ, na.rm = TRUE)) / (max(â–ˆ, na.rm = TRUE) - min(â–ˆ, na.rm = TRUE))\n```\n\nWe need three things to turn this into a function:\n\n1.  **A name.** Here we'll use rescale01 because this function rescales a vector to lie between 0 and 1.\n2.  **The arguments.** The arguments are things that vary across calls and our analysis above tells us that we have just one. We'll call it x because this is the conventional name for a numeric vector.\n3.  **The body.** The body is the code that's repeated across all the calls.\n\n-   The template of a function looks something like this:\n\n```{r, eval = F}\nname <- function(arguments){\n  body\n}\n```\n\n-   Although for local functions, I suggest something like this:\n\n```{r, eval = F}\nname <- function(\n    argument1 # description of first argument\n    , argument2 # description of second argument\n    ){\n  body\n}\n```\n\n-   That's not going to fit well on my slides, so I will minimally do it in class.\n\n-   Following this, we return to our function:\n\n```{r}\npomp <- function(x, na) {\n  (x - min(x, na.rm = na))/(max(x, na.rm = na) - min(x, na.rm = na))*100\n}\n```\n\n## Function Efficiency\n\n-   For most of our purposes, we may not care that much about how fast our code is\n-   Unless you're working with huge data, things typically run fast\n-   But with a function like POMP, I may need to run it on 100+ variables each with 10,000+ observations, so speed may be a consideration\n-   So how do you speed up your functions?\n    1.  Avoid computing something twice\n    2.  Try to anticipate where things could go wrong\n-   So in our function, we calculate `min()` twice and `min()` once\n-   We could just calculate `range()` instead, which reduces that computation time\n-   Watch (note you have to run this in your console to see the difference)!\n\n::: columns\n::: column\n```{r}\npomp <- function(x) {\n  (x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T))*100\n}\n\nstart <- Sys.time()\ntmp <- bfi %>%\n  select(matches(\"1$\")) %>%\n  mutate_at(vars(matches(\"1$\")), pomp)\n(diff1 <- Sys.time() - start)\n```\n:::\n:::\n\n::: column\n```{r}\npomp <- function(x) {\n  rng <- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])*100\n}\n\nstart <- Sys.time()\ntmp <- bfi %>%\n  select(matches(\"1$\")) %>%\n  mutate_at(vars(matches(\"1$\")), pomp)\n(diff2 <- Sys.time() - start)\n```\n:::\n\n::::\n\n-   But those differences are really minimal right? Just `r round(as.numeric(diff1 - diff2), 4)` seconds.\n-   But across 150 variables that's `r round(as.numeric(diff1 - diff2)*150, 4)` seconds.\n\n## Cautionary note on outputs\n\n-   The function we wrote above could be a \"mutate()\" function because the output is the same length / size as the input.\n-   If you write a function that drops cases, you will get an error because R can't make the resulting vector fit back into the data frame\n-   Or, if you write a function that returns length `1` within mutate, it will just repeat that value for every row\n-   So, just be careful to think about what the output is and make sure that it matches the use case you are working with!\n\n## More notes on outputs\n\n-   You can output anything you want from a function!\n    -   data frames\n    -   vectors\n    -   single values\n    -   strings\n    -   lists (very common for base R and package functions!!)\n    -   model objects\n    -   plots\n    -   tables\n    -   etc.!\n\n## You Try:\n\nTry turning the following into functions:\n\n```{r, eval = F}\nmean(is.na(x))\nmean(is.na(y))\nmean(is.na(z))\n\nx / sum(x, na.rm = TRUE)\ny / sum(y, na.rm = TRUE)\nz / sum(z, na.rm = TRUE)\n\nround(x / sum(x, na.rm = TRUE) * 100, 1)\nround(y / sum(y, na.rm = TRUE) * 100, 1)\nround(z / sum(z, na.rm = TRUE) * 100, 1)\n\n(x - mean(x, na.rm = T))/sd(x, na.rm = T)\n(y - mean(y, na.rm = T))/sd(y, na.rm = T)\n(z - mean(z, na.rm = T))/sd(z, na.rm = T)\n```\n\n```{r}\nprop_missing <- function(x) mean(is.na(x))\n\nprop_total   <- function(x) x / sum(x, na.rm = T)\n\nround_prop   <- function(x) round(prop_total(x)*100, 1)\n\nz_score      <- function(x) (x - mean(x, na.rm = T))/sd(x, na.rm = T)\n```\n\n## Data Frame Functions\n\n-   All the functions we've covered so far are vector functions (i.e. they input a vector, not a matrix or data frame and work well within `mutate()` calls)\n\n-   But if you have a long string of dplyr verbs, it can be useful to put these into a data frame function where you provide a data frame input and flexible way of naming the columns\n\n-   I'm going to give a brief example here, but suggest that you check out more in [*R for Data Science*](https://r4ds.hadley.nz/functions#data-frame-functions).\n\n-   Let's start with a brief example. To do it, let's first make the bfi data frame long\n\n```{r}\nbfi_long <- bfi %>%\n  rownames_to_column(\"SID\") %>%\n  pivot_longer(\n    cols = c(-SID, -education, -gender, -age)\n    , names_to = c(\"trait\", \"item\")\n    , names_sep = -1\n    , values_to = \"value\"\n    , values_drop_na = T\n  )\nhead(bfi_long, 8)\n```\n\n-   So maybe I want to get means for each of the Big Five from this, so I write a function like this:\n\n```{r, error=FALSE, eval=FALSE}\ngrouped_mean <- function(df, group_var, mean_var) {\n  df %>%\n    group_by(group_var) %>%\n    summarize(mean(mean_var))\n}\n\nbfi_long %>% grouped_mean(trait, value)\n```\n\n-   That didn't work because we can't specify grouping variables like that\n-   To get around it, we have to use something called **embracing**: We have to wrap the variables like this {{ trait }}\n-   This just nudges the dplyr functions to look inside the data frame for the column you specify\n\n```{r}\ntidy_describe <- function(df, var) {\n  df %>%\n    summarize(\n      mean   = mean({{ var }},   na.rm = TRUE),\n      sd     = sd({{ var }},     na.rm = TRUE),\n      median = median({{ var }}, na.rm = TRUE),\n      min    = min({{ var }},    na.rm = TRUE),\n      max    = max({{ var }},    na.rm = TRUE),\n      n      = n(),\n      n_miss = sum(is.na({{ var }})),\n      .groups = \"drop\"\n      )\n}\n\nbfi_long %>% \n  group_by(trait) %>%\n  tidy_describe(value)\n```\n\n-   Or remember when I had us getting counts and proportions for continuous x categorical relationships?\n\n```{r}\ncount_prop <- function(df, var, sort = FALSE) {\n  df %>%\n    count({{ var }}, sort = sort) %>%\n    mutate(prop = n / sum(n))\n}\n\nbfi_long %>% \n  count_prop(education)\n```\n\n## Wrap-Up\n\n-   This is just a brief introduction to functions.\n-   Functions are absolutely essential part of workflows, and you'll see them pop up in every lesson from here on out (and you already saw them pop up in previous lessons)\n-   As we continue to see them, I'll ramp up their complexity, showing you how to write functions for estimating models, model predictions, figures, tables, and more\n\n# Iteration and `purrr`\n\n## Iteration\n\nIteration is everywhere. It underpins much of mathematics and statistics. If you've ever seen the $\\Sigma$ symbol, then you've seen (and probably used) iteration.\n\nReasons for iteration:\\\n- reading in multiple files from a directory\\\n- running the same operation multiple times\\\n- running different combinations of the same model\\\n- creating similar figures / tables / outputs\n\n## `for` loops\n\nEnter `for` loops. `for` loops are the \"OG\" form of iteration in computer science. The basic syntax is below. Basically, we can use a for loop to loop through and print a series of things.\n\n```{r basic loop}\nfor(i in letters[1:5]){\n  print(i)\n}\n```\n\n-   The code above \"loops\" through 5 times, printing the iteration letter.\n\n## `_apply()` family\n\n-   A somewhat faster version of `for` loops comes from the `_apply()` family of functions, including:\n    -   `apply()`, `lapply()`, `sapply()`, and `mapply()`. Unlike `for` loops, these are vectorized, which makes them more efficient.\n\n```{r apply functions}\nlapply(letters[1:5], print)\nsapply(letters[1:5], print)\nmapply(print, letters[1:5])\n```\n\n## `purrr` and `_map_()` functions\n\n-   Today, though, we'll focus on the `map()` family of functions, which is the functions through which `purrr` iterates.\n\n```{r map functions}\nmap(letters[1:5], print)\n```\n\n-   **For a more thorough comparison of `for` loops, the `_apply()` family, and `_map_()` functions, see https://jennybc.github.io/purrr-tutorial/**\n\n## `purrr` and `_map_()` predicates {.smaller}\n\n-   Today, though, we'll focus on the `map()` family of functions, which is the functions through which `purrr` iterates.\n\n```{r map functions 2, eval = F}\nmap(letters[1:5], print)\n```\n\n-   Note that this returns a list, which we may not always want.\n-   With `purrr`, we can change the kind of output of `map()` by adding a predicate, like `lgl`, `dbl`, `chr`, and `df`.\n-   So in the example above, we may have wanted just the characters to print. To do that we'd call `map_chr()`:\n\n```{r basic map_chr}\nmap_chr(letters[1:5], print)\n```\n\n-   Note that it also returns the concatenated character vector as well as printing each letter individually (i.e. iteratively).\n\n## `purrr` and `_map_()` antecedents\n\n-   How many mappings?\n    -   Single mapping: `map_()`\\\n    -   Parallel (2) mapping(s): `map2_()`\\\n    -   3 or more mappings: `pmap_()`\n\n```{r}\nmap2_chr(letters[1:5], 1:5, paste)\n```\n\n-   Note here that we can use `map2()` and `pmap()` with the predicates from above.\n\n## List Columns and the Power of `purrr`\n\n-   On the previous slide, we saw a data frame **inside** of a data frame. This is called a list column within a nested data frame.\n\n-   In this case, we created a list column using map, but one of the best things about `purrr` is how it combines with the `nest()` and `unnest()` functions from the `tidyr` package.\n\n-   We'll return to `nest()` later to demonstrate how anything you would iterate across is also something we can `nest()` by in long format data frames.\n\n## Use Cases\n\n1.  Reading Data\\\n2.  Cleaning Data\n3.  Running Models\\\n4.  (Plotting Figures - Week 8)\\\n5.  (Creating Tables - Week 8)\n\n### Reading Data\n\nThere are a number of different cases where `purrr` and `map()` maybe useful for reading in data including:\n\n-   subject-level files for an experimental task\\\n-   subject- and task-level files for an experimental task\n-   EMA data\\\n-   longitudinal data\\\n-   web scraping and text mining\n\n#### Reading Data: Participant-Level EMA\n\nFor this first example, I'll show you how this would look with a `for` loop before I show you how it looks with `purrr`.\n\nAssuming you have all the data in a single folder and the format is reasonably similar, you have the following basic syntax:\n\n```{r simple reading loop, eval = F}\nfiles <- list.files(data_path)\ndata <- list()\nfor(i in files){\n  data[[i]] <- read_csv(i)\n}\ndata <- combine(data)\n```\n\n-   This works fine in this simple case, but where `purrr` really shines in when you need to make modifications to your data before combining, whether this be recoding, removing missing cases, or renaming variables.\n\n-   But first, the simple case of reading data. The code below will download a .zip file when you run it. Once, you do, navigate to your Desktop to unzip the folder. Open the R Project and you should be able to run the rest of the code\n\n```{r get data}\ndata_source <- \"https://github.com/emoriebeck/psc290-data-FQ23/raw/main/04-workshops/05-week5-purrr/05-week5-purrr.zip\"\ndata_dest <- \"~/Desktop/05-week5-purrr.zip\"\ndownload.file(data_source, data_dest)\n```\n\n```{r read data ex1, eval = T}\ndf1 <- tibble(\n  ID = list.files(\"data/example_1\")\n  ) %>%\n  mutate(data = map(ID, ~read_csv(sprintf(\"data/example_1/%s\", .)))) %>%\n  unnest(data) \n```\n\n-   The code above creates a list of ID's from the data path (files named for each person), reads the data in using the `map()` function from `purrr`, removes the \".csv\" from the ID variable, then unnests the data, resulting in a data frame for each person.\n\n-   Now, we're going to combine with what we learned about last time with codebooks.\n\n```{r codebook}\ncodebook <- read_csv(\"data/codebook_ex1.csv\")\ncodebook\n```\n\n-   Now, that we have a codebook, what are the next steps?\n\n```{r read data ex1 2, eval = T}\ndf1 <- tibble(\n  ID = list.files(\"data/example_1\")\n  ) %>%\n  mutate(data = map(ID, ~read_csv(sprintf(\"data/example_1/%s\", .)))) %>%\n  unnest(data) \nhead(df1, 6)\n```\n\n1.  pull old names in raw data from codebook\\\n2.  pull new names from codebook\\\n3.  select columns from codebook in loaded data\\\n4.  rename columns with new names\n\n```{r read data complex ex1, eval = T}\nold.names <- codebook$old_name # pull old names in raw data from codebook  \nnew.names <- codebook$item_name # pull new names from codebook  \ndf1 <- tibble(\n  ID = list.files(\"data/example_1\")\n  ) %>%\n  mutate(data = map(ID, ~read_csv(sprintf(\"data/example_1/%s\", .)))\n         , ID = str_remove_all(ID, \".csv\")) %>%\n  unnest(data) %>%\n  select(ID, count, all_of(old.names)) %>% # select columns from codebook in loaded data  \n  setNames(c(\"ID\", \"count\", new.names)) # rename columns with new names  \nhead(df1, 6)\n```\n\n#### Descriptives: Within-Person Correlations\n\nWith these kinds of data, the first thing, we may want to do is look at within-person correlations, which we can do with `purrr`.\n\n```{r}\ncor_fun <- function(x) cor(x %>% select(-count), use = \"pairwise\")\n\nnested_r <- df1 %>%\n  group_by(ID) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(r = map(data, cor_fun))\nhead(nested_r, 6)\n```\n\nWe can access it like a list:\n\n```{r}\nnested_r$data[[1]]\n```\n\n-   But we can't easily (well, nicely) just `unnest()` a matrix\n-   We've lost a lot of information along the way\n-   So what do we do?\n\n```{r}\nnested_r %>%\n  select(-data) %>%\n  unnest(r)\n```\n\n-   Write a function, of course!\n\n```{r}\ncor_fun <- function(x) {\n  r <- cor(x %>% select(-count), use = \"pairwise\")\n  r %>% \n    data.frame() %>%\n    rownames_to_column(\"var\") %>%\n    as_tibble()\n}\n\nnested_r <- nested_r %>%\n  mutate(r = map(data, cor_fun))\nhead(nested_r, 10)\n```\n\n-   Let's try unnesting again:\n\n```{r}\nnested_r %>%\n  select(-data) %>%\n  unnest(r) %>% \n  arrange(desc(var))\n```\n\n-   There's more I would usually do here to format a correlation table for each participant and output the file as a PDF or html so I can post it on GitHub / OSF\n-   But we'll get there in Week 8/9!\n\n#### Descriptives: Means, sds, etc.\n\n```{r}\ntidy_describe <- function(df) {\n  df %>%\n    pivot_longer(\n      -count\n      , names_to = \"item\"\n      , values_to = \"value\"\n      , values_drop_na = T\n      ) %>%\n    group_by(item) %>%\n    summarize(\n      mean   = mean(value,   na.rm = TRUE),\n      sd     = sd(value,     na.rm = TRUE),\n      median = median(value, na.rm = TRUE),\n      min    = min(value,    na.rm = TRUE),\n      max    = max(value,    na.rm = TRUE),\n      n      = n(),\n      n_miss = sum(is.na(value)),\n      .groups = \"drop\"\n      )\n}\n```\n\n```{r}\nnested_r <- nested_r %>%\n  mutate(desc = map(data, tidy_describe)) \nnested_r\n```\n\n```{r}\nnested_r %>%\n  select(-data, -r) %>%\n  unnest(desc)\n```\n\n#### Models\n\n-   We can put essentially anything into a nested data frame.\n\n-   The magic happens because everything is indexed by the other columns in the data frame, so we can keep track of it\n\n-   And unlike a normal list, we aren't stuck with nested list structures that are really hard to parse and navigate through\n\n-   Next, I'm going to show you how to use purrr with models\n\n-   Modeling is not a focus of this class, but I want to demonstrate this as a ~workflow~ because it completely revolutionized mine!\n\n-   But first, we need to format our data:\n\n```{r}\ndf_long <- df1 %>%\n  select(-satisfaction) %>%\n  pivot_longer(\n    cols = c(-count, -ID)\n    , names_to = c(\"trait\", \"item\")\n    , names_sep = \"_\"\n    , values_to = \"value\"\n    , values_drop_na = T\n    )\ndf_long\n```\n\nTo create composites, we'll:\\\n1. separate traits from items 2. `group_by()` trait, count, and ID 3. calculate the composites using `summarize()`\n\n```{r}\ndf_long <- df_long %>%\n  group_by(ID, count, trait) %>%\n  summarize(value = mean(value)) %>%\n  ungroup() %>%\n  left_join(df1 %>% select(ID, count, satisfaction))\ndf_long\n```\n\nThen we'll get within-person centered values using our own little function!\n\n```{r}\ncenter <- function(x) x - mean(x, na.rm = T)\n\ndf_long <- df_long %>%\n  group_by(ID, trait) %>%\n  mutate(value_c = center(value)) %>%\n  ungroup()\ndf_long\n```\n\nAnd grand-mean centered within-person averages\n\n```{r}\ndf_long <- df_long %>%\n  group_by(ID, trait) %>%\n  mutate(value_gmc = mean(value)) %>%\n  group_by(trait) %>%\n  mutate(value_gmc = center(value_gmc)) %>%\n  ungroup()\ndf_long\n```\n\nAnd now we are ready to run our models. But first, we'll `nest()` our data.\n\n```{r}\nnested_mods <- df_long %>%\n  group_by(trait) %>%\n  nest() %>%\n  ungroup() \nnested_mods\n```\n\nAnd now run the models.\n\n```{r}\nrun_model <- function(d) lmer(satisfaction ~ value_c * value_gmc + (1 | ID), data = d)\n\nnested_mods <- df_long %>%\n  group_by(trait) %>%\n  nest() %>%\n  ungroup() %>% \n  mutate(model = map(data, run_model))\nnested_mods\n```\n\nAnd get data frames of the results:\n\n```{r}\nsprintfna <- function(x) ifelse(is.na(x), NA_character_, sprintf(\"%.2f\", x))\n\ntidy_tab <- function(m){\n  tidy(m, conf.int = T) %>%\n    mutate(pval = pnorm(abs(estimate/`std.error`), lower.tail = FALSE),\n           p = round(pval, digits = 3),\n           p = ifelse(pval < .001, \"p &lt; .001\", paste0(\"p = \", p))) %>%\n    mutate_at(vars(estimate, conf.low, conf.high), sprintfna) %>%\n    mutate(CI = ifelse(is.na(conf.low), \"\", sprintf(\"[%s,%s]\", conf.low, conf.high))) %>%\n    dplyr::select(term, estimate, CI, p)\n}\n\nnested_mods <- nested_mods %>%\n  mutate(tidy = map(model, tidy_tab))\nnested_mods\n```\n\n##### Unnesting\n\nWhich we can print into pretty data frames\n\n```{r}\nnested_mods %>%\n  select(trait, tidy) %>%\n  unnest(tidy)\n```\n\n##### Unnesting & Tabling\n\nWhich we can pretty easily turn into tables:\n\n```{r}\nmod_tab <- nested_mods %>%\n  select(trait, tidy) %>%\n  unnest(tidy) %>%\n  pivot_wider(\n    names_from = \"trait\"\n    , names_glue = \"{trait}_{.value}\"\n    , values_from = c(\"estimate\", \"CI\", \"p\")\n  ) %>%\n  select(term, starts_with(\"E\"), starts_with(\"A\"), starts_with(\"C\"), starts_with(\"N\"), starts_with(\"O\"))\n\nmod_tab\n```\n\n```{r}\nhdr <- c(1, rep(3, 5))\nnames(hdr) <- c(\" \", \"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Neuroticism\", \"Openness\")\n\nmod_tab <- mod_tab %>%\n  kable(.\n        , \"html\"\n        , escape = F\n        , col.names = c(\"Term\", rep(c(\"<em>b</em>\", \"CI\", \"<em>p</em>\"), times = 5))\n        , align = c(\"r\", rep(\"c\", 15))\n        , caption = \"<strong>Table 1</strong><br><em>Multilevel Model Estimates of Between- and Within-Person Big Five-State Satisfaction Associations\"\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\", font_size = 15) %>%\n  add_header_above(hdr)\nmod_tab\n```\n\n# Appendix\n\n## Appendix: Sourcing Functions\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":true,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"highlight-style":"tango","self-contained":true,"output-file":"05-week5-workbook.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"cosmo","title":"Week 5 Workbook","author":"Emorie D Beck","code-copy":true,"toc-float":true},"extensions":{"book":{"multiFile":true}}}}}