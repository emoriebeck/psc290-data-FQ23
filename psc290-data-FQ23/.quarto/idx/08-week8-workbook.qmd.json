{"title":"Week 8 Workbook","markdown":{"yaml":{"title":"Week 8 Workbook","author":"Emorie D Beck","format":{"html":{"code-tools":true,"code-copy":true,"code-line-numbers":true,"code-link":true,"theme":"united","highlight-style":"tango","df-print":"paged","code-fold":"show","toc":true,"toc-float":true,"self-contained":true}},"editor":"visual","editor_options":{"chunk_output_type":"console"}},"headingText":"Week 8 - Functional Tables and Figures","containsRefs":false,"markdown":"\n\n\n```{r, echo = F}\npkg <- c(\"knitr\", \"psych\", \"lme4\", \"broom\", \"broom.mixed\", \"kableExtra\", \"lubridate\", \"plyr\", \"tidyverse\")\npkg <- pkg[!pkg %in% rownames(installed.packages())]\nif(length(pkg) > 0) map(pkg, install.packages)\n\nlibrary(knitr)\nlibrary(psych)\nlibrary(lme4)\nlibrary(lavaan)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(kableExtra)\nlibrary(lubridate)\nlibrary(plyr)\nlibrary(tidyverse)\n```\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, \n                      message = FALSE,\t\n                      warning = FALSE,\n                      results = 'show',\n                      fig.width = 4, \n                      fig.height = 4, \n                      fig.retina = 3)\noptions(htmltools.dir.version = FALSE\n        , knitr.kable.NA = \"\")\n```\n\n# Outline\n\n1.  Questions on Homework\n2.  Tables\n3.  Figures\n4.  (GitHub)\n\n# APA Tables\n\nIn psychology, we must work within the confines of APA style. Although these guidelines have been updated, the style guide remains quite similar to earlier guidelines with respect to tables.\n\nBut psychology research is heterogeneous and expectations for modern tables require combining multiple models in creative ways.\n\nSmall tweaks to data or model arguments can spell disaster for creating a table. It's easy to make mistakes in copying values or matching different models to their respective rows and columns.\n\nThankfully, the increasing popularity of `R` has been coupled with more methods for creating a reproducible workflow that includes tables.\n\n## Tables in R {.smaller}\n\nIn this workshop, we will directly cover 3 different use cases, while a few others will be included in supplementary materials.\n\nPersonally, I favor non-automated tools, so we will cover the following packages:\\\n- `kable` + `kableExtra` (<a href=\"http://haozhu233.github.io/kableExtra/awesome_table_in_html.html\">html</a> and <a href=\"https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf\">LaTeX</a>)\\\n- <a href =\"https://github.com/crsh/papaja\">`papaja`</a>\n\nUsing these packages will build on earlier tutorials using `tidyr`, `dplyr`, workflow, and `purrr` and round out our discuss on data presentation using `ggplot2`.\n\nFor less flexible but more accessible tables see:\\\n- <a href=\"https://cran.r-project.org/web/packages/apaTables/vignettes/apaTables.html\">`apaTable`</a>\\\n- <a href=\"http://www.strengejacke.de/sjPlot/\">`sjPlot`</a>\\\n- <a href=\"https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html\">`corrplot`</a>\n\n## Important Tools\n\nAlthough it doesn't cover all models, the `broom` and `broom.mixed` family of packages will provide easy to work with estimates of nearly all types of models and will also provide the model terms that are ideal for most APA tables, including estimates, standard errors, and confidence intervals.\n\n`lavaan` models are slightly more complicated, but it's actually relatively easy to deal with them (and how to extract their terms), assuming that you understand the models you are running.\n\n## Data {.smaller}\n\nThe data we're going to use are from the teaching sample from the German Socioeconomic Panel Study. These data have been pre-cleaned (see earlier workshop on workflow and creating guidelines for tips).\n\nThe data we'll use fall into three categories:\\\n1. **Personality trait composites:** Negative Affect, Positive Affect, Self-Esteem, CESD Depression, and Optimism. These were cleaned, reversed coded, and composited prior to being included in this final data set.\\\n2. **Outcomes:** Moving in with a partner, marriage, divorce, and child birth. These were cleaned, coded as 1 (occurred) or 0 (did not occur) according to whether an outcome occurred for each individual or not *after* each possible measured personality year. Moreover, people who experienced these outcomes prior to the target personality year are excluded.\\\n3. **Covariates:** Age, gender (0 = male, 1 = female, education (0 = high school or below, 1 = college, 2 = higher than college), gross wages, self-rated health, smoking (0 = never smoked 1 = ever smoked), exercise, BMI, religion, parental education, and parental occupational prestige (ISEI). Each of these were composited for all available data up to the measured personality years.\n\n```{r}\n(gsoep <- read_csv(\"week8-data.csv\"))\n```\n\n# Basic: One DV/Outcome, Multiple Model Terms\n\nWe'll start with a basic case, predicting who has a child from personality, both with and without control variables.\n\nBecauce outcome variables are binary, we'll use logistic regression.\n\nThe basic form of the model is: $\\log\\Big(\\frac{p_i}{1-p_i}\\Big) = b_0 + b_1X_1 + b_2X_2 ... b_pXp$\n\nIn other words, we're predicting the log odds of having a child from a linear combination of predictor variables.\n\n## Set up the data frame\n\nFirst, we'll use some of what we learned in the `purrr` workshop to set ourselves up to be able to create these tables easily, using `group_by()` and `nest()` to create nested data frames for our target personality + outcome combinations. To do this, we'll also use what you learned about `filter()` and `mutate()`.\n\n```{r}\ngsoep_nested1 <- gsoep %>%\n  filter(Outcome == \"chldbrth\") %>%\n  group_by(Trait, Outcome) %>%\n  nest() %>%\n  ungroup()\n```\n\nNext, let's pause and see what we have. We now have a data frame with 3 columns (Outcome, Trait, and data) and 4 rows. The data column is of class list, meaning it's a \"list column\" that contains a `tibble` in each cell. This means that we can use `purrr` functions to run operations on each of these data frames individually but without having to copy and paste the same operation multiple times for each model we want to run.\n\n```{r}\ngsoep_nested1\n```\n\n## Run Models\n\nTo run the models, I like to write short functions that are easier to read than including a local function within a call to `purrr::map()`. Here, we're just going to write a simple function to predict child birth from personality.\n\n```{r}\nmod1_fun <- function(d){\n  d$o_value <- factor(d$o_value)\n  glm(o_value ~ p_value, data = d, family = binomial(link = \"logit\"))\n}\n\ngsoep_nested1 <- gsoep_nested1 %>%\n  mutate(m = map(data, mod1_fun))\n```\n\nNow, when we look at the nested frame, we see an additional column, which is also a list, but this column contains `<glm>` objects rather than `tibbles`.\n\n```{r}\ngsoep_nested1\n```\n\n## Get Key Terms\n\nNow that we have the models, we want to get our key terms. I'm a big fan of using the function `tidy` from the `broom` package to do this. Bonus because it plays nicely with `purrr`. Double bonus because it will give us confidence intervals, which I generally prefer over p-values and standard erorrs because I find them more informative.\n\n```{r}\ngsoep_nested1 <- gsoep_nested1 %>%\n  mutate(tidy = map(m, ~tidy(., conf.int = T)))\ngsoep_nested1\n```\n\nNote that what I've used here is a local function, meaning that I've used the notation `~`function(., arguments). The tilda tells `R` we want a local function, and the `.` tells R to use the mapped `m` column as the function input.\n\nNow we have a fifth column, which is a list column called `tidy` that contains a `tibble`, just like the `data` column.\n\n## Creating a Table\n\nNow we are finally ready to create a table! I'm going to use `kable` + `kableExtra` to do this in steps.\n\nFirst, we'll unnest the `tidy` column from our data frame. Before doing so, we will drop the `data` and `m` columns because they've done their work for now.\n\n```{r}\ntidy1 <- gsoep_nested1 %>%\n  select(-data, -m) %>%\n  unnest(tidy)\ntidy1\n```\n\nAs you can see, we now have lots of information about our model terms, which are already nicely indexed by Outcome and Trait combinations.\n\nBut before we're ready to create a table, we have to make a few considerations:\n\n-   What is our target term? In this case \"p_value\" which is the change in log odds associated with a 1 unit increase/decrease in p_value.\\\n-   How will we denote significance? In this case, we'll use confidence intervals whose signs match. We'll then bold these terms for our table.\n-   What is the desired final structure for the table? I'd like columns for Trait, estimate (b), and confidence intervals (CI) formatted to two decimal places and bolded if significant. I'd also like a span header denoting that the outcome measure is child birth.\n\n### What is our target term?\n\nIn this case \"p_value\" which is the change in log odds associated with a 1 unit increase/decrease in p_value.\n\n```{r}\ntidy1 <- tidy1 %>% filter(term == \"p_value\")\ntidy1\n```\n\n### How will we denote significance?\n\nIn this case, we'll use confidence intervals whose signs match. We'll then bold these terms for our table.\n\n```{r}\ntidy1 <- tidy1 %>% mutate(sig = ifelse(sign(conf.low) == sign(conf.high), \"sig\", \"ns\"))\ntidy1\n```\n\n### What is the desired final structure for the table?\n\nI'd like columns for Trait, estimate (b), and confidence intervals (CI) formatted to two decimal places and bolded if significant. I'd also like a span header denoting that the outcome measure is child birth.\n\nBefore we do this, though, we need to convert our log odds to odds ratios, using the `exp()` function.\n\n```{r}\ntidy1 <- tidy1 %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) \ntidy1\n```\n\nNow, we can format them.\n\n```{r}\ntidy1 <- tidy1 %>%\n  mutate_at(vars(estimate, conf.low, conf.high), ~sprintf(\"%.2f\", .)) \ntidy1\n```\n\n`sprintf()` is my favorite base `R` formatting function. \"%.2f\" means I'm asking it to take a floating point number and include 2 digits after the \".\" and 0 before. We can now see that the `estimate`, `conf.low`, and `conf.high` columns are of class `<chr>` instead of `<dbl>`.\n\nBut now we need to create our confidence intervals.\n\n```{r}\ntidy1 <- tidy1 %>%\n  mutate(CI = sprintf(\"[%s, %s]\", conf.low, conf.high))\ntidy1\n```\n\nAnd bold the significant confidence intervals and estimates.\n\n```{r}\ntidy1 <- tidy1 %>%\n  mutate_at(vars(estimate, CI), ~ifelse(sig == \"sig\", sprintf(\"<strong>%s</strong>\", .), .))\ntidy1\n```\n\nThis reads as \"for both the estimate and the CI columns, if the sig column is equal to\"sig\", then let's format it as bold using html. Otherwise, let's leave it alone.\" And indeed, we can see that the final result formats 3/4 rows.\n\nThankfully, these can be achieved without considerable reshaping of the data, which is why we've started here, so we're almost done. We just need to get rid of some unnecessary columnns.\n\n```{r}\ntidy1 <- tidy1 %>%\n  select(Trait, OR = estimate, CI)\n```\n\nBecause we just have one target term and one outcome, we don't need those columns, so we're just keeping Trait, OR, which I renamed as such within in the select call, and CI.\n\n## Kabling a Table\n\nNow let's `kable`. You've likely used the `kable()` function from the `knitr` before. It's a very useful and simple function in most occasions.\n\n```{r}\nkable(tidy1)\n```\n\nIt will automatically generate the html code needed to create a table. But if we look closely at the code, it gives us some gobbledigook where we inputted html, so we need a way around that. I'm also going to throw in `kable_styling(full_width = F)` from the `kableExtra` package to help out here. It's not doing much, but it will make the formatted table print in your Viewer.\n\n```{r}\nkable(tidy1, escape = F) %>%\n  kable_classic(full_width = F, html_font = \"Times\")\n```\n\nMuch better. But this still doesn't look like an APA table, so let's keep going.\n\n1.  APA tables usually write out long names for our predictors, so let's change those first. I'm going to create a reference tibble and use `mapvalues()` from the `plyr` function for this.\\\n\n```{r}\np_names <- tibble(\n  old = c(\"NegAff\", \"PA\", \"SE\", \"OP\", \"DEP\"),\n  new = c(\"Negative Affect\", \"Positive Affect\", \"Self-Esteem\", \"Optimism\", \"Depression\")\n)\n\ntidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(., escape = F) %>%\n  kable_classic(full_width = F, html_font = \"Times\")\n```\n\n2.  The alignment of the columns isn't quite right. Let's fix that. We'll change the trait to right justified and b and CI to centered.\\\n\n```{r}\ntidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(.\n        , escape = F\n        , align = c(\"r\", \"c\", \"c\")\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\")\n```\n\n3.  But we're still missing our span header. There's a great function in the `kableExtra` package for this `add_header_above`. This function takes a named vector as argument, where the elements of the vector refer to the number of columns the named element should span.\n\n```{r}\ntidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(.\n        , escape = F\n        , align = c(\"r\", \"c\", \"c\")\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\") %>%\n  add_header_above(c(\" \" = 1, \"Birth of a Child\" = 2))\n```\n\nNote that what the `\" \" = 1` does is skip the Trait column. This is very useful because it let's us not have a span header over every column.\n\n4.  APA style requires we note how we denote significance and have a title, so let's add a title and a note.\\\n\n```{r}\ntidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(.\n        , escape = F\n        , align = c(\"r\", \"c\", \"c\")\n        , caption = \"<strong>Table 1</strong><br><em>Estimated Personality-Outcome Associations</em>\"\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\")%>%\n  add_header_above(c(\" \" = 1, \"Birth of a Child\" = 2)) %>%\n  add_footnote(label = \"Bold values indicate terms whose confidence intervals did not overlap with 0\", notation = \"none\")\n```\n\nWe did it!\n\n## A Quick Note: HTML v. LaTeX\n\nWhen creating tables, I prefer using HTML when I need the resulting tables to be in HTML and LaTeX when I can place the tables in a PDF. The syntax using `kable` and `kableExtra` is the same with the following exceptions:\n\n1.  The `format` argument in `kable()` would need to be set as `format = \"latex\"`.\\\n2.  The chunk option for a table to render would need to be set as `{r, results = 'asis'}`.\\\n3.  Bolding would need to be done as `\\\\textbf{}`, rather than the `html` `<strong></strong>` tag.\\\n4.  When using `collapse_rows()`, which we'll get to later, you'd want to set the `latex_hline` argument to `latex_hline = \"none\"`.\n\n# Extracting terms from `lavaan`\n\nMost models in `R` can easily have terms extracted via `broom` or `broom.mixed`, but one major exception to this are SEM models in `lavaan`. Let's step through an example using lavaan to show you some of my tricks for working with it.\n\n# `lavaan` Example 1:\n\n## Data\n\nFor this example, we'll use a different data set from the SOEP that have the item level personality data. The code below reads it in, recodes, and reverse scores it.\n\n```{r}\nrev_code <- c(\"Big5__A_coarse\", \"Big5__C_lazy\", \"Big5__E_reserved\", \"Big5__N_dealStress\")\n(gsoep2 <- read_csv(\"week8-data-2.csv\") %>%\n   mutate_at(vars(matches(\"Big5\")), function(x) {x[x < 0] <- NA; x}) %>%\n   mutate_at(vars(matches(\"LifeEvent\")), ~mapvalues(., seq(-7,1), c(rep(NA, 5), 0, NA, NA, 1), warn_missing = F)) %>%\n    mutate_at(\n    vars(all_of(rev_code))\n    , ~as.numeric(reverse.code(., keys = -1, mini = 1, maxi = 7))\n    ))\n```\n\nNow we'll change it to long format for easier cleaning and reshaping\n\n```{r}\ngsoep2_long <- gsoep2 %>%\n  pivot_longer(\n    cols = c(starts_with(\"Big5\"), starts_with(\"Life\"))\n    , names_to = c(\"category\", \"item\")\n    , names_sep = \"__\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) \n```\n\n## Data Cleaning\n\n-   Now, we need to reshape the Big Five data for lavaan\n-   If you learn nothing else from this class, I want you to learn this trick I use to run the same model multiple times on different variables\n-   We are going to run a second-order latent growth model of the BFI-S, which has three items per Big Five trait\n-   The item numbers are arbitrary but consistent across traits (3) as are the years (2005, 2009, 2013)\n-   So if we separate the trait information from the item and year, then we can run identical models across traits\n\n```{r}\ngsoep2_lavaan <- gsoep2_long %>% \n  filter(category == \"Big5\") %>%\n  separate(item, c(\"trait\", \"item\"), sep = \"_\") %>%\n  group_by(Procedural__SID, year, trait) %>%\n  mutate(item = mapvalues(item, unique(item), 1:n())) %>%\n  ungroup() %>%\n  pivot_wider(\n    names_from = c(\"item\", \"year\")\n    , names_prefix = \"I\"\n    , values_from = \"value\"\n  )\ngsoep2_lavaan\n```\n\n## Second-Order Latent Growth Model\n\n-   Because of how we set up the data, we only have to write this out once. This is extra helpful for SEM because the model syntax can get really long and it would be time consuming to have to write this out separately for each of the Big Five\n\n```{r}\nmod <- '\n  W1 =~ NA*I1_2005 + lambda1*I1_2005 + lambda2*I2_2005 + lambda3*I3_2005\n  W2 =~ NA*I1_2009 + lambda1*I1_2009 + lambda2*I2_2009 + lambda3*I3_2009\n  W3 =~ NA*I1_2013 + lambda1*I1_2013 + lambda2*I2_2013 + lambda3*I3_2013\n  \n  i =~ 1*W1 + 1*W2 + 1*W3\n  s =~ -1*W1 + 0*W2 + 1*W3\n  \n  ## intercepts\n  I1_2005 ~ t1*1\n  I1_2009 ~ t2*1\n  I1_2013 ~ t3*1\n  \n  I2_2005 ~ t1*1\n  I2_2009 ~ t2*1\n  I2_2013 ~ t3*1\n  \n  I3_2005 ~ t1*1\n  I3_2009 ~ t2*1\n  I3_2013 ~ t3*1\n  \n  ## correlated residuals across time\n  I1_2005 ~~ I1_2009 + I1_2013\n  I1_2009 ~~ I1_2013\n  I2_2005 ~~ I2_2009 + I2_2013\n  I2_2009 ~~ I2_2013\n  I3_2005 ~~ I3_2009 + I3_2013\n  I3_2009 ~~ I3_2013\n  \n  ## latent variable intercepts\n  W1 ~ 0*1\n  W2 ~ 0*1\n  W3 ~ 0*1\n  \n  #model constraints for effect coding\n  ## loadings must average to 1\n  lambda1 == 3 - lambda2 - lambda3\n  ## means must average to 0\n  t1 == 0 - t2 - t3\n  '\n```\n\nNow, we'll write a little function that will run the model syntax across cross-sections of our data for each Big Five trait\n\n```{r}\nlavaan_fun <- function(d){\n  m <- growth(\n    mod\n    , data = d\n    , missing = \"fiml\"\n  )\n  return(m)\n}\n```\n\nNow, we can create those cross-sections using `tidyr::nest()` and then run the model using `purrr::map()`. Having the trait information as rows in our data frame and using list columns let's us separate out the trait information from the item/year information we need to run the models.\n\n```{r}\ngsoep_nested2 <- gsoep2_lavaan %>%\n  group_by(category, trait) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(m = map(data, lavaan_fun))\ngsoep_nested2\n```\n\n## Extracting Results\n\n-   Summaries of SEM models are going to spit out a MUCH longer summary of results than the typical models we run and extract information from using `broom`/`broom.mixed`.\n-   When I do SEM, I do make a table that includes all of these parameters for each model and put it in the supplement\n-   But for the paper itself, we only want a subset of key model terms across all traits\n\n```{r}\nsummary(gsoep_nested2$m[[1]])\n```\n\nSo what I do is to write a little function. I try to write it so that it can be used across multiple qualitatively different models if I'm running different (e.g., see second example later). To do that, I have (1) the function and (2) a `tibble` called `terms` that contains the paths I want to extract.\n\n```{r}\nterms <- tribble(\n  ~path,     ~new,\n  \"i~1\", \"Intercept\",\n  \"s~1\", \"Slope\",\n  \"i~~i\", \"Intercept Variance\",\n  \"s~~s\", \"Slope Variance\",\n  \"i~~s\", \"Intercept-Slope Covariance\"\n)\n\nextract_fun <- function(m){\n  parameterEstimates(m) %>%\n    data.frame() %>%\n    unite(path, lhs, op, rhs, sep = \"\") %>%\n    filter(path %in% terms$path) %>%\n    left_join(terms) %>%\n    select(term = new, est, se, ci.lower, ci.upper, pvalue)\n}\n```\n\nNow we can run that function to extract the results we care about\n\n```{r}\ngsoep_nested2 <- gsoep_nested2 %>%\n  mutate(summary = map(m, extract_fun))\ngsoep_nested2\n```\n\n## Formatting Results\n\nThe way that we need to format the data is pretty standard, so below are three functions I use when I need to format results from lavaan for putting into a table\n\n```{r}\nround_fun <- function(x) if(!is.na(x)) if(abs(x) > .01) sprintf(\"%.2f\", x) else sprintf(\"%.3f\", x) else \"\"\npround_fun <- function(x) if(!is.na(x)) if(abs(x) > .01) sprintf(\"%.2f\", x) else if(x >= .001) sprintf(\"%.3f\", x) else \"&lt; .001\" else \"\"\n\nformat_fun <- function(d){\n  d %>%\n    mutate(sig = ifelse(pvalue < .05, \"sig\", \"ns\")) %>%\n    rowwise() %>%\n    mutate_at(vars(est, ci.lower, ci.upper), round_fun) %>%\n    mutate_at(vars(pvalue), pround_fun) %>%\n    ungroup() %>%\n    mutate(CI = sprintf(\"[%s,%s]\", ci.lower, ci.upper)) %>%\n    mutate_at(vars(est, CI, pvalue), ~ifelse(sig == \"sig\" & !is.na(sig), sprintf(\"<strong>%s</strong>\", .), .)) \n}\n```\n\nNow let's run that and see what it looks like\n\n```{r}\ngsoep_tab <- gsoep_nested2 %>%\n  select(-data, -m) %>%\n  unnest(summary) %>%\n  format_fun() \ngsoep_tab\n```\n\nWe're really close to being ready to make the table using kable, but we need to do a little reshaping and to remove some columns first.\n\n```{r}\ngsoep_tab <- gsoep_tab %>%\n  select(-ci.lower, -ci.upper, -sig, -category, -pvalue) %>%\n  pivot_wider(\n    names_from = \"trait\"\n    , names_glue = \"{trait}_{.value}\"\n    , values_from = c(\"est\", \"CI\")\n  ) \ngsoep_tab\n```\n\nUnfortunately, `pivot_wider()` doesn't really let us have too much control over the order of the columns, so we need to move them around so the estimates and CI's for each trait are next to each other. There are many ways to do this, but I'm showing you two below:\n\n```{r}\nord <- paste(rep(c(\"E\", \"A\", \"C\", \"N\", \"O\"), each = 2), rep(c(\"est\", \"CI\"), times = 5), sep = \"_\")\ngsoep_tab <- gsoep_tab %>%\n  select(term, all_of(ord)) %>%\n  # select(starts_with(\"E\"), starts_with(\"A\"), starts_with(\"C\"), starts_with(\"N\"), starts_with(\"O\"))\n  mutate(term = factor(term, terms$new)) %>%\n  arrange(term)\ngsoep_tab\n```\n\n## Kabling the Table\n\nNow, let's go ahead and create the table using kable. Like before, we're going to use `add_header_above()`, but I'm going to show you a trick that I use for it to make specifying it a little easier. - `add_header_above()` takes in a named vector as input where the values have to sum to the number of columns in the data frame (11)\n\n```{r}\nhdr <- c(1, rep(2,5))\nnames(hdr) <- c(\" \", \"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Neuroticism\", \"Openness\")\ngsoep_tab %>%\n  kable(.\n        , escape = F\n        , align = c(\"r\", rep(\"c\", 10))\n        , col.names = c(\"Term\", rep(c(\"<em>est.</em>\", \"CI\"), times = 5))\n        , caption = \"<strong>Table 2</strong><br><em>Big Five Personality Trait Trajectories from Latent Growth Models</em>\"\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\") %>%\n  add_header_above(hdr) %>%\n  add_footnote(label = \"Bold values indicate terms p < .05\", notation = \"none\")\n```\n\n# `lavaan` Example 2\n\n## Second-Order Latent Growth Model: Slopes as Predictors\n\nNow, let's do a second example that is more complex and uses the intercepts and slopes we estimated to predict life outcomes. To do so, we first need to clean and prep the life event data and merge it back with our Big Five item-level data.\n\nNote that in this case, we're using long data two ways: for the Big Five traits **and** for the life events. In other words, we're getting every combination of the two and are using `inner_join()` to make that happen!\n\n```{r}\ngsoep2_lavaan <- gsoep2_long %>%\n  filter(category == \"LifeEvent\") %>%\n  group_by(Procedural__SID, Demographic__DOB, item) %>%\n  summarize(le_value = max(value)) %>%\n  ungroup() %>%\n  mutate(age = 2005 - Demographic__DOB - 45) %>%\n  rename(le = item) %>%\n  inner_join(gsoep2_lavaan)\ngsoep2_lavaan\n```\n\n## Data Cleaning\n\nNow let's create the nested data frame.\n\n```{r}\ngsoep_nested3 <- gsoep2_lavaan %>%\n  drop_na(trait, le) %>%\n  group_by(trait, le) %>%\n  nest() %>%\n  ungroup()\ngsoep_nested3\n```\n\n## Model Setup\n\nWe're really just building on the second-order latent growth models we already ran. We already ran those and know that we have specified the models correctly / have enough variance to use the slopes and intercept to predict other things. So now we just need to add the new regression paths to that syntax to be able to run the models.\n\nAs with the model we specified before, by having the life event data long, we are able to do this just one time and have that work for all the life events. So using this trick, we can run 50 unique models with one set of model syntax!\n\nNote that we have to modify the function that calls `growth()` because we have a categorical outcome that needs a different estimator.\n\n```{r, eval = F}\nmod2 <- '\nle_value ~ i + age\nle_value ~ s\n'\n\nmod2 <- paste(mod, mod2, collapse = \"\\n\")\n\nlavaan_fun <- function(d){\n  m <- growth(\n    mod2\n    , data = d\n    , ordered = \"le_value\"\n    , estimator = 'WLSMV'\n    , missing = \"pairwise\"\n    , parallel = \"multicore\"\n  )\n  return(m)\n}\n\ngsoep_nested3 <- gsoep_nested3 %>%\n  mutate(m = map(data, lavaan_fun))\n# saveRDS(gsoep_nested3, \"gsoep_nested3.RDS\")\n```\n\n## Extract Results\n\nRemember how I said that the reason we created the `terms` tibble was because it would make `extract_fun()` more flexible / portable? Let's see that in action. We'll create a new one that lists only the key additional terms in this model and use that to extract those estimates.\n\n```{r}\ngsoep_nested3 <- readRDS(\"gsoep_nested3.RDS\")\nterms <- tribble(\n  ~path,     ~new,\n  \"le_value~i\", \"Intercept\",\n  \"le_value~s\", \"Slope\",\n  \"le_value~age\", \"Age\"\n)\n\ngsoep_nested3 <- gsoep_nested3 %>%\n  mutate(summary = map(m, extract_fun))\n```\n\n## Formatting Results\n\nNow we're ready to format the results using the same steps as before. Note that I'm combining all them here because we've already stepped through them slowly: 1. Remove other list columns and `unnest`() 2. Format the results using `format_fun()` 3. Remove unnecessary columns and `pivot_wider()` 4. Change the order of the columns 5. Factor the terms to help us arrange the rows 6. Sort the rows by Life Event and then term\n\n```{r}\nord <- paste(rep(c(\"E\", \"A\", \"C\", \"N\", \"O\"), each = 2), rep(c(\"est\", \"CI\"), times = 5), sep = \"_\")\ngsoep_tab2 <- gsoep_nested3 %>%\n  select(-data, -m) %>%\n  unnest(summary) %>%\n  format_fun() %>%\n  select(-ci.lower, -ci.upper, -sig, -pvalue) %>%\n  pivot_wider(\n    names_from = \"trait\"\n    , names_glue = \"{trait}_{.value}\"\n    , values_from = c(\"est\", \"CI\")\n  ) %>%\n  select(le, term, all_of(ord)) %>%\n  # select(starts_with(\"E\"), starts_with(\"A\"), starts_with(\"C\"), starts_with(\"N\"), starts_with(\"O\"))\n  mutate(term = factor(term, terms$new)) %>%\n  arrange(le, term)\ngsoep_tab2\n```\n\n## Kabling the Table\n\nNow we're ready to kable the table! This looks identical to before, but I'm going to show you one more trick using kableExtra::group_rows. Rather than having to count the rows and adding a bunch of manual calls, I always just use the data frame to count for me to avoid errors. Then, I can create the table and use a `for` loop to tack on all the grouped rows using that reference data frame. Much more flexible and less error prone!\n\n```{r}\nhdr <- c(1, rep(2,5))\nnames(hdr) <- c(\" \", \"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Neuroticism\", \"Openness\")\nrs <- gsoep_tab2 %>% group_by(le) %>% tally() %>% \n    mutate(end = cumsum(n), start = lag(end) + 1, start = ifelse(is.na(start), 1, start))\ngsoep_tab2_kable <- gsoep_tab2 %>%\n  select(-le) %>%\n  kable(.\n        , escape = F\n        , align = c(\"r\", rep(\"c\", 10))\n        , col.names = c(\"Term\", rep(c(\"<em>est.</em>\", \"CI\"), times = 5))\n        , caption = \"<strong>Table 3</strong><br><em>Big Five Personality Trait Trajectory Associations with Life Events from Latent Growth Models</em>\"\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\") %>%\n  add_header_above(hdr) %>%\n  add_footnote(label = \"Bold values indicate terms p < .05. \", notation = \"none\")\n\nfor(i in 1:nrow(rs)){\n    gsoep_tab2_kable <- gsoep_tab2_kable %>% kableExtra::group_rows(rs$le[i], rs$start[i], rs$end[i])\n}\ngsoep_tab2_kable\n```\n\n## Kabling lots of tables\n\nThe code below is a pretty chunk of code I use to generate tables for each model I run with all the model parameter estimates.\n\n```{r}\nall_term_tab_fun <- function(m, trait){\n  long_trait <- mapvalues(trait, p_names$old, p_names$new, warn_missing = F)\n  cap <- sprintf(\"<strong>Table SX</strong><br><em>Second Order Latent Growth Models of %s</em>\", long_trait)\n  note <- \"Bold values indicate estimates were significant at p < .05. est. = unstandardized estimate. \"\n  tab <- parameterEstimates(m, standardized = T) %>%\n    data.frame() %>%\n    format_fun() %>%\n    select(lhs:est, CI, pvalue) %>%\n    kable(.\n          , format = \"html\"\n          , escape = F\n          , align = c(\"r\", \"c\", \"l\", \"l\", rep(\"c\", 3))\n          , col.names = c(\"LHS\", \"op\", \"RHS\", \"label\", \"est.\", \"CI\", \"<em>p</em>\")\n          , caption = cap\n          ) %>%\n    kable_classic(full_width = F, html_font = \"Times\") %>%\n    footnote(note)\n  # save_kable(tab, file = sprintf(\"results/tables/all-terms/%s.html\", trait))\n  return(tab)\n}\n\ngsoep_nested2 <- gsoep_nested2 %>%\n  mutate(all_term_tab = map2(m, trait, all_term_tab_fun))\n```\n\n# Plots\n\nThe same functional, iterative approach also applies to creating plots! I teach a whole course on data visualization, so I'm not going to spend a ton of time on this. But I'll show you an example for basic R models and for lavaan.\n\n# Plots Example 1: Base R GLM\n\n## Set up the data frame\n\nFirst, we'll use some of what we learned in the `purrr` workshop to set ourselves up to be able to create these tables easily, using `group_by()` and `nest()` to create nested data frames for our target personality + outcome combinations. To do this, we'll also use what you learned about `filter()` and `mutate()`.\n\n```{r}\noutcomes <- tribble(\n  ~old, ~new,\n  \"chldbrth\", \"Child Birth\"\n  , \"divorced\", \"Divorced\"\n  , \"married\", \"Married\"\n  , \"mvInPrtner\", \"Move in with Partner\"\n)\n\ngsoep_nested4 <- gsoep %>%\n  mutate(age = age - 45) %>%\n  group_by(Trait, Outcome) %>%\n  nest() %>%\n  ungroup()\ngsoep_nested4\n```\n\n## Run Models\n\nNow, we'll run the models predicting outcomes from personality x age interactions (and their lower order terms)\n\n```{r}\nmod2_fun <- function(d){\n  d$o_value <- factor(d$o_value)\n  glm(o_value ~ p_value*age, data = d, family = binomial(link = \"logit\"))\n}\n\ngsoep_nested4 <- gsoep_nested4 %>%\n  mutate(m = map(data, mod2_fun))\n```\n\n## Generating Model Predictions\n\nThe next step is to get predictions from the model. It is also good practice to always get standard errors and/or confidence intervals of the estimates. Thankfully, with `lm()` and `glm()`, this is easy. Lavaan will be covered briefly and more complex model forms are covered in my data visualization class.\n\n```{r}\nglm_pred_fun <- function(m){\n  rng_p <- range(m$model$p_value, na.rm = T)\n  frame <- crossing(\n    age = c(-15, 0, 15)\n    , p_value = seq(rng_p[1], rng_p[2], length.out = 30)\n  )\n  \n  pred <- predict(m, newdata = frame, se.fit = T)[1:2]\n  frame <- frame %>% \n    mutate(fit = pred$fit\n           , se = pred$se.fit\n           , lower = fit - 1.96*se\n           , upper = fit + 1.96*se\n           ) %>%\n    mutate_at(vars(fit, lower, upper), exp)\n}\n```\n\nNow we run the predictions\n\n```{r}\ngsoep_nested4 <- gsoep_nested4 %>%\n  mutate(pred = map(m, glm_pred_fun))\ngsoep_nested4\n```\n\n## Plot the Predictions\n\nOne of the greatest strengths of list-columns and using `purrr` is what I think of as the data accordion. With list-columns, I can `unnest()` whatever level the models were estimated at and then reaggregate using whatever combinations / groups make the most sense.\n\nSo here, for example, we ran models for all Big Five trait (5) x life outcome (10) combinations. But having 50 separate plots would be a little silly, so we'd much rather create one for each outcome. But I'm not going to write that code 10 times because that'd be a waste. So instead if I reaggregate the data, I can then use a function + `purrr` to generate each of the plots.\n\n```{r}\ngsoep_plot4 <- gsoep_nested4 %>%\n  select(-data, -m) %>%\n  unnest(pred) %>%\n  group_by(Outcome) %>%\n  nest() %>%\n  ungroup()\ngsoep_plot4\n```\n\nWhen we write code for plots in ggplot, we have a lot of things that we end up writing over and over, especially for theme elements. To get around this, I use this little function to modify all of my theme elements in a single line.\n\n```{r}\nmy_theme <- function(){\n  theme_classic() + \n  theme(\n    legend.position = \"bottom\"\n    , legend.title = element_text(face = \"bold\", size = rel(1))\n    , legend.text = element_text(face = \"italic\", size = rel(1))\n    , axis.text = element_text(face = \"bold\", size = rel(1.1), color = \"black\")\n    , axis.title = element_text(face = \"bold\", size = rel(1.2))\n    , panel.grid.major = element_line(color = \"grey90\", linewidth = .2)\n    , plot.title = element_text(face = \"bold\", size = rel(1.2), hjust = .5)\n    , plot.subtitle = element_text(face = \"italic\", size = rel(1.2), hjust = .5)\n    , strip.text = element_text(face = \"bold\", size = rel(1.1), color = \"white\")\n    , strip.background = element_rect(fill = \"black\")\n    )\n}\n```\n\nLike I mentioned, once we reaggregate the data, we can then write a function to generate the plot for each outcome separately. Let's start by building the basic code for the plot. We need it to: 1. Have x = personality, y = predicted values, separate lines for moderator levels, and separate panels for different traits 2. A line indicating personality-outcome associations (again separately across age groups) and a ribbon with the confidence interval around the prediction\n\n```{r}\nplot_fun <- function(d){\n  d %>%\n    mutate(age_fac = factor(age, c(-15, 0, 15), c(\"30\", \"45\", \"60\"))) %>%\n    ggplot(aes(x = p_value, y = fit, color = age_fac)) +\n    geom_ribbon(\n      aes(ymin = lower, ymax = upper, fill = age_fac)\n      , alpha = .4\n    ) + \n    geom_line() + \n    facet_wrap(~Trait, scales = \"free_x\") + \n    my_theme()\n}\n```\n\nWith our reaggregated data, we can easily just run the plots and view the results. This is getting there, but let's make some additional modifications to get them publication ready.\n\n```{r}\ngsoep_plot4 <- gsoep_plot4 %>%\n  mutate(p = map(data, plot_fun))\ngsoep_plot4\ngsoep_plot4$p[[1]]\n```\n\nLet's add: 1. full personality trait names 2. the outcome in the title 3. better axis and other labels\n\n```{r}\nplot_fun <- function(d, outcome){\n  out <- mapvalues(outcome, outcomes$old, outcomes$new, warn_missing = F)\n  d %>%\n    mutate(age_fac = factor(age, c(-15, 0, 15), c(\"30\", \"45\", \"60\"))\n           , Trait = factor(Trait, p_names$old, p_names$new)) %>%\n    ggplot(aes(x = p_value, y = fit, color = age_fac)) +\n    geom_ribbon(\n      aes(ymin = lower, ymax = upper, fill = age_fac)\n      , alpha = .4\n    ) + \n    geom_line() + \n    labs(\n      x = \"Personality Trait Level\"\n      , y = \"Predicted Value (CI)\"\n      , color = \"Age\"\n      , fill = \"Age\"\n      , title = sprintf(\"%s: Personality x Age Interaction\", out)\n    ) + \n    facet_wrap(~Trait, scales = \"free_x\") + \n    my_theme() + \n    theme(\n      legend.position = c(.8, .25)\n      \n    )\n  # ggsave(filename = sprintf(\"results/figures/%s.png\", outcome), width = 6, height = 6)\n  # ggsave(filename = sprintf(\"results/figures/%s.pdf\", outcome), width = 6, height = 6)\n}\n```\n\nNow, we can run these and get the results.\n\n```{r}\ngsoep_plot4 <- gsoep_plot4 %>%\n  mutate(p = map2(data, Outcome, plot_fun))\ngsoep_plot4\ngsoep_plot4$p[[1]]\n```\n\n## Lavaan Trajectories\n\nLavaan is trickier. Because of the way residual variance, etc. is calculted in SEM, it's much less straightforward to estimate standard errors and confidence intervals of predictions (not to mention that lavaan just doesn't make predictions easy). So instead, I just use some basic matrix algebra and bootstrapping to get these. If you aren't comfortable with basic matrix algebra, you can just write a little function that uses terms individually.\n\n```{r, eval = F}\nlavaan_pred_fun <- function(m){\n  coef <- coef(gsoep_nested3$m[[1]])[c(\"i~1\", \"s~1\")]\n  b <- bootstrapLavaan(m, parallel = \"multicore\", R = 100)\n  frame <- tibble(intercept = 1, wave = seq(-1,1, length.out = 50))\n  pred_boot <- function(x,y) bind_cols(frame, pred = as.vector(as.matrix(frame) %*% c(x,y)))\n  frame <- frame %>%\n    mutate(pred = as.vector(as.matrix(frame) %*% coef)) %>%\n    left_join(\n      tibble(sample = 1:100, pred = map2(b[,\"i~1\"], b[,\"s~1\"], pred_boot)) %>%\n        unnest(pred) %>%\n        group_by(wave) %>%\n        summarize(lower = quantile(pred, probs = .025)\n                  , upper = quantile(pred, probs = .975))\n    )\n}\n\ngsoep_nested2 <- gsoep_nested2 %>%\n  mutate(pred = map(m, lavaan_pred_fun))\n# saveRDS(gsoep_nested2, file = \"gsoep_nested2.RDS\")\n```\n\n```{r, echo = F}\ngsoep_nested2 <- readRDS(\"gsoep_nested2.RDS\")\n```\n\nOnce we've estimated the predictions, we can graph them just like we did for the previous example. (Again, a full discussion of plotting is beyond the scope of this course and is covered for a whole quarter in my data visualization class instead.)\n\n```{r}\ngsoep_nested2 %>%\n  select(-data, -m, -summary) %>%\n  unnest(pred) %>%\n  ggplot(aes(x = wave + 1, y = pred)) + \n    geom_ribbon(\n      aes(ymin = lower, ymax = upper, fill = trait)\n      , alpha = .4\n    ) + \n    geom_line() + \n    scale_y_continuous(limits = c(1,7), breaks = seq(1,7,1)) + \n    scale_x_continuous(limits = c(-.25,2.25), breaks = 0:2, labels = c(2005, 2009, 2013)) + \n    labs(\n        x = \"Personality Trait Level\"\n        , y = \"Predicted Value (CI)\"\n        , title = sprintf(\"Personality Trajectories\")\n      ) + \n    facet_wrap(~trait, scales = \"free_x\") + \n    my_theme() + \n    theme(legend.position = \"none\")\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":true,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"highlight-style":"tango","self-contained":true,"output-file":"08-week8-workbook.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"cosmo","title":"Week 8 Workbook","author":"Emorie D Beck","editor_options":{"chunk_output_type":"console"},"code-copy":true,"toc-float":true},"extensions":{"book":{"multiFile":true}}}}}