[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Cleaning and Management (Fall 2023)",
    "section": "",
    "text": "Mondays, 2:10-5 PM (October 2-December 4, 2023)\n166 Young Hall\nPsychology Department\nUniversity of California, Davis"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Data Cleaning and Management (Fall 2023)",
    "section": "Course Description",
    "text": "Course Description\n\n\n\nIn graduate education, training on research (and statistical) methods and conceptual frameworks far outpaces training on key technical skills that underpin all research, empirical or otherwise. On average, researchers spend about 80% of their (analytic) time on data cleaning, but we spend comparatively little teaching those skills. This course aims to fill that gap by helping researchers to (1) build their reproducible research workflow and (2) improve their data cleaning and general statistical programming skills. To that end, each session will be split to address each of these goals, with the beginning of class focused on conceptual ideas about best practices in building a workflow and the latter half focused on technical training on programming and cleaning data in R. This course will be set up as a “bring your own data” course to allow students to anticipate specific challenges that face different types of research. \nThis course is not a “pure” data science (i.e. we won’t be working with databases, etc.) because it focuses on the skills and tools most common within the social sciences. Science is a collaborative enterprise, and these tools are widely used among many social scientists, which promotes an open, equitable workflow by using tools available and most commonly used by the majority of our peers."
  },
  {
    "objectID": "index.html#navigating-this-site",
    "href": "index.html#navigating-this-site",
    "title": "Data Cleaning and Management (Fall 2023)",
    "section": "Navigating This Site:",
    "text": "Navigating This Site:\n1. Weekly assignments are under Problem Sets. These are due at 12:01 AM on the day of class. \n2. Reading list (and links) and links to workshop slides are under Schedule and on Canvas. I recommend bookmarking this site to allow you access to all materials in perpetuity.\n3. Final Project Information will be under Final Project. The proposal instructions (Due November 19 at 11:59 PM PST) is now posted. More details about the final project will be posted by November 7, and a rubric for the final project will be posted by November 28.\n4. The most updated version of the syllabus will be on the Syllabus page and can be downloaded there as well."
  },
  {
    "objectID": "index.html#course-zoom-link",
    "href": "index.html#course-zoom-link",
    "title": "Data Cleaning and Management (Fall 2023)",
    "section": "Course Zoom Link",
    "text": "Course Zoom Link\nThis course is in person, but you may access it on Zoom due to illness, exposure, travel, etc."
  },
  {
    "objectID": "03-week3-workbook.html",
    "href": "03-week3-workbook.html",
    "title": "Week 3 Workbook",
    "section": "",
    "text": "Codelibrary(knitr)\nlibrary(psych)\nlibrary(plyr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.3\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()     masks psych::%+%()\n✖ ggplot2::alpha()   masks psych::alpha()\n✖ dplyr::arrange()   masks plyr::arrange()\n✖ purrr::compact()   masks plyr::compact()\n✖ dplyr::count()     masks plyr::count()\n✖ dplyr::desc()      masks plyr::desc()\n✖ dplyr::failwith()  masks plyr::failwith()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::id()        masks plyr::id()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::mutate()    masks plyr::mutate()\n✖ dplyr::rename()    masks plyr::rename()\n✖ dplyr::summarise() masks plyr::summarise()\n✖ dplyr::summarize() masks plyr::summarize()"
  },
  {
    "objectID": "03-week3-workbook.html#outline",
    "href": "03-week3-workbook.html#outline",
    "title": "Week 3 Workbook",
    "section": "Outline",
    "text": "Outline\n\nWelcome & Q’s on homework\nPart 1: Data Quality and Descriptives\nPart 2: tidyr\n\nProblem set & Q time"
  },
  {
    "objectID": "03-week3-workbook.html#what-is-data-quality",
    "href": "03-week3-workbook.html#what-is-data-quality",
    "title": "Week 3 Workbook",
    "section": "What is data quality",
    "text": "What is data quality\nIBM’s definition of data quality:\n\n“Data quality measures how well a dataset meets criteria for accuracy, completeness, validity, consistency, uniqueness, timeliness, and fitness for purpose”"
  },
  {
    "objectID": "03-week3-workbook.html#aspects-of-data-quality",
    "href": "03-week3-workbook.html#aspects-of-data-quality",
    "title": "Week 3 Workbook",
    "section": "Aspects of data quality",
    "text": "Aspects of data quality\n\n\nAccuracy: Do the data reflect reality / truth?\n\nCompleteness: Are the data usable or complete (no missing people, values, etc. beyond random)\n\n\nUniqueness: There is no duplicated data\n\nValidity: Do the data have the correct properties (values, ranges, etc.)\n\nConsistency: When integrating across multiple data sources, information should converge across sources and match reality\n\nTimeliness: Can the data be maintained and distributed within a specified time frame\n\nFitness for purpose: Do the data meet your research need?"
  },
  {
    "objectID": "03-week3-workbook.html#why-should-we-care-about-data-quality",
    "href": "03-week3-workbook.html#why-should-we-care-about-data-quality",
    "title": "Week 3 Workbook",
    "section": "Why should we care about data quality",
    "text": "Why should we care about data quality\n\nYou aren’t responsible for poor quality data you receive, but you are responsible for the data products you work with – that is, you are responsible for improving data quality\nPoor quality data threatens scientific integrity\n\nPoor quality data are a pain for you to work with and for others to work with"
  },
  {
    "objectID": "03-week3-workbook.html#what-can-data-quality-do-for-my-career",
    "href": "03-week3-workbook.html#what-can-data-quality-do-for-my-career",
    "title": "Week 3 Workbook",
    "section": "What can data quality do for my career?",
    "text": "What can data quality do for my career?\n\nThe virtuous cycle of data cleaning\n\nSome people get a reputation for getting their data, analyses, etc. right\nThis is important for publications, grant funding, etc.\nIt tends to be inter-generational – you inherit some of your reputation on this from your advisor\nStart paying it forward now to build your own career, whether it’s in academia or industry"
  },
  {
    "objectID": "03-week3-workbook.html#how-do-i-ensure-data-quality",
    "href": "03-week3-workbook.html#how-do-i-ensure-data-quality",
    "title": "Week 3 Workbook",
    "section": "How do I ensure data quality?",
    "text": "How do I ensure data quality?\nThe Towards Data Science website has a nice definition of EDA:\n\n“Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics”\n\n\nSo EDA is basically a fancy word for the descriptive statistics you’ve been learning about for years\n\nI think about “exploratory data analysis for data quality”\n\nInvestigating values and patterns of variables from “input data”\nIdentifying and cleaning errors or values that need to be changed\nCreating analysis variables\nChecking values of analysis variables against values of input variables"
  },
  {
    "objectID": "03-week3-workbook.html#how-i-will-teach-exploratory-data-analysis",
    "href": "03-week3-workbook.html#how-i-will-teach-exploratory-data-analysis",
    "title": "Week 3 Workbook",
    "section": "How I will teach exploratory data analysis",
    "text": "How I will teach exploratory data analysis\nWill teach exploratory data analysis (EDA) in two sub-sections:\n\nProvide “Guidelines for EDA”\n\nLess about coding, more about practices you should follow and mentality necessary to ensure high data quality\n\n\nIntroduce “Tools of EDA”:\n\nDemonstrate code to investigate variables and relationship between variables\nMost of these tools are just the application of programming skills you have already learned (or will learn soon!)"
  },
  {
    "objectID": "03-week3-workbook.html#guidelines-for-eda-for-data-quality",
    "href": "03-week3-workbook.html#guidelines-for-eda-for-data-quality",
    "title": "Week 3 Workbook",
    "section": "Guidelines for “EDA for data quality”",
    "text": "Guidelines for “EDA for data quality”\nAssume that your goal in “EDA for data quality” is to investigate “input” data sources and create “analysis variables”\n\nUsually, your analysis dataset will incorporate multiple sources of input data, including data you collect (primary data) and/or data collected by others (secondary data)\n\nEDA is not a linear process, and the process will vary across people and projects Some broad steps:\n\nUnderstand how input data sources were created\n\ne.g., when working with survey data, have survey questionnaire and codebooks on hand (watch out for skip patterns!!!)\n\n\nFor each input data source, identify the “unit of analysis” and which combination of variables uniquely identify observations\nInvestigate patterns in input variables\nCreate analysis variable from input variable(s)\nVerify that analysis variable is created correctly through descriptive statistics that compare values of input variable(s) against values of the analysis variable\n\n\nIt is critically important to step through EDA processes at multiple points during data cleaning, from the input / raw data to the output / analysis / clean data.\nAlways be aware of missing values\nThey will not always be coded as NA in input variables (e.g., some projects code them as 99, 99999, negative values, etc.)"
  },
  {
    "objectID": "03-week3-workbook.html#unit-of-analysis-and-which-variables-uniquely-identify-observations",
    "href": "03-week3-workbook.html#unit-of-analysis-and-which-variables-uniquely-identify-observations",
    "title": "Week 3 Workbook",
    "section": "“Unit of analysis” and which variables uniquely identify observations",
    "text": "“Unit of analysis” and which variables uniquely identify observations\n“Unit of analysis” refers to “what does each observation represent” in an input data source\n\nIf each obs represents a trial in an experiment, you have “trial level data”\nIf each obs represents a participant, you have “participant level data”\nIf each obs represents a sample, you have “sample-level data”\nIf each obs represents a year, you have “year level data” (i.e. longitudinal)\n\nHow to identify unit of analysis\n\ndata documentation\ninvestigating the data set\nThis is very important because we often conduct analyses that span multiple units of analysis (e.g., between- v within-person, person- v stimuli-level, etc.)\nWe have to be careful and thoughtful about identifiers that let us do that (important for joining data together, which will be the focus on our R workshop today)"
  },
  {
    "objectID": "03-week3-workbook.html#rules-for-creating-new-variables",
    "href": "03-week3-workbook.html#rules-for-creating-new-variables",
    "title": "Week 3 Workbook",
    "section": "Rules for creating new variables",
    "text": "Rules for creating new variables\nRules I follow for variable creation\n\nNever modify “input variable”; instead create new variable based on input variable(s)\n\nAlways keep input variables used to create new variables\n\n\nInvestigate input variable(s) and relationship between input variables\nDeveloping a plan for creation of analysis variable\n\ne.g., for each possible value of input variables, what should value of analysis variable be?\n\n\nWrite code to create analysis variable\nRun descriptive checks to verify new variables are constructed correctly\n\nCan “comment out” these checks, but don’t delete them\n\n\nDocument new variables with notes and labels"
  },
  {
    "objectID": "03-week3-workbook.html#data-we-will-use",
    "href": "03-week3-workbook.html#data-we-will-use",
    "title": "Week 3 Workbook",
    "section": "Data we will use",
    "text": "Data we will use\nUse read_csv() function from readr (loaded with tidyverse) to import .csv dataset into R.\n\nCodelibrary(plyr)\nlibrary(tidyverse)\nsoep_long <- read_csv(file=\"https://github.com/emoriebeck/psc290-data-FQ23/raw/main/04-workshops/03-week3-tidyr/gsoep.csv\")\nsoep_long\n\n\nLet’s examine the data [you must run this code chunk]\n\nCodesoep_long %>% names()\nsoep_long %>% names() %>% str()\n\nstr(soep_long) # ugh\n\nstr(soep_long$LifeEvent__Married)\nattributes(soep_long$LifeEvent__Married)\ntypeof(soep_long$LifeEvent__Married)\nclass(soep_long$LifeEvent__Married)"
  },
  {
    "objectID": "03-week3-workbook.html#rule-1",
    "href": "03-week3-workbook.html#rule-1",
    "title": "Week 3 Workbook",
    "section": "Rule 1",
    "text": "Rule 1\n\nNever modify “input variable”; instead create new variable based on input variable(s)\n\nAlways keep input variables used to create new variables\n\n\n\n\nI already did this before the data were loaded in. I renamed all the input variables with interpretable names and reshaped them so the time variable (year) is long and the other variables are wide"
  },
  {
    "objectID": "03-week3-workbook.html#rule-2",
    "href": "03-week3-workbook.html#rule-2",
    "title": "Week 3 Workbook",
    "section": "Rule 2",
    "text": "Rule 2\n\nInvestigate input variable(s) and relationship between input variables\n\n\nWe’ll talk more about this in a bit when we discuss different kinds of descriptives, but briefly let’s look at basic descriptives + zero-order correlations\n\n\nCodedescribe(soep_long)\n\n\n\n  \n\n\n\nThis doesn’t look great because we’ve negative values where we shouldn’t, which represent flags for different kinds of missing variables. We’ll have to fix that\n\nI’ll show you a better way later, but we haven’t learned everything to do it nicely yet. So instead, we’ll use cor.plot() from the psych package to make a simple heat map of the correlations.\nWe shouldn’t see that many negative correlations, which flags that we need to reverse score some items\n\n\nCodesoep_2005 <- soep_long %>% filter(year == 2005) %>% select(-year)\ncor.plot(soep_2005, diag = F)"
  },
  {
    "objectID": "03-week3-workbook.html#rule-3",
    "href": "03-week3-workbook.html#rule-3",
    "title": "Week 3 Workbook",
    "section": "Rule 3",
    "text": "Rule 3\n\nDeveloping a plan for creation of analysis variable\n\ne.g., for each possible value of input variables, what should value of analysis variable be?\n\n\n\n\nI do this in my codebooks, and this topic warrants a discussion in itself. This is our focal topic for next week!\nIn this case, we want Big Five (EACNO) composites for each wave and to create composites of life events experienced across all years"
  },
  {
    "objectID": "03-week3-workbook.html#rule-4",
    "href": "03-week3-workbook.html#rule-4",
    "title": "Week 3 Workbook",
    "section": "Rule 4",
    "text": "Rule 4\n\nWrite code to create analysis variable\n\n\nFrom Rule 2, we know we need to recode missing values to NA and reverse code some items. From Rule 3, we know we need to create some composites.\nLet’s do that now!\n\nRecoding:\n\nCodesoep_long <- soep_long %>%\n  mutate_at(\n    vars(contains(\"Big5\"))\n    , ~ifelse(. < 0 | is.na(.), NA, .)\n    ) %>%\n  mutate_at(\n    vars(contains(\"LifeEvent\"))\n    , ~mapvalues(., seq(-7,1), c(rep(NA, 5), 0, NA, NA, 1), warn_missing = F)\n    )\n\n\nReverse Coding:\n\nCoderev_code <- c(\"Big5__A_coarse\", \"Big5__C_lazy\", \"Big5__E_reserved\", \"Big5__N_dealStress\")\nsoep_long <- soep_long %>%\n  mutate_at(\n    vars(all_of(rev_code))\n    , ~as.numeric(reverse.code(., keys = -1, mini = 1, maxi = 7))\n    )\n\n\nLet’s check to make sure some correlations just reversed:\n\nCodesoep_2005 <- soep_long %>% filter(year == 2005) %>% select(-year)\ncor.plot(soep_2005, diag = F)\n\n\n\n\nCreate Composites:\n(Note: I honestly wouldn’t normally do it like this, but we haven’t learned how to reshape data yet! Check the online materials for code on how to do this)\n\nCodesoep_long <- soep_long %>% \n  group_by(year, Procedural__SID) %>%\n  rowwise() %>%\n  mutate(\n    Big5__E = mean(cbind(Big5__E_reserved, Big5__E_communic, Big5__E_sociable), na.rm = T),\n    Big5__A = mean(cbind(Big5__A_coarse, Big5__A_friendly, Big5__A_forgive), na.rm = T),\n    Big5__C = mean(cbind(Big5__C_thorough, Big5__C_efficient, Big5__C_lazy), na.rm = T),\n    Big5__N = mean(cbind(Big5__N_worry, Big5__N_nervous, Big5__N_dealStress), na.rm = T),\n    Big5__O = mean(cbind(Big5__O_original, Big5__O_artistic, Big5__O_imagin), na.rm = T)) %>%\n  group_by(Procedural__SID) %>%\n  mutate_at(\n    vars(contains(\"LifeEvent\"))\n    , lst(ever = ~max(., na.rm = T))\n    ) %>%\n  ungroup() %>%\n  filter(year %in% c(2005, 2009, 2013))"
  },
  {
    "objectID": "03-week3-workbook.html#rule-5",
    "href": "03-week3-workbook.html#rule-5",
    "title": "Week 3 Workbook",
    "section": "Rule 5",
    "text": "Rule 5\n\nRun descriptive checks to verify new variables are constructed correctly\n\nCan “comment out” these checks, but don’t delete them\n\n\n\n\nCodesoep_long %>% \n  select(Big5__E:LifeEvent__SepPart_ever) %>%\n  describe()\n\n\n\n  \n\n\n\n\nUh oh, Inf values popping up what went wrong?\n\n-Inf pops up when there were no non-missing values and you use na.rm = T\n\nLet’s recode those as NA\n\n\n\nCodesoep_long <- soep_long %>%\n  mutate_all(~ifelse(is.infinite(.) | is.nan(.), NA, .))\n\n\nAnd check out the descriptives again\n\nCodesoep_long %>% \n  select(Big5__E:LifeEvent__SepPart_ever) %>%\n  describe()\n\n\n\n  \n\n\n\n\nCodesoep_long %>%\n  filter(year == 2005) %>%\n  select(Big5__E:LifeEvent__SepPart_ever) %>%\n  cor.plot(., diag = F)"
  },
  {
    "objectID": "03-week3-workbook.html#rule-6",
    "href": "03-week3-workbook.html#rule-6",
    "title": "Week 3 Workbook",
    "section": "Rule 6",
    "text": "Rule 6\n\nDocument new variables with notes and labels\n\n\nAgain, I do this in my codebooks, so more on this next week!!"
  },
  {
    "objectID": "03-week3-workbook.html#eda",
    "href": "03-week3-workbook.html#eda",
    "title": "Week 3 Workbook",
    "section": "EDA",
    "text": "EDA\n\n\nOne-way descriptive analyses (i.e,. focus on one variable)\n\nDescriptive analyses for continuous variables\nDescriptive analyses for discreet/categorical variables\n\n\n\nTwo-way descriptive analyses (relationship between two variables)\n\nCategorical by categorical\nCategorical by continuous\nContinuous by continuous\n\n\nRealistically, we’ve actually already covered all this above, so we’ll loop back to this after learning tidyr\n\n\n\n\n\n\n\nData Wrangling in tidyr\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/master/thumbs/tidyr.png\")"
  },
  {
    "objectID": "03-week3-workbook.html#pivot_longer",
    "href": "03-week3-workbook.html#pivot_longer",
    "title": "Week 3 Workbook",
    "section": "1. pivot_longer()\n",
    "text": "1. pivot_longer()\n\n\n(Formerly gather()) Makes wide data long, based on a key \n\nCore arguments:\n\n\ndata: the data, blank if piped\n\ncols: columns to be made long, selected via select() calls\n\nnames_to: name(s) of key column(s) in new long data frame (string or string vector)\n\nvalues_to: name of values in new long data frame (string)\n\nnames_sep: separator in column headers, if multiple keys\n\nvalues_drop_na: drop missing cells (similar to na.rm = T) \n\n\n\nBasic Application\nLet’s start with an easy one – one key, one value:\n\nCodebfi %>%\n  rownames_to_column(\"SID\") %>%\n  pivot_longer(\n    cols = A1:O5\n    , names_to = \"item\"\n    , values_to = \"values\"\n    , values_drop_na = T\n  ) %>%\n  print(n = 8)\n\n# A tibble: 69,492 × 6\n  SID   gender education   age item  values\n  <chr>  <int>     <int> <int> <chr>  <int>\n1 61617      1        NA    16 A1         2\n2 61617      1        NA    16 A2         4\n3 61617      1        NA    16 A3         3\n4 61617      1        NA    16 A4         4\n5 61617      1        NA    16 A5         4\n6 61617      1        NA    16 C1         2\n7 61617      1        NA    16 C2         3\n8 61617      1        NA    16 C3         3\n# ℹ 69,484 more rows\n\n\nMore Advanced Application\nNow a harder one – two keys, one value:\n\nCodebfi %>%\n  rownames_to_column(\"SID\") %>%\n  pivot_longer(\n    cols = A1:O5\n    , names_to = c(\"trait\", \"item_num\")\n    , names_sep = -1\n    , values_to = \"values\"\n    , values_drop_na = T\n  ) %>%\n  print(n = 8)\n\n# A tibble: 69,492 × 7\n  SID   gender education   age trait item_num values\n  <chr>  <int>     <int> <int> <chr> <chr>     <int>\n1 61617      1        NA    16 A     1             2\n2 61617      1        NA    16 A     2             4\n3 61617      1        NA    16 A     3             3\n4 61617      1        NA    16 A     4             4\n5 61617      1        NA    16 A     5             4\n6 61617      1        NA    16 C     1             2\n7 61617      1        NA    16 C     2             3\n8 61617      1        NA    16 C     3             3\n# ℹ 69,484 more rows"
  },
  {
    "objectID": "03-week3-workbook.html#pivot_wider",
    "href": "03-week3-workbook.html#pivot_wider",
    "title": "Week 3 Workbook",
    "section": "2. pivot_wider()\n",
    "text": "2. pivot_wider()\n\n\n(Formerly spread()) Makes wide data long, based on a key \n\nCore arguments:\n\n\ndata: the data, blank if piped\n\nnames_from: name(s) of key column(s) in new long data frame (string or string vector)\n\nnames_sep: separator in column headers, if multiple keys\n\nnames_glue: specify multiple or custom separators of multiple keys\n\nvalues_from: name of values in new long data frame (string)\n\nvalues_fn: function applied to data with duplicate labels \n\n\n\nBasic Application\n\nCodebfi_long <- bfi %>%\n  rownames_to_column(\"SID\") %>%\n  pivot_longer(\n    cols = A1:O5\n    , names_to = \"item\"\n    , values_to = \"values\"\n    , values_drop_na = T\n  )\n\n\nMore Advanced\n\nCodebfi_long <- bfi %>%\n  rownames_to_column(\"SID\") %>%\n  pivot_longer(\n    cols = A1:O5\n    , names_to = c(\"trait\", \"item_num\")\n    , names_sep = -1\n    , values_to = \"values\"\n    , values_drop_na = T\n  )\n\n\n\nCodebfi_long %>%\n  pivot_wider(\n    names_from = c(\"trait\", \"item_num\")\n    , values_from = \"values\"\n    , names_sep = \"_\"\n  )\n\n\n\n  \n\n\n\nA Little More Advanced\n\nCodebfi_long %>%\n  select(-item_num) %>%\n  pivot_wider(\n    names_from = \"trait\"\n    , values_from = \"values\"\n    , names_sep = \"_\"\n    , values_fn = mean\n  )"
  },
  {
    "objectID": "03-week3-workbook.html#the-_join-functions",
    "href": "03-week3-workbook.html#the-_join-functions",
    "title": "Week 3 Workbook",
    "section": "The _join() Functions",
    "text": "The _join() Functions\n\nOften we may need to pull different data from different sources\nThere are lots of reasons to need to do this\nWe don’t have time to get into all the use cases here, so we’ll talk about them in high level terms\n\nWe’ll focus on:\n\nfull_join()\ninner_join()\nleft_join()\nright_join()\n\n\nLet’s separate demographic and BFI data\n\n\nCodebfi_only <- bfi %>% \n  rownames_to_column(\"SID\") %>%\n  select(SID, matches(\"[0-9]\"))\nbfi_only %>% as_tibble() %>% print(n = 6)\n\n# A tibble: 2,800 × 26\n  SID      A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\n  <chr> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 61617     2     4     3     4     4     2     3     3     4     4     3     3\n2 61618     2     4     5     2     5     5     4     4     3     4     1     1\n3 61620     5     4     5     4     4     4     5     4     2     5     2     4\n4 61621     4     4     6     5     5     4     4     3     5     5     5     3\n5 61622     2     3     3     4     5     4     4     5     3     2     2     2\n6 61623     6     6     5     6     5     6     6     6     1     3     2     1\n# ℹ 2,794 more rows\n# ℹ 13 more variables: E3 <int>, E4 <int>, E5 <int>, N1 <int>, N2 <int>,\n#   N3 <int>, N4 <int>, N5 <int>, O1 <int>, O2 <int>, O3 <int>, O4 <int>,\n#   O5 <int>\n\n\n\nCodebfi_dem <- bfi %>%\n  rownames_to_column(\"SID\") %>%\n  select(SID, education, gender, age)\nbfi_dem %>% as_tibble() %>% print(n = 6)\n\n# A tibble: 2,800 × 4\n  SID   education gender   age\n  <chr>     <int>  <int> <int>\n1 61617        NA      1    16\n2 61618        NA      2    18\n3 61620        NA      2    17\n4 61621        NA      2    17\n5 61622        NA      1    17\n6 61623         3      2    21\n# ℹ 2,794 more rows\n\n\nBefore we get into it, as a reminder, this is what the data set looks like before we do any joining:\n\nCodebfi %>%\n  rownames_to_column(\"SID\") %>%\n  as_tibble() %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 29\n  SID      A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\n  <chr> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 61617     2     4     3     4     4     2     3     3     4     4     3     3\n2 61618     2     4     5     2     5     5     4     4     3     4     1     1\n3 61620     5     4     5     4     4     4     5     4     2     5     2     4\n4 61621     4     4     6     5     5     4     4     3     5     5     5     3\n5 61622     2     3     3     4     5     4     4     5     3     2     2     2\n6 61623     6     6     5     6     5     6     6     6     1     3     2     1\n# ℹ 2,794 more rows\n# ℹ 16 more variables: E3 <int>, E4 <int>, E5 <int>, N1 <int>, N2 <int>,\n#   N3 <int>, N4 <int>, N5 <int>, O1 <int>, O2 <int>, O3 <int>, O4 <int>,\n#   O5 <int>, gender <int>, education <int>, age <int>"
  },
  {
    "objectID": "03-week3-workbook.html#full_join",
    "href": "03-week3-workbook.html#full_join",
    "title": "Week 3 Workbook",
    "section": "3. full_join()\n",
    "text": "3. full_join()\n\nMost simply, we can put those back together keeping all observations.\n\nCodebfi_only %>%\n  full_join(bfi_dem) %>%\n  as_tibble() %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 29\n  SID      A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\n  <chr> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 61617     2     4     3     4     4     2     3     3     4     4     3     3\n2 61618     2     4     5     2     5     5     4     4     3     4     1     1\n3 61620     5     4     5     4     4     4     5     4     2     5     2     4\n4 61621     4     4     6     5     5     4     4     3     5     5     5     3\n5 61622     2     3     3     4     5     4     4     5     3     2     2     2\n6 61623     6     6     5     6     5     6     6     6     1     3     2     1\n# ℹ 2,794 more rows\n# ℹ 16 more variables: E3 <int>, E4 <int>, E5 <int>, N1 <int>, N2 <int>,\n#   N3 <int>, N4 <int>, N5 <int>, O1 <int>, O2 <int>, O3 <int>, O4 <int>,\n#   O5 <int>, education <int>, gender <int>, age <int>"
  },
  {
    "objectID": "03-week3-workbook.html#inner_join",
    "href": "03-week3-workbook.html#inner_join",
    "title": "Week 3 Workbook",
    "section": "4. inner_join()\n",
    "text": "4. inner_join()\n\nWe can also keep all rows present in both data frames\n\nCodebfi_dem %>%\n  filter(row_number() %in% 1:1700) %>%\n  inner_join(\n    bfi_only %>%\n      filter(row_number() %in% 1200:2800)\n  ) %>%\n  as_tibble() %>%\n  print(n = 6)\n\n# A tibble: 501 × 29\n  SID   education gender   age    A1    A2    A3    A4    A5    C1    C2    C3\n  <chr>     <int>  <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 64151         3      2    18     1     5     6     5     5     5     6     5\n2 64152         4      2    29     1     5     6     5     5     2     1     4\n3 64154         5      1    46     2     5     6     5     6     6     6     6\n4 64155         5      1    58     5     4     4     4     5     4     4     5\n5 64156         5      2    38     1     4     6     6     6     4     4     5\n6 64158         5      2    27     2     3     1     1     1     4     2     2\n# ℹ 495 more rows\n# ℹ 17 more variables: C4 <int>, C5 <int>, E1 <int>, E2 <int>, E3 <int>,\n#   E4 <int>, E5 <int>, N1 <int>, N2 <int>, N3 <int>, N4 <int>, N5 <int>,\n#   O1 <int>, O2 <int>, O3 <int>, O4 <int>, O5 <int>"
  },
  {
    "objectID": "03-week3-workbook.html#left_join",
    "href": "03-week3-workbook.html#left_join",
    "title": "Week 3 Workbook",
    "section": "5. left_join()\n",
    "text": "5. left_join()\n\nOr all rows present in the left (first) data frame, perhaps if it’s a subset of people with complete data\n\nCodebfi_dem %>%\n  drop_na() %>%\n  left_join(bfi_only) %>%\n  as_tibble() %>%\n  print(n = 6)\n\n# A tibble: 2,577 × 29\n  SID   education gender   age    A1    A2    A3    A4    A5    C1    C2    C3\n  <chr>     <int>  <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 61623         3      2    21     6     6     5     6     5     6     6     6\n2 61629         2      1    19     4     3     1     5     1     3     2     4\n3 61630         1      1    19     4     3     6     3     3     6     6     3\n4 61634         1      1    21     4     4     5     6     5     4     3     5\n5 61640         1      1    17     4     5     2     2     1     5     5     5\n6 61661         5      1    68     1     5     6     5     6     4     3     2\n# ℹ 2,571 more rows\n# ℹ 17 more variables: C4 <int>, C5 <int>, E1 <int>, E2 <int>, E3 <int>,\n#   E4 <int>, E5 <int>, N1 <int>, N2 <int>, N3 <int>, N4 <int>, N5 <int>,\n#   O1 <int>, O2 <int>, O3 <int>, O4 <int>, O5 <int>"
  },
  {
    "objectID": "03-week3-workbook.html#right_join",
    "href": "03-week3-workbook.html#right_join",
    "title": "Week 3 Workbook",
    "section": "6. right_join()\n",
    "text": "6. right_join()\n\nOr all rows present in the right (second) data frame, such as I do when I join a codebook with raw data\n\nCodebfi_dem %>%\n  drop_na() %>%\n  right_join(bfi_only) %>%\n  as_tibble() %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 29\n  SID   education gender   age    A1    A2    A3    A4    A5    C1    C2    C3\n  <chr>     <int>  <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 61623         3      2    21     6     6     5     6     5     6     6     6\n2 61629         2      1    19     4     3     1     5     1     3     2     4\n3 61630         1      1    19     4     3     6     3     3     6     6     3\n4 61634         1      1    21     4     4     5     6     5     4     3     5\n5 61640         1      1    17     4     5     2     2     1     5     5     5\n6 61661         5      1    68     1     5     6     5     6     4     3     2\n# ℹ 2,794 more rows\n# ℹ 17 more variables: C4 <int>, C5 <int>, E1 <int>, E2 <int>, E3 <int>,\n#   E4 <int>, E5 <int>, N1 <int>, N2 <int>, N3 <int>, N4 <int>, N5 <int>,\n#   O1 <int>, O2 <int>, O3 <int>, O4 <int>, O5 <int>"
  },
  {
    "objectID": "03-week3-workbook.html#eda-1",
    "href": "03-week3-workbook.html#eda-1",
    "title": "Week 3 Workbook",
    "section": "EDA",
    "text": "EDA\n\n\nOne-way descriptive analyses (i.e,. focus on one variable)\n\nDescriptive analyses for continuous variables\nDescriptive analyses for discreet/categorical variables\n\n\n\nTwo-way descriptive analyses (relationship between two variables)\n\nCategorical by categorical\nCategorical by continuous\nContinuous by continuous"
  },
  {
    "objectID": "03-week3-workbook.html#one-way-descriptive-analyses",
    "href": "03-week3-workbook.html#one-way-descriptive-analyses",
    "title": "Week 3 Workbook",
    "section": "One-way descriptive analyses",
    "text": "One-way descriptive analyses\n\nThese are basically what they sound like – the focus is on single variables\nDescriptive analyses for continuous variables\n\nmeans, standard deviations, minima, maxima, counts\n\n\nDescriptive analyses for discreet/categorical variables\n\ncounts, percentages\n\n\n\nContinuous\n\nCodesoep_long %>% \n  select(Procedural__SID,year,Big5__E:Big5__O) %>%\n  pivot_longer(\n    cols = contains(\"Big5\")\n    , names_to = \"trait\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) %>%\n  group_by(year, trait) %>%\n  summarize_at(\n    vars(value)\n    , lst(mean, sd, min, max)\n    , na.rm = T\n  ) %>%\n  ungroup()\n\n\n\n  \n\n\n\nCategorical / Count\n\nCodesoep_long %>%\n  select(Procedural__SID, contains(\"_ever\")) %>%\n  distinct() %>%\n  pivot_longer(\n    cols = contains(\"LifeEvent\")\n    , names_to = \"event\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) %>%\n  group_by(event, value) %>%\n  tally() %>%\n  group_by(event) %>%\n  mutate(total = sum(n)\n         , perc = n/total*100)"
  },
  {
    "objectID": "03-week3-workbook.html#two-way-descriptive-analyses",
    "href": "03-week3-workbook.html#two-way-descriptive-analyses",
    "title": "Week 3 Workbook",
    "section": "Two-way descriptive analyses",
    "text": "Two-way descriptive analyses\n\nAims to capture relationships between variables\n\nCategorical by categorical\n\ncross-tabs, percentages\n\n\n\nCategorical by continuous\n\nmeans, standard deviations, etc. within categories\n\n\n\nContinuous by continuous\n\ncorrelations, covariances, etc.\n\n\n\nCategorical x Categorical\n\nCodesoep_long %>%\n  select(Procedural__SID, Demographic__Sex, contains(\"_ever\")) %>%\n  distinct() %>%\n  pivot_longer(\n    cols = contains(\"LifeEvent\")\n    , names_to = \"event\"\n    , values_to = \"occurred\"\n    , values_drop_na = T\n  ) %>%\n  mutate(Demographic__Sex = mapvalues(Demographic__Sex, c(1,2), c(\"Male\", \"Female\"))\n         , occurred = mapvalues(occurred, c(0,1), c(\"No Event\", \"Event\"))) %>%\n  group_by(event, occurred, Demographic__Sex) %>%\n  tally() %>%\n  group_by(event) %>%\n  mutate(perc = n/sum(n)*100) %>%\n  pivot_wider(\n    names_from = c(occurred)\n    , values_from = c(n, perc)\n  )\n\n\n\n  \n\n\n\nCategorical x Continuous\nSet up the data\n\nCodesoep_twoway <- soep_long %>% \n  filter(year == 2005) %>%\n  select(Procedural__SID, Big5__E:Big5__O) %>%\n  pivot_longer(\n    cols = contains(\"Big5\")\n    , names_to = \"trait\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) %>%\n  left_join(\n    soep_long %>%\n      select(Procedural__SID, contains(\"_ever\")) %>%\n      distinct() %>%\n      pivot_longer(\n        cols = contains(\"LifeEvent\")\n        , names_to = \"event\"\n        , values_to = \"occurred\"\n        , values_drop_na = T\n      ) %>%\n      mutate(occurred = mapvalues(occurred, c(0,1), c(\"No Event\", \"Event\")))\n  )\n\n\nRun the descriptives\n\nCodesoep_twoway %>%\n  group_by(trait, event, occurred) %>%\n  summarize_at(\n    vars(value)\n    , lst(mean, sd, min, max)\n    , na.rm = T\n  ) %>%\n  ungroup() %>%\n  pivot_wider(\n    names_from = trait\n    , values_from = c(mean, sd, min, max)\n  )\n\n\n\n  \n\n\n\nContinuous x continuous\nHere’s how I create more customizable heat maps in ggplot2 for those who would like a reference for themselves.\n\nCoder <- soep_long %>% \n  filter(year == 2005) %>%\n  select(Big5__E:Big5__O) %>%\n  cor(., use = \"pairwise\") \n\nr[lower.tri(r, diag = T)] <- NA\nvars <- rownames(r)\nr %>%\n  data.frame() %>%\n  rownames_to_column(\"V1\") %>%\n  pivot_longer(\n    cols = -V1\n    , names_to = \"V2\"\n    , values_to = \"r\"\n  ) %>%\n  mutate(V1 = factor(V1, levels = vars)\n         , V2 = factor(V2, levels = rev(vars))) %>%\n  ggplot(aes(x = V1, y = V2, fill = r)) + \n    geom_raster() + \n  geom_text(aes(label = round(r, 2))) + \n  scale_fill_gradient2(\n    limits = c(-1,1)\n    , breaks = c(-1, -.5, 0, .5, 1)\n    , low = \"blue\", high = \"red\"\n    , mid = \"white\", na.value = \"white\") + \n  labs(\n    x = NULL\n    , y = NULL\n    , fill = \"Zero-Order Correlation\"\n    , title = \"Zero-Order Correlations Among Variables\"\n    ) + \n  theme_classic() + \n  theme(\n    legend.position = \"bottom\"\n    , axis.text = element_text(face = \"bold\")\n    , axis.text.x = element_text(angle = 45, hjust = 1)\n    , plot.title = element_text(face = \"bold\", hjust = .5)\n    , plot.subtitle = element_text(face = \"italic\", hjust = .5)\n    , panel.background = element_rect(color = \"black\", size = 1)\n  )"
  },
  {
    "objectID": "03-week3-workbook.html#attributions",
    "href": "03-week3-workbook.html#attributions",
    "title": "Week 3 Workbook",
    "section": "Attributions",
    "text": "Attributions\nParts of Part 1 of these slides was adapted from Ozan Jaquette’s EDUC 260A at UCLA."
  },
  {
    "objectID": "02-week2-workbook.html",
    "href": "02-week2-workbook.html",
    "title": "Week 2 Workbook",
    "section": "",
    "text": "Codelibrary(knitr)\nlibrary(psych)\nlibrary(emo)\nlibrary(plyr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "02-week2-workbook.html#why-reproducibility-and-values",
    "href": "02-week2-workbook.html#why-reproducibility-and-values",
    "title": "Week 2 Workbook",
    "section": "Why reproducibility AND values?",
    "text": "Why reproducibility AND values?\n\n\nThe definition of reproducibility is somewhat debated\n\n“‘Reproducibility’ refers to instances in which the original researcher’s data and computer codes are used to regenerate the results”\n\n“‘Reproducibility’ refers to independent researchers arriving at the same results using their own data and methods”\n\n\n\nBut regardless of what definition you choose, reproducibility starts with a commitment in research to be\nclear\ntransparent\nhonest\nthorough"
  },
  {
    "objectID": "02-week2-workbook.html#why-reproducibility-and-values-1",
    "href": "02-week2-workbook.html#why-reproducibility-and-values-1",
    "title": "Week 2 Workbook",
    "section": "Why reproducibility AND values?",
    "text": "Why reproducibility AND values?\n\nReproducibility is ethical.\nWhen I post a project, I pour over my code for hours, adding comments, rendering to multiple formats, trying to flag locations in online materials in the mansucript, etc.\nI am trying to prevent errors, but I am also trying to make sure that other people know what I did, especially if I did make errors\nReproducible research is also equitable.\nA reproducible research workflow can be downloaded by another person as a starting point, providing tools to other researchers who may not have the same access to education and resources as you"
  },
  {
    "objectID": "02-week2-workbook.html#where-should-we-reproducible",
    "href": "02-week2-workbook.html#where-should-we-reproducible",
    "title": "Week 2 Workbook",
    "section": "Where should we reproducible?",
    "text": "Where should we reproducible?\n\nPlanning\n\nStudy planning and design\n\nLab Protocols\n\nCodebooks\n\netc.\n\n\n\nAnalyses\n\nScripting\n\nCommunication\n\netc."
  },
  {
    "objectID": "02-week2-workbook.html#aspects-of-reproducibility",
    "href": "02-week2-workbook.html#aspects-of-reproducibility",
    "title": "Week 2 Workbook",
    "section": "Aspects of Reproducibility",
    "text": "Aspects of Reproducibility\n\nData within files should be ‘tidy’ (next week – tidyr)\nProject based approach (today)\nConsistency: naming, space, style (today)\nDocumentation: commenting and README (today)\nLiterate programming e.g. Rmarkdown (every day!)"
  },
  {
    "objectID": "02-week2-workbook.html#reproducible-workflow",
    "href": "02-week2-workbook.html#reproducible-workflow",
    "title": "Week 2 Workbook",
    "section": "Reproducible Workflow",
    "text": "Reproducible Workflow\nA reproducible workflow is organized. What does it mean to be be organized? At least:\n\nUse a project based approach, e.g., RStudio project or similar\n\nHave a hierarchical folder structure\n\nHave a consistent and informative naming system that ‘plays nice’\n\nDocument code with comments and analyses with README\n\n\nMore advanced (later in the class)\n\n\nGeneralize with functions and packages\nversion control"
  },
  {
    "objectID": "02-week2-workbook.html#what-is-a-project",
    "href": "02-week2-workbook.html#what-is-a-project",
    "title": "Week 2 Workbook",
    "section": "What is a project?",
    "text": "What is a project?\n\nA project is a discrete piece of work which has a number of files associated with it such as the data and scripts for an analysis and the production reports.\nUsing a project-oriented workflow means to have a hierarchical folder structure with everything needed to reproduce an analysis.\n\nOne research project might have several organizational projects associated with it, for example:\n\ndata files and metadata (which may be made into a package)\npreregistration\nanalysis and reporting\na package developed for the analysis\nan app for allowing data to be explored by others"
  },
  {
    "objectID": "02-week2-workbook.html#example",
    "href": "02-week2-workbook.html#example",
    "title": "Week 2 Workbook",
    "section": "Example",
    "text": "Example\nGood Workflows are:\n\nstructured\n\nsystematic\n\nrepeatable\n\nNaming\n\nhuman and machine readable\n\nno spaces\n\nuse snake/kebab case\n\nordering: numbers (zero left padded), dates\n\nfile extensions\n\n\n\n-- ipcs_data_2019\n   |__ipcs_data_2019.Rproj\n   |__data\n      |__raw_data\n         |__2019-03-21_ema_raw.csv\n         |__2019-03-21_baseline_raw.csv\n      |__clean_data\n         |__2019-06-21_ema_long.csv\n         |__2019-06-21_ema_long.RData\n         |__2019-06-21_baseline_wide.csv\n         |__2019-06-21_baseline_wide.RData\n   |__results\n      |__01_models\n         |__E_mortality.RData\n         |__A_mortality.RData\n      |__02_summaries\n         |__E_mortality.RData\n         |__A_mortality.RData\n      |__03_figures\n         |__mortality.png\n         |__mortality.pdf\n      |__04_tables\n         |__zero_order_cors.RData\n         |__descriptives.RData\n         |__key_terms.RData\n         |__all_model_terms.RData\n   |__README.md\n   |__refs\n      |__r_refs.bib\n      |__proj_refs.bib\n   |__analyses\n      |__01_background.Rmd\n      |__02_data_cleaning.Rmd\n      |__03_models.Rmd\n      |__04_summary.Rmd"
  },
  {
    "objectID": "02-week2-workbook.html#what-is-a-path",
    "href": "02-week2-workbook.html#what-is-a-path",
    "title": "Week 2 Workbook",
    "section": "What is a path?",
    "text": "What is a path?\nA path gives the address - or location - of a filesystem object, such as a file or directory.\n\nPaths appear in the address bar of your browser or file explorer.\nWe need to know a file path whenever we want to read, write or refer to a file using code rather than interactively pointing and clicking to navigate.\nA path can be absolute or relative\n\nabsolute = whole path from root\nrelative = path from current directory\n\n\n\nAbsolute paths\n\nAn Absolute path is given from the “root directory” of the object.\nThe root directory of a file system is the first or top directory in the hierarchy.\nFor example, C:\\ or M:\\ on windows or / on a Mac which is displayed as Macintosh HD in Finder.\n\nThe absolute path for a file, pigeon.txt could be:\n\nwindows: C:/Users/edbeck/Desktop/pigeons/data-raw/pigeon.txt\n\nMac/unix systems: /Users/edbeck/Desktop/pigeons/data-raw/pigeon.txt\n\nweb: http://github.com/emoriebeck/pigeons/data/pigeon.txt\n\nWhat is a directory?\n\nDirectory is the old word for what many now call a folder 📂.\nCommands that act on directories in most programming languages and environments reflect this.\nFor example, in R this means “tell me my working directory”:\ngetwd() get working directory in R\nWhat is a working directory?\n\nThe working directory is the default location a program is using. It is where the program will read and write files by default. You have only one working directory at a time.\nThe terms ‘working directory’, ‘current working directory’ and ‘current directory’ all mean the same thing.\n\nFind your current working directory with:\n\nCodegetwd()\n\n[1] \"/Users/emoriebeck/Documents/teaching/PSC290-cleaning-fall-2023/psc290-data-FQ23/psc290-data-FQ23\"\n\n\nRelative paths\nA relative path gives the location of a filesystem object relative to the working directory, (i.e., that returned by getwd()).\n\nWhen pigeon.txt is in the working directory the relative path is just the file * name: pigeon.txt\nIf there is a folder in the working directory called data-raw and pigeon.txt is in there then the relative path is data-raw/pigeon.txt\nPaths: moving up the hierarchy\n\n../ allows you to look in the directory above the working directory\nWhen pigeon.txt is in folder above the working the relative path is ../pigeon.txt\nAnd if it is in a folder called data-raw which is in the directory above the working directory then the relative path is ../data-raw/pigeon.txt\nWhat’s in my directory?\nYou can list the contents of a directory using the dir() command\n\n\ndir() list the contents of the working directory\n\ndir(\"..\") list the contents of the directory above the working directory\n\ndir(\"../..\") list the contents of the directory two directories above the working directory\n\ndir(\"data-raw\") list the contents of a folder call data-raw which is in the working directory.\nRelative or absolute\n\nMost of the time you should use relative paths because that makes your work portable (i.e. to a different machine / user / etc.).\n🥳 The tab key is your friend!\nYou only need to use absolute paths when you are referring to filesystem outside the one you are using.\n\nI often store the beginning of that path as object.\n\nweb_wd <- “https://github.com/emoriebeck/pigeons/”\nThen I can use sprintf() or paste() to add different endings\n\n\n\n\nCodeweb_wd <- \"https://github.com/emoriebeck/pigeons/\"\nsprintf(\"%s/data-raw/pigeon.txt\", web_wd)\n\n[1] \"https://github.com/emoriebeck/pigeons//data-raw/pigeon.txt\""
  },
  {
    "objectID": "02-week2-workbook.html#example-1",
    "href": "02-week2-workbook.html#example-1",
    "title": "Week 2 Workbook",
    "section": "Example",
    "text": "Example\nDownload and unzip pigeons.zip which has the following structure:\n-- pigeons\n   |__data-processed\n      |__pigeon_long.txt\n   |__data-raw\n      |__pigeon.txt\n   |__figures\n      |__fig1.tiff\n   |__scripts\n      |__analysis.R\n      |__import_reshape.R\n   |__pigeons.Rproj"
  },
  {
    "objectID": "02-week2-workbook.html#rstudio-projects-1",
    "href": "02-week2-workbook.html#rstudio-projects-1",
    "title": "Week 2 Workbook",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\nProject is obviously a commonly used word. When I am referring to an RStudio Project I will use the capitalised words ‘RStudio Project’ or ‘Project’.\nIn other cases, I will use ‘project’.\nAn RStudio Project is a directory with an .Rproj file in it.\nThe name of the RStudio Project is the same as the name of the top level directory which is referred to as the Project directory.\n\nFor example, if you create an RStudio Project ipcs_data_2019 your folder structure would look something like this:\n-- ipcs_data_2019\n   |__ipcs_data_2019.Rproj\n   |__data\n      |__raw_data\n         |__2019-03-21_ema_raw.csv\n         |__2019-03-21_baseline_raw.csv\n      |__clean_data\n         |__2019-06-21_ema_long.csv\n         |__2019-06-21_ema_long.RData\n         |__2019-06-21_baseline_wide.csv\n         |__2019-06-21_baseline_wide.RData\n   |__results\n      |__01_models\n      |__02_summaries\n      |__03_figures\n      |__04_tables\n   |__README.md\n   |__refs\n      |__r_refs.bib\n      |__proj_refs.bib\n   |__analyses\n      |__01_background.Rmd\n      |__02_data_cleaning.Rmd\n      |__03_models.Rmd\n      |__04_summary.Rmd\n\nthe .RProj file which is the defining feature of an RStudio Project\nWhen you open an RStudio Project, the working directory is set to the Project directory (i.e., the location of the .Rproj file).\nThis makes your work portable. You can zip up the project folder and send it to any person, including future you, or any computer.\nThey will be able to unzip, open the project and have all the code just work.\n(This is great for sending code and/or results to your advisors)"
  },
  {
    "objectID": "02-week2-workbook.html#directory-structure",
    "href": "02-week2-workbook.html#directory-structure",
    "title": "Week 2 Workbook",
    "section": "Directory structure",
    "text": "Directory structure\nYou are aiming for structured, systematic and repeatable. For example, the Project directory might contain:\n\n.RProj file\n\nREADME - tell people what the project is and how to use it\n\nLicense - tell people what they are allowed to do with your project\nDirectories\ndata/\n\nprereg/\n\nscripts/\nresults/\n\nmanuscript/"
  },
  {
    "objectID": "02-week2-workbook.html#readme",
    "href": "02-week2-workbook.html#readme",
    "title": "Week 2 Workbook",
    "section": "README",
    "text": "README\n\nREADMEs are a form of documentation which have been widely used for a long time. They contain all the information about the other files in a directory. They can be extensive.\nWikipedia README page\nGitHub Doc’s About READMEs\nOSF\n\nA minimal README might give:\n\nTitle\nDescription, 50 words or so on what the project is\nTechnical Description of the project\n\nWhat software and packages are needed including versions\nAny instructions needed to run the analysis/use the software\nAny issues that a user might face in running the analysis/using the software\n\n\nInstructions on how to use the work\nLinks to where other files, materials, etc. are stored\n\nE.g., an OSF readme may point to GitHub, PsyArxiv, etc."
  },
  {
    "objectID": "02-week2-workbook.html#license",
    "href": "02-week2-workbook.html#license",
    "title": "Week 2 Workbook",
    "section": "License",
    "text": "License\nA license tells others what they can and can’t do with your work.\nchoosealicense.com is a useful explainer.\nI typically use:\n\n\nMIT License for software\n\nCC-BY-SA-4.0 for other work"
  },
  {
    "objectID": "02-week2-workbook.html#rstudio-project-infrastructure",
    "href": "02-week2-workbook.html#rstudio-project-infrastructure",
    "title": "Week 2 Workbook",
    "section": "RStudio Project infrastructure",
    "text": "RStudio Project infrastructure\n🎬 create a new Project called iris by:\n\nclicking File->New Project…\nclicking on the little icon (second from the left) at the top\nChoose New Project, then New Directory, then New Project. Name the RStudio Project iris.\nCreate folders in iris called data-raw, data-processed and figures.\nStart new scripts called 01-import.R, 02-tidy.R, and 03-figures.R"
  },
  {
    "objectID": "02-week2-workbook.html#save-and-import",
    "href": "02-week2-workbook.html#save-and-import",
    "title": "Week 2 Workbook",
    "section": "Save and Import",
    "text": "Save and Import\n\nSave a copy of iris.csv to your data-raw folder. These data give the information about different species of irises.\nIn your 01-import.R script, load the tidyverse set of packages.\n\n\nCodelibrary(tidyverse)\nwrite_csv(iris, file = \"data-raw/iris.csv\")\n\n\n\nAdd the command to import the data:\n\n\nCodeiris <- read_csv(\"data-raw/iris.csv\")\n\n\n\n\n\n\nThe relative path is data-raw/iris.csv because your working directory is the Project directory, iris."
  },
  {
    "objectID": "02-week2-workbook.html#reformat-the-data",
    "href": "02-week2-workbook.html#reformat-the-data",
    "title": "Week 2 Workbook",
    "section": "Reformat the data",
    "text": "Reformat the data\nThis dataset has three observations in a row - it is not ‘tidy’.\n\nOpen your 02-tidy.R script, and reshape the data using:\n\n\nCodeiris <- pivot_longer(data = iris, \n                     cols = -Species, \n                     names_to = \"attribute\", \n                     values_to = \"value\")\n\n\n\nThis reformats the dataframe in R but does not overwrite the text file of the data.\nDon’t worry too much about this right now. We’ll spend a lot of time talking about reshaping data next week!"
  },
  {
    "objectID": "02-week2-workbook.html#writing-files",
    "href": "02-week2-workbook.html#writing-files",
    "title": "Week 2 Workbook",
    "section": "Writing files",
    "text": "Writing files\nOften we want to write to files.\n\nMy main reasons for doing so are to save copies of data that have been processed and to save manuscripts and graphics.\nAlso, as someone who collects a lot of data, the de-identified, fully anonymized data files I can share and the identifiable data I collect require multiple versions (and encryption, keys, etc.)\nWrite your dataframe iris to a csv file named iris-long.csv in your data-processed folder:\n\n\nCodefile <- \"data-processed/iris-long.csv\"\nwrite_csv(iris, file)\n\n\n\n\n\n\nPutting file paths into variables often makes your code easier to read especially when file paths are long or used multiple times."
  },
  {
    "objectID": "02-week2-workbook.html#create-a-plot",
    "href": "02-week2-workbook.html#create-a-plot",
    "title": "Week 2 Workbook",
    "section": "Create a plot",
    "text": "Create a plot\nOpen your 03-figures.R script and create a simple plot of this data with:\n\nCodefig1 <- ggplot(\n  data = iris\n  , aes(y = Species, x = value, fill = Species)\n  ) + \n  geom_boxplot() +                       \n  facet_grid(attribute~.) + \n  scale_x_continuous(name = \"Attribute\") +\n  scale_y_discrete(name = \"Species\") +\n  theme_classic() + \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "02-week2-workbook.html#view-plot",
    "href": "02-week2-workbook.html#view-plot",
    "title": "Week 2 Workbook",
    "section": "View plot",
    "text": "View plot\nView plot with:\n\nCodefig1"
  },
  {
    "objectID": "02-week2-workbook.html#write-ggplot-figure-to-file",
    "href": "02-week2-workbook.html#write-ggplot-figure-to-file",
    "title": "Week 2 Workbook",
    "section": "Write ggplot figure to file",
    "text": "Write ggplot figure to file\n\nA useful function for saving ggplot figures is ggsave().\nIt has arguments for the size, resolution and device for the image. See the ggsave() reference page.\nSince I often make more than one figure, I might set these arguments first.\n\n\n\n\nAssign ggsave argument values to variables:\n\n\nCode# figure saving settings\nunits <- \"in\"  \nfig_w <- 3.2\nfig_h <- fig_w\ndpi <- 600\ndevice <- \"tiff\" \n\n\n\n\nSave the figure to your figures directory:\n\n\nCodeggsave(\"figures/fig1.tiff\",\n       plot = fig1,\n       device = device,\n       width = fig_w,\n       height = fig_h,\n       units = units,\n       dpi = dpi)\n\n\n\n\n\n\nCheck it is there!\n\n\n\n\n\n\n\nData Manipulation in dplyr"
  },
  {
    "objectID": "02-week2-workbook.html#core-functions",
    "href": "02-week2-workbook.html#core-functions",
    "title": "Week 2 Workbook",
    "section": "Core Functions",
    "text": "Core Functions\n\n\n\n\n%>%\nfilter()\nselect()\narrange()\ngroup_by()\nmutate()\nsummarize()\n\n\n\nAlthough each of these functions are powerful alone, they are incredibly powerful in conjunction with one another. So below, I’ll briefly introduce each function, then link them all together using an example of basic data cleaning and summary."
  },
  {
    "objectID": "02-week2-workbook.html#section",
    "href": "02-week2-workbook.html#section",
    "title": "Week 2 Workbook",
    "section": "1. %>%\n",
    "text": "1. %>%\n\n\nThe pipe %>% is wonderful. It makes coding intuitive. Often in coding, you need to use so-called nested functions. For example, you might want to round a number after taking the square of 43.\n\n\nCodesqrt(43)\n\n[1] 6.557439\n\nCoderound(sqrt(43), 2)\n\n[1] 6.56\n\n\nThe issue with this comes whenever we need to do a series of operations on a data set or other type of object. In such cases, if we run it in a single call, then we have to start in the middle and read our way out.\n\nCoderound(sqrt(43/2), 2)\n\n[1] 4.64\n\n\nThe pipe solves this by allowing you to read from left to right (or top to bottom). The easiest way to think of it is that each call of %>% reads and operates as “and then.” So with the rounded square root of 43, for example:\n\nCodesqrt(43) %>%\n  round(2)\n\n[1] 6.56"
  },
  {
    "objectID": "02-week2-workbook.html#filter",
    "href": "02-week2-workbook.html#filter",
    "title": "Week 2 Workbook",
    "section": "2. filter()\n",
    "text": "2. filter()\n\nOften times, when conducting research (experiments or otherwise), there are observations (people, specific trials, etc.) that you don’t want to include.\n\n\nCodedata(bfi) # grab the bfi data from the psych package\nbfi <- bfi %>% as_tibble()\nhead(bfi)\n\n\n\n  \n\n\n\nOften times, when conducting research (experiments or otherwise), there are observations (people, specific trials, etc.) that you don’t want to include.\n\nCodesummary(bfi$age) # get age descriptives\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   3.00   20.00   26.00   28.78   35.00   86.00 \n\n\nOften times, when conducting research (experiments or otherwise), there are observations (people, specific trials, etc.) that you don’t want to include.\n\nCodebfi2 <- bfi %>% # see a pipe!\n  filter(age <= 18) # filter to age up to 18\n\nsummary(bfi2$age) # summary of the new data \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    3.0    16.0    17.0    16.3    18.0    18.0 \n\n\nBut this isn’t quite right. We still have folks below 12. But, the beauty of filter() is that you can do sequence of OR and AND statements when there is more than one condition, such as up to 18 AND at least 12.\n\nCodebfi2 <- bfi %>%\n  filter(age <= 18 & age >= 12) # filter to age up to 18 and at least 12\n\nsummary(bfi2$age) # summary of the new data \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    16.0    17.0    16.4    18.0    18.0 \n\n\nGot it!\n\nBut filter works for more use cases than just conditional\n\n\n<, >, <=, and >=\n\n\n\nIt can also be used for cases where we want a single values to match cases with text.\nTo do that, let’s convert one of the variables in the bfi data frame to a string.\nSo let’s change gender (1 = male, 2 = female) to text (we’ll get into factors later).\n\n\nCodebfi$education <- plyr::mapvalues(bfi$education, 1:5, c(\"Below HS\", \"HS\", \"Some College\", \"College\", \"Higher Degree\"))\n\n\nNow let’s try a few things:\n1. Create a data set with only individuals with some college (==).\n\nCodebfi2 <- bfi %>% \n  filter(education == \"Some College\")\nunique(bfi2$education)\n\n[1] \"Some College\"\n\n\n2. Create a data set with only people age 18 (==).\n\nCodebfi2 <- bfi %>%\n  filter(age == 18)\nsummary(bfi2$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     18      18      18      18      18      18 \n\n\n3. Create a data set with individuals with some college or above (%in%).\n\nCodebfi2 <- bfi %>%\n  filter(education %in% c(\"Some College\", \"College\", \"Higher Degree\"))\nunique(bfi2$education)\n\n[1] \"Some College\"  \"Higher Degree\" \"College\"      \n\n\n%in% is great. It compares a column to a vector rather than just a single value, you can compare it to several\n\nCodebfi2 <- bfi %>%\n  filter(age %in% 12:18)\nsummary(bfi2$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    16.0    17.0    16.4    18.0    18.0"
  },
  {
    "objectID": "02-week2-workbook.html#select",
    "href": "02-week2-workbook.html#select",
    "title": "Week 2 Workbook",
    "section": "3. select()\n",
    "text": "3. select()\n\n\nIf filter() is for pulling certain observations (rows), then select() is for pulling certain variables (columns).\nit’s good practice to remove these columns to stop your environment from becoming cluttered and eating up your RAM.\nIn our bfi data, most of these have been pre-removed, so instead, we’ll imagine we don’t want to use any indicators of Agreeableness (A1-A5) and that we aren’t interested in gender.\nWith select(), there are few ways choose variables. We can bare quote name the ones we want to keep, bare quote names we want to remove, or use any of a number of select() helper functions.\n\nA. Bare quote columns we want to keep:\n\n\n\nCodebfi %>%\n  select(C1, C2, C3, C4, C5) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 5\n     C1    C2    C3    C4    C5\n  <int> <int> <int> <int> <int>\n1     2     3     3     4     4\n2     5     4     4     3     4\n3     4     5     4     2     5\n4     4     4     3     5     5\n5     4     4     5     3     2\n6     6     6     6     1     3\n# ℹ 2,794 more rows\n\n\n\n\nCodebfi %>%\n  select(C1:C5) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 5\n     C1    C2    C3    C4    C5\n  <int> <int> <int> <int> <int>\n1     2     3     3     4     4\n2     5     4     4     3     4\n3     4     5     4     2     5\n4     4     4     3     5     5\n5     4     4     5     3     2\n6     6     6     6     1     3\n# ℹ 2,794 more rows\n\n\n\n\n\nB. Bare quote columns we don’t want to keep:\n\nCodebfi %>% \n  select(-(A1:A5), -gender) %>% # Note the `()` around the columns\n  print(n = 6)\n\n# A tibble: 2,800 × 22\n     C1    C2    C3    C4    C5    E1    E2    E3    E4    E5    N1    N2    N3\n  <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1     2     3     3     4     4     3     3     3     4     4     3     4     2\n2     5     4     4     3     4     1     1     6     4     3     3     3     3\n3     4     5     4     2     5     2     4     4     4     5     4     5     4\n4     4     4     3     5     5     5     3     4     4     4     2     5     2\n5     4     4     5     3     2     2     2     5     4     5     2     3     4\n6     6     6     6     1     3     2     1     6     5     6     3     5     2\n# ℹ 2,794 more rows\n# ℹ 9 more variables: N4 <int>, N5 <int>, O1 <int>, O2 <int>, O3 <int>,\n#   O4 <int>, O5 <int>, education <chr>, age <int>\n\n\nC. Add or remove using select() helper functions.\n\n\n\n\nstarts_with()\n\nends_with()\ncontains()\nmatches()\nnum_range()\none_of()\nall_of()\n\n\n\n\nCodebfi %>%\n  select(starts_with(\"C\"))"
  },
  {
    "objectID": "02-week2-workbook.html#arrange",
    "href": "02-week2-workbook.html#arrange",
    "title": "Week 2 Workbook",
    "section": "4. arrange()\n",
    "text": "4. arrange()\n\n\nSometimes, either in order to get a better sense of our data or in order to well, order our data, we want to sort it\nAlthough there is a base R sort() function, the arrange() function is tidyverse version that plays nicely with other tidyverse functions.\n\n\nSo in our previous examples, we could also arrange() our data by age or education, rather than simply filtering. (Or as we’ll see later, we can do both!)\n\n\nCode# sort by age\nbfi %>% \n  select(gender:age) %>%\n  arrange(age) %>% \n  print(n = 6)\n\n# A tibble: 2,800 × 3\n  gender education       age\n   <int> <chr>         <int>\n1      1 Higher Degree     3\n2      2 <NA>              9\n3      2 Some College     11\n4      2 <NA>             11\n5      2 <NA>             11\n6      2 <NA>             12\n# ℹ 2,794 more rows\n\n\n\n\nCode# sort by education\nbfi %>%\n  select(gender:age) %>%\n  arrange(education) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 3\n  gender education   age\n   <int> <chr>     <int>\n1      1 Below HS     19\n2      1 Below HS     21\n3      1 Below HS     17\n4      1 Below HS     18\n5      1 Below HS     18\n6      2 Below HS     18\n# ℹ 2,794 more rows\n\n\n\n\nWe can also arrange by multiple columns, like if we wanted to sort by gender then education:\n\nCodebfi %>%\n  select(gender:age) %>%\n  arrange(gender, education) %>% \n  print(n = 6)\n\n# A tibble: 2,800 × 3\n  gender education   age\n   <int> <chr>     <int>\n1      1 Below HS     19\n2      1 Below HS     21\n3      1 Below HS     17\n4      1 Below HS     18\n5      1 Below HS     18\n6      1 Below HS     32\n# ℹ 2,794 more rows"
  },
  {
    "objectID": "02-week2-workbook.html#group_by",
    "href": "02-week2-workbook.html#group_by",
    "title": "Week 2 Workbook",
    "section": "5. group_by()\n",
    "text": "5. group_by()\n\n\nThe group_by() function is the “split” of the method\nIt basically implicitly breaks the data set into chunks by whatever bare quoted column(s)/variable(s) are supplied as arguments.\n\nSo imagine that we wanted to group_by() education levels to get average ages at each level\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  group_by(education) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 8\n# Groups:   education [6]\n     C1    C2    C3    C4    C5   age gender education   \n  <int> <int> <int> <int> <int> <int>  <int> <chr>       \n1     2     3     3     4     4    16      1 <NA>        \n2     5     4     4     3     4    18      2 <NA>        \n3     4     5     4     2     5    17      2 <NA>        \n4     4     4     3     5     5    17      2 <NA>        \n5     4     4     5     3     2    17      1 <NA>        \n6     6     6     6     1     3    21      2 Some College\n# ℹ 2,794 more rows\n\n\n\nHadley’s first law of data cleaning: “What is split, must be combined”\nThis is super easy with the ungroup() function:\n\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  group_by(education) %>%\n  ungroup() %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 8\n     C1    C2    C3    C4    C5   age gender education   \n  <int> <int> <int> <int> <int> <int>  <int> <chr>       \n1     2     3     3     4     4    16      1 <NA>        \n2     5     4     4     3     4    18      2 <NA>        \n3     4     5     4     2     5    17      2 <NA>        \n4     4     4     3     5     5    17      2 <NA>        \n5     4     4     5     3     2    17      1 <NA>        \n6     6     6     6     1     3    21      2 Some College\n# ℹ 2,794 more rows\n\n\nMultiple group_by() calls overwrites previous calls:\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  group_by(education) %>%\n  group_by(gender, age) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 8\n# Groups:   gender, age [115]\n     C1    C2    C3    C4    C5   age gender education   \n  <int> <int> <int> <int> <int> <int>  <int> <chr>       \n1     2     3     3     4     4    16      1 <NA>        \n2     5     4     4     3     4    18      2 <NA>        \n3     4     5     4     2     5    17      2 <NA>        \n4     4     4     3     5     5    17      2 <NA>        \n5     4     4     5     3     2    17      1 <NA>        \n6     6     6     6     1     3    21      2 Some College\n# ℹ 2,794 more rows"
  },
  {
    "objectID": "02-week2-workbook.html#mutate",
    "href": "02-week2-workbook.html#mutate",
    "title": "Week 2 Workbook",
    "section": "6. mutate()\n",
    "text": "6. mutate()\n\n\n\nmutate() is one of your “apply” functions\nWhen you use mutate(), the resulting data frame will have the same number of rows you started with\nYou are directly mutating the existing data frame, either modifying existing columns or creating new ones\n\nTo demonstrate, let’s add a column that indicated average age levels within each age group\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  arrange(education) %>%\n  group_by(education) %>% \n  mutate(age_by_edu = mean(age, na.rm = T)) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 9\n# Groups:   education [6]\n     C1    C2    C3    C4    C5   age gender education age_by_edu\n  <int> <int> <int> <int> <int> <int>  <int> <chr>          <dbl>\n1     6     6     3     4     5    19      1 Below HS        25.1\n2     4     3     5     3     2    21      1 Below HS        25.1\n3     5     5     5     2     2    17      1 Below HS        25.1\n4     5     5     4     1     1    18      1 Below HS        25.1\n5     4     5     4     3     3    18      1 Below HS        25.1\n6     3     2     3     4     6    18      2 Below HS        25.1\n# ℹ 2,794 more rows\n\n\nmutate() is also super useful even when you aren’t grouping\nWe can create a new category\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  mutate(gender_cat = plyr::mapvalues(gender, c(1,2), c(\"Male\", \"Female\")))\n\n\n\n  \n\n\n\nWe could also just overwrite it:\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  mutate(gender = plyr::mapvalues(gender, c(1,2), c(\"Male\", \"Female\")))"
  },
  {
    "objectID": "02-week2-workbook.html#summarize-summarise",
    "href": "02-week2-workbook.html#summarize-summarise",
    "title": "Week 2 Workbook",
    "section": "7. summarize() / summarise()\n",
    "text": "7. summarize() / summarise()\n\n\n\nsummarize() is one of your “apply” functions\nThe resulting data frame will have the same number of rows as your grouping variable\nYou number of groups is 1 for ungrouped data frames\n\n\nCode# group_by() education\nbfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  arrange(education) %>%\n  group_by(education) %>% \n  summarize(age_by_edu = mean(age, na.rm = T))  \n\n\n\n  \n\n\n\n\nCode# no groups  \nbfi %>% \n  select(starts_with(\"C\"), age, gender, education) %>%\n  arrange(education) %>%\n  summarize(age_by_edu = mean(age, na.rm = T))"
  },
  {
    "objectID": "05-week5-slides.html",
    "href": "05-week5-slides.html",
    "title": "Week 5 Slides",
    "section": "",
    "text": "Welcome to Week 4! This week, we’ll talk about data documentation, readable codebooks, and data cleaning workflows."
  },
  {
    "objectID": "ps4-week4.html",
    "href": "ps4-week4.html",
    "title": "Problem Set Week 4",
    "section": "",
    "text": "Due Date: Monday, October 30, 12:01 AM PST.\nDownload your problem set for week 4 below or on Canvas.\nAnswers will be posted after the due date."
  },
  {
    "objectID": "ps3-week3.html",
    "href": "ps3-week3.html",
    "title": "Problem Set Week 3",
    "section": "",
    "text": "Due Date: Monday, October 23, 12:01 AM PST.\nDownload your problem set for week 3 below or on Canvas.\nAnswers will be posted after the due date."
  },
  {
    "objectID": "ps5-week5.html",
    "href": "ps5-week5.html",
    "title": "Problem Set Week 5",
    "section": "",
    "text": "Due Date: Monday, November 6, 12:01 AM PST.\nDownload your problem set for week 5 below or on Canvas.\nAnswers will be posted after the due date."
  },
  {
    "objectID": "ps2-week2.html",
    "href": "ps2-week2.html",
    "title": "Problem Set Week 2",
    "section": "",
    "text": "Due Date: Monday, October 16, 12:01 AM PST.\nDownload your problem set for week 2 below or on Canvas.\nAnswers can be found here."
  },
  {
    "objectID": "ps1-week1.html",
    "href": "ps1-week1.html",
    "title": "Problem Set Week 1",
    "section": "",
    "text": "Due Date: Monday, October 9, 12:01 AM PST.\nDownload your problem set for week 1 below or on Canvas.\nAnswers can be found here."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr. Emorie D. Beck (she/her/hers)\nE-mail: edbeck@ucdavis.edu\nOffice: 268J\nOffice Hours:\n- Drop-in hour: Tuesdays 11a-12p\n- Or by appointment: edbeck@ucdavis.edu\n\n\n\n\n\n\nDr. Beck is an Assistant Professor in the Psychology Department specializing in personality psychology. She received her PhD in Social and Personality Psychology from Washington University in St. Louis in 2020 and her BA (with honors) from Brown University in 2016. \nDr. Beck’s research focuses around the question of what personality is. Definitions have big consequences for how we measure personality, what those measures predict both short- and long-term, how personality is thought to change, and more. One way of doing this is to focus on different levels of aggregation. Thus, she studies how to understand the personality of an individual relative to only themself, relative to some others, and relative to all others. To do so, she uses a mix of methods, including experience sampling methods, passive sensing, survey data, panel data, cognitive tests, and more measured across time intervals from moments to years along with an array of statistical approaches, including time series analysis, multilevel / hierarchical modeling, machine learning, network psychometrics, structural equation modeling, and more. For example, Dr. Beck has been working to build personalized machine learning prediction of behaviors, experiences, and more, finding that we can predict behaviors and experiences better when we don’t assume that people have the same antecedents of the behaviors and experiences.  Instead, people have unique antecedents, which could have consequences for how to change or intervene upon behaviors and experiences. In other work, Dr. Beck uses longitudinal panel data across multiple continents to answer questions about what personality traits predict over time. For example, she recently examined personality trait and well-being predictors of later dementia diagnoses and neuropathology measures after death, finding that personality traits are strong predictors of dementia diagnosis but have a much more complex relationship with neuropathology measures."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "After successful completion of this course, you will be able to:\n1.    Build your own research workflow that can be ported to future projects. \n2.    Learn new programming skills that will help you efficiently, accurately, and deliberately clean and manage your data.  \n3.    Create a bank of code and tools that can be used for a variety of types of research."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\nThere is no official textbook for this course (but if there was, it’d be Wickham, Cetinkaya-Rundel, & Grolemund’s R for Data Science [2nd edition]). However, many of you are coming in with different levels of knowledge and different types of questions, so I am providing some suggested readings below. \nI have arranged for students in this course to receive free access to Data Camp, a library of R (other programming languages) tutorials. Sign up using your UC Davis email here. \nWe will pull from the following two (freely available) books:\nHadley Wickham & Garret Grolemund: R for Data Science\nHadley Wickham: Advanced R \nAll course materials comply with copyright/fair use policies."
  },
  {
    "objectID": "syllabus.html#technology-requirements",
    "href": "syllabus.html#technology-requirements",
    "title": "Syllabus",
    "section": "Technology Requirements",
    "text": "Technology Requirements\nThe lecture presentations, links to articles, assignments, and rubrics are located on this Canvas site for the course and on the Quarto site. To participate in learning activities and complete assignments, you will need:\n\nAccess to a working computer that has a current operating system with updates installed;\nReliable Internet access and a UCD email account;\nA current Internet browser that is compatible with Canvas;\nR and R Studio (see below)\nReliable data storage for your work, such as Box, Office 365, or a USB drive.\n\nWe will do all of our data cleaning work in this class using the R programming language. We will use RStudio to interface with R console for a more user-friendly experience.\nPlease install both R and RStudio before the first day of class. Here’s how:\n\nGet the most recent version of R (free). Download the version of R compatible with your operating system (Mac, Linux, or Windows). If you are running Windows or MacOS, you should choose one of the precompiled binary distributions (i.e., ready-to-run applications; .exe for windows or .pkg for Mac) linked at the top of the R Project’s webpage.\nOnce R is installed, download and install R Studio (soon to be Pivot). R Studio is an “Integrated Development Environment”, or IDE. This means it is a front-end for R that makes it much easier to work with. R Studio is also free, and available for Windows, Mac, and Linux platforms.\nInstall the tidyverse library and several other add-on packages for R. These are sets of tolls or functions that will aid us in cleaning and wrangling data, and more. This is a non-exhaustive list that will get us started.\n\n\nmy_packages <- c(\n  \"plyr\", \"tidyverse\", \"furrr\", \"broom\",\n  \"MASS\", \"quantreg\", \"rlang\", \"scales\",\n  \"survey\", \"srvyr\", \"devtools\", \"future\"\n)\n\ninstall.packages(my_packages, repos = \"http://cran.rstudio.com\")"
  },
  {
    "objectID": "syllabus.html#minimum-technical-skills-needed",
    "href": "syllabus.html#minimum-technical-skills-needed",
    "title": "Syllabus",
    "section": "Minimum Technical Skills Needed",
    "text": "Minimum Technical Skills Needed\nMinimum technical skills are needed in this course. All work in this course must be completed and submitted online through Canvas and all assignments will be completed in R / Rmarkdown / Quarto. Therefore, you must have consistent and reliable access to a computer and the Internet.\nThe basic technical skills you have include the ability to:\n\nOrganize and save electronic files;\nUse UCD email and attached files;\nCheck email and Canvas a few times / week;\nDownload and upload documents;\nLocate information with a browser; and\nUse Canvas.\n\nHowever, you will spend about 50% of this course using R. Therefore, to get the most out of this class, I highly recommend having a better-than-beginner understanding or and experience with the R programming language. R is a skill, just like understanding the components of quality data and workflows, and for the purposes of this course, both are equally necessary and important. If you have any concerns about whether your R skills are strong enough for the course, please talk to the instructor or consider taking the course in a future year."
  },
  {
    "objectID": "syllabus.html#course-assignments-and-grading",
    "href": "syllabus.html#course-assignments-and-grading",
    "title": "Syllabus",
    "section": "Course Assignments and Grading",
    "text": "Course Assignments and Grading\n\nGeneral Assignment Information\n\nAll coursework (assignments) is secured in Canvas with a username and password.\nAll assignments are due on the day indicated on the course schedule.\nComplete rubrics (final project presentations and paper only) will be provided in Canvas.\n\n\n\nWeekly Assignments\nThe goal of this course is not simply to teach you how to clean hypothetical or convenient data. Rather, the goal is to teach you principles of good, accurate, reproducible, and efficient data cleaning and management, how to identify features of high quality data, and how to produce results of analyses efficiently.\nWeekly homework (40%) in this class will focus on programming concepts from that week. Each week, you will complete one problem set, applying the skills you learned that week to your own data. Submit each of these via Canvas by midnight the Sunday before class.\nThese will be graded for completion (you turned it in), relevance (it should be clear that you actually tried to do what you asked), and effort (please show your work). You will not receive feedback on them unless there is an ongoing problem (e.g., lack of depth or effort).\nThis is good opportunity to:\n\nBetter understand challenges with your own data (relative to others)\nReflect on features of your current workflow you like or dislike\nCritique your own work and note ideas to improve (I will probably do this a lot in class!).\nCreate a repository of ideas and code for future research.\n\n\n\nFinal Exam\nThe final exam for this course is instead a final project, due at the day and time of the scheduled final exam. The last day of the course will (likely) be used for presentations on the final project in order to receive feedback from the class and instructor.\nAdditional information on the project will be provided as a separate document on Canvas, announced in week 4 or 5. The project will not be long and the goal will be for you create a document outlining your workflow.\nTo ensure that your workflows are as effective as possible, this will proceed in five parts:\n\nInitial proposal of an idea submitted via Canvas.\nUpdated proposal submitted via Canvas.\n5-10 minute presentation to the class on the last day of the course (10% of your grade).\nFinal Project (20%).\n\n\n\nEvaluation and Grading Scale\nAll grades will be posted on Canvas. You are strongly encouraged to check your scores in Canvas regularly. A final letter grade will be assigned based on percentages.\n\n\n\nAssignment Weights\nPercent\n\n\nClass Participation\n20%\n\n\nProblem Sets\n40%\n\n\nFinal Project Proposal\n10%*\n\n\nClass Presentation\n10%*\n\n\nFinal Project\n20%*\n\n\nTotal\n100%\n\n\n\n* If presentations are omitted, proposals will be worth 15% and Final Projects 25%.\n\nGrading Scale\n92.5% - 100% = A; 89.5% - 92.4% = A-\n87.5% - 89.4% = B+; 82.5% - 87.4% = B; 79.5% - 82.4% = B-\n77.5% - 79.4% = C+; 72.5% - 77.4% = C; 69.5% - 72.4% = C-\n67.5% - 69.4% = D+; 62.5% - 67.4% = D; 59.5% - 62.4% = D-\n0% - 59.4% = F"
  },
  {
    "objectID": "syllabus.html#course-policies-and-procedures",
    "href": "syllabus.html#course-policies-and-procedures",
    "title": "Syllabus",
    "section": "Course Policies and Procedures",
    "text": "Course Policies and Procedures\nMany of the below are also outlined in the UC Davis Code of Academic Conduct.\n\nAttendance Policy\nWhen you miss class, you miss important information, not all of which will be available in the zoom recordings. This course is only 10 class meetings, so each meeting comprises 10% of your in-class time. If you need to miss more than one class, I suggest considering whether taking this course in a future term. I will teach this course either annually or biennially, so there will be future opportunities to take this course in many cases (e.g., for example, if you are a second year student who will miss two meetings, taking the course in your fourth year may be more effective).\n\n\nLate Work/Make-up Policy\nLate work will be allowed per instructor discretion. Please try to proactively communicate these needs. Assignments due at midnight will have a 9 hour “grace period” with no penalty. Each day late is subject to a 20% drop in course grade (e.g., a 10-point response is worth 8 points on day 1 late, 6 points on day 2 late, etc.).\n\n\nAcademic Integrity\nYou are expected to practice the highest possible standards of academic integrity. Any deviation from this expectation will result in a minimum academic penalty of your failing the assignment, and will result in additional disciplinary measures. This includes improper citation of sources, using another student’s work, and any other form of academic misrepresentation.\n\nPlagiarism\nUsing the words or ideas of another as if they were one’s own is a serious form of academic dishonesty. If another person’s complete sentence, syntax, key words, or the specific or unique ideas and information are used, one must give that person credit through proper citation.\n\n\n\nIncomplete Grades\nYou may assigned an ‘I’ (Incomplete) grade if you are unable to complete some portion of the assigned course work because of an unanticipated illness, accident, work-related responsibility, family hardship, or verified learning disability. An Incomplete grade is not intended to give you additional time to complete course assignments or extra credit unless there is indication that the specified circumstances prevented you from completing course assignments on time.\n\n\nInstructional Methods\nThe course will be taught using multiple instructional methods. I will typically briefly (45-50 minutes) lecture at the beginning of the class on conceptual topics related to data cleaning and management. We will then have a 75 minute workshop, which will be a mix of going through code and examples together and working in small groups (if preferred) on short exercises. The remainder of the class will be available to receive support on Problem Sets for that week and other general questions (optional). The proportion of these will vary by week and portions of the course will be shortened or dropped as needed.\n\n\nDiversity and Inclusion\nThe university is committed to a campus environment that is inclusive, safe, and respectful for all persons. To that end, all course activities will be conducted in an atmosphere of friendly participation and interaction among colleagues, recognizing and appreciating the unique experiences, background, and point of view each student brings. You are expected at all times to apply the highest academic standards to this course and to treat others with dignity and respect.\n\nAccessibility, Disability, and Triggers [credit to Dr. David Moscowitz]\nI am committed to ensuring course accessibility for all students. If you have a documented disability and expect reasonable accommodation to complete course requirements, please notify me at least one week before accommodation is needed. Please also provide SDRC (https://sc.edu/about/offices_and_divisions/student_disability_resource_center/) documentation to me before requesting accommodation. Likewise, if you are aware of cognitive or emotional triggers that could disrupt your intellectual or mental health, please let me know so that I can be aware in terms of course content. \nAbsences for Personal or Religious Holidays\nI am committed to allowing students to exercise their rights to religious freedom. Accommodations on assignment due dates and absences will be allowed for students observing religious holidays that fall on course days. Please email me to let me know ahead of time to allow for accommodations to be made.\n\n\nTitle IX and Gendered Pronouns [credit to Dr. David Moscowitz]\nThis course affirms equality and respect for all gendered identities and expressions. Please don’t hesitate to correct me regarding your preferred gender pronoun and/or name if different from what is indicated on the official class roster. Likewise, I am committed to nurturing an environment free from discrimination and harassment. Consistent with Title IX policy, please be aware that I as a responsible employee am obligated to report information that you provide to me about a situation involving sexual harassment or assault. \n\n\nValues [credit to Dr. David Moscowitz]\nTwo core values, inquiry and civility, govern our class. Inquiry demands that we all cultivate an open forum for exchange and substantiation of ideas. Strive to be creative, to take risks, and to challenge our conventional wisdom when you see the opportunity. Civility supports our inquiry by demanding ultimate respect for the voice, rights, and safety of others. Threatening or disruptive conduct may result in course and/or university dismissal. Civility also presumes basic courtesy: please be well rested, on time, and prepared for class (class time also includes a break to use the restroom, etc.), which includes silencing all personal devices. \nMy perspective is that we never cease being students of this world, so I believe that attentive, reflective people always have something to learn from others. Good discussions can be energetic and passionate but are neither abusive nor offensive. Vibrant, vigorous inquiry derives from discussions that:\n\nchallenge, defend, and apply different ideas, theories, perspectives, and skills,\nextend a body of knowledge into different arenas and applications, and\nresult in a synergy that compels us to seek resolution to these discussions.\n\n\n\n\nCopyright/Fair Use\nI will cite and/or reference any materials that I use in this course that I do not create.  You, as students, are expected to not distribute any of these materials, resources, homework assignments, etc. (whether graded or ungraded) without permission from the instructor."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Course Schedule\nNote: Course schedule is subject to change without notice.\n\n\n\nDay\nDate\nTopic\nDue Today\n\n\n\n\nFirst Day of Classes September 27\n\n\n\n1\n10/02/2023\nLecture: Basics of Workflow\nWorkshop: Introduction to R & Workflow Basics; Quarto\nReadings:\n-       r4ds: Ch. 3, 5, 7, 29\nZoom Recording\n\n\n\n2\n10/09/2023\nLecture: Reproducibility and Workflow Values\nWorkshop: Data Transformation: Introduction to dplyr\nReadings:\n-       r4ds: Ch. 4\nZoom Recording\nProblem Set 1 Due\n\n\n3\n10/16/2023\nLecture: Understanding and Assessing Data Quality\nWorkshop: Reshaping and Joining: Introduction to tidyr\nReadings:\n-       r4ds: Ch. 6\nZoom Recording\nProblem Set 2 Due\n\n\n4\n10/23/2023\nLecture: Documenting Data and Procedures\nWorkshop: Using Codebooks to Aid Data Import\nReadings:\n-       r4ds: Ch. 8, 21, 24\nZoom Recording in 2 parts due to technical issues (Part 1; Part 2)\nProblem Set 3 Due\n\n\n5\n10/30/2023\nLecture: Functions\nWorkshop: Iteration: Introduction to purrr\nReadings:\n-       r4ds: Ch. 26, 27\nProblem Set 4 Due\n\n\n6\n11/06/2023\nLecture: Data Structures in R\nWorkshop: Data Transformation: Dates, Strings, regex, and Other Tricky Classes\nReadings: r4ds: Ch. 13-19\nProblem Set 5 Due\n\n\n7\n11/13/2023\nLecture: Using R Resources Efficiently\nWorkshop: Parallelization: Introduction to future and furrr\nReadings:\n-       https://dcgerard.github.io/advancedr/09_future.html\nProblem Set 6 Due\n\n\n8\n11/20/2023\nLecture: GitHub and Versioning\nWorkshop: (Functional) Tables & Figures\nReadings:\n-       TBD (Probably none )\nProblem Set 7 Due\nPROPOSALS DUE\n\n\n\n\nThanksgiving Break\n\n\n\n9\n11/27/2023\nLecture: None (in class help with projects instead)\nWorkshop: Odds, Ends, and Requested Topics\nReadings:\n-       TBD (Probably none )\nProblem Set 8 Due\n\n\n10\n12/04/2023\nIn-Class Presentations\n\n\n\n\n\nFinal Exam"
  },
  {
    "objectID": "05-week5-workbook.html#why-write-your-own-functions",
    "href": "05-week5-workbook.html#why-write-your-own-functions",
    "title": "Week 5 Workbook",
    "section": "Why Write Your Own Functions?",
    "text": "Why Write Your Own Functions?\n\nAutomate your workflow\nPrevent copy-paste errors (only need to update the function, not update the same task in a bunch of areas)\nSaves you time by providing tools that port to new projects\nFunctions make you think, which improves the quality of our work"
  },
  {
    "objectID": "05-week5-workbook.html#when-should-you-write-your-own-functions",
    "href": "05-week5-workbook.html#when-should-you-write-your-own-functions",
    "title": "Week 5 Workbook",
    "section": "When Should You Write Your Own Functions?",
    "text": "When Should You Write Your Own Functions?\n\nHadley Wickham’s rule of thumb (which is common in a lot of CS and DS circles) is that if you have to copy-paste something more than twice, you should write a function\nBut really you can write one whenever you’d like\nI often write functions when a link of code starts getting because it’s usually a good signal that I’m trying to do too much at once."
  },
  {
    "objectID": "05-week5-workbook.html#types-of-functions",
    "href": "05-week5-workbook.html#types-of-functions",
    "title": "Week 5 Workbook",
    "section": "Types of Functions",
    "text": "Types of Functions\n\nVector functions take one or more vectors as input and return a vector as output.\nData frame functions take a data frame as input and return a data frame as output.\nPlot functions that take a data frame as input and return a plot as output.\nand so on…\nWe’ll talk extensively about this again when we talk about building figures and tables into your workflow in Week 8"
  },
  {
    "objectID": "05-week5-workbook.html#vector-functions",
    "href": "05-week5-workbook.html#vector-functions",
    "title": "Week 5 Workbook",
    "section": "Vector Functions",
    "text": "Vector Functions\n\nIn my work, I often want to POMP (Percentage Of Maximum Possible) score or z-score (standardize) variables, which allows me to get data on a standard and interpretable scale (percentage or standard deviations)\nI often want to do this with lots of variables, so to do each one separately would take forever and introduce chances for error.\n\n\nCodebfi %>%\n  select(matches(\"1$\")) %>%\n  mutate(\n    E1 = (E1 - min(E1, na.rm = T))/(max(E1, na.rm = T) - min(E1, na.rm = T))*100\n    , A1 = (A1 - min(A1, na.rm = T))/(max(A1, na.rm = T) - min(A1, na.rm = T))*100\n    , C1 = (C1 - min(C1, na.rm = T))/(max(C1, na.rm = T) - min(C1, na.rm = T))*100\n    , N1 = (N1 - min(N1, na.rm = T))/(max(N1, na.rm = T) - min(N1, na.rm = T))*100\n    , O1 = (O1 - min(O1, na.rm = T))/(max(O1, na.rm = T) - min(O1, na.rm = T))*100\n  ) %>%\n  head(10)\n\n\n\n  \n\n\n\n\nInstead we could make this a function because I don’t want to talk about how long it took me to do that copy-pasting 😭\n\n\nCodepomp <- function(x, na) (x - min(x, na.rm = na))/(max(x, na.rm = na) - min(x, na.rm = na))*100\nbfi %>%\n  select(matches(\"1$\")) %>%\n  mutate(\n    E1 = pomp(E1, T)\n    , A1 =  pomp(A1, T)\n    , C1 =  pomp(C1, T)\n    , N1 =  pomp(N1, T)\n    , O1 =  pomp(O1, T)\n  ) %>%\n  head(10)\n\n\n\n  \n\n\n\n\nAnd remember, we could simplify this even more with mutate_at()!\nAnd I know in this example it saved us about four lines of code, but having a function like this that you can just reference, especially in conjunction with mutate_at() saves you time in the short and long run.1\n\n\n\nCodepomp <- function(x, na) (x - min(x, na.rm = na))/(max(x, na.rm = na) - min(x, na.rm = na))*100\nbfi %>%\n  select(matches(\"1$\")) %>%\n  mutate_at(vars(matches(\"1$\")), pomp, na = T) %>%\n  head(10)"
  },
  {
    "objectID": "05-week5-workbook.html#writing-functions",
    "href": "05-week5-workbook.html#writing-functions",
    "title": "Week 5 Workbook",
    "section": "Writing Functions",
    "text": "Writing Functions\n\nThe first step in writing a function is to find the pattern:\nLet’s look back at the code above:\n\n\nCodeE1 = (E1 - min(E1, na.rm = T))/(max(E1, na.rm = T) - min(E1, na.rm = T))*100\nA1 = (A1 - min(A1, na.rm = T))/(max(A1, na.rm = T) - min(A1, na.rm = T))*100\nC1 = (C1 - min(C1, na.rm = T))/(max(C1, na.rm = T) - min(C1, na.rm = T))*100\nN1 = (N1 - min(N1, na.rm = T))/(max(N1, na.rm = T) - min(N1, na.rm = T))*100\nO1 = (O1 - min(O1, na.rm = T))/(max(O1, na.rm = T) - min(O1, na.rm = T))*100\n\n\n\nDo you see the pattern?\n\n\nCode(█ - min(█, na.rm = TRUE)) / (max(█, na.rm = TRUE) - min(█, na.rm = TRUE))\n\n\nWe need three things to turn this into a function:\n\n\nA name. Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1.\n\nThe arguments. The arguments are things that vary across calls and our analysis above tells us that we have just one. We’ll call it x because this is the conventional name for a numeric vector.\n\nThe body. The body is the code that’s repeated across all the calls.\n\n\nThe template of a function looks something like this:\n\n\nCodename <- function(arguments){\n  body\n}\n\n\n\nAlthough for local functions, I suggest something like this:\n\n\nCodename <- function(\n    argument1 # description of first argument\n    , argument2 # description of second argument\n    ){\n  body\n}\n\n\n\nThat’s not going to fit well on my slides, so I will minimally do it in class.\nFollowing this, we return to our function:\n\n\nCodepomp <- function(x, na) {\n  (x - min(x, na.rm = na))/(max(x, na.rm = na) - min(x, na.rm = na))*100\n}"
  },
  {
    "objectID": "05-week5-workbook.html#function-efficiency",
    "href": "05-week5-workbook.html#function-efficiency",
    "title": "Week 5 Workbook",
    "section": "Function Efficiency",
    "text": "Function Efficiency\n\nFor most of our purposes, we may not care that much about how fast our code is\nUnless you’re working with huge data, things typically run fast\nBut with a function like POMP, I may need to run it on 100+ variables each with 10,000+ observations, so speed may be a consideration\nSo how do you speed up your functions?\n\nAvoid computing something twice\nTry to anticipate where things could go wrong\n\n\nSo in our function, we calculate min() twice and min() once\nWe could just calculate range() instead, which reduces that computation time\nWatch (note you have to run this in your console to see the difference)!\n\n\n\n\nCodepomp <- function(x) {\n  (x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T))*100\n}\n\nstart <- Sys.time()\ntmp <- bfi %>%\n  select(matches(\"1$\")) %>%\n  mutate_at(vars(matches(\"1$\")), pomp)\n(diff1 <- Sys.time() - start)\n\nTime difference of 0.01908612 secs\n\n\n\n\n\nCodepomp <- function(x) {\n  rng <- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])*100\n}\n\nstart <- Sys.time()\ntmp <- bfi %>%\n  select(matches(\"1$\")) %>%\n  mutate_at(vars(matches(\"1$\")), pomp)\n(diff2 <- Sys.time() - start)\n\nTime difference of 0.02045012 secs\n\n\n\n::::\n\nBut those differences are really minimal right? Just -0.0014 seconds.\nBut across 150 variables that’s -0.2046 seconds."
  },
  {
    "objectID": "05-week5-workbook.html#cautionary-note-on-outputs",
    "href": "05-week5-workbook.html#cautionary-note-on-outputs",
    "title": "Week 5 Workbook",
    "section": "Cautionary note on outputs",
    "text": "Cautionary note on outputs\n\nThe function we wrote above could be a “mutate()” function because the output is the same length / size as the input.\nIf you write a function that drops cases, you will get an error because R can’t make the resulting vector fit back into the data frame\nOr, if you write a function that returns length 1 within mutate, it will just repeat that value for every row\nSo, just be careful to think about what the output is and make sure that it matches the use case you are working with!"
  },
  {
    "objectID": "05-week5-workbook.html#more-notes-on-outputs",
    "href": "05-week5-workbook.html#more-notes-on-outputs",
    "title": "Week 5 Workbook",
    "section": "More notes on outputs",
    "text": "More notes on outputs\n\nYou can output anything you want from a function!\n\ndata frames\nvectors\nsingle values\nstrings\nlists (very common for base R and package functions!!)\nmodel objects\nplots\ntables\netc.!"
  },
  {
    "objectID": "05-week5-workbook.html#you-try",
    "href": "05-week5-workbook.html#you-try",
    "title": "Week 5 Workbook",
    "section": "You Try:",
    "text": "You Try:\nTry turning the following into functions:\n\nCodemean(is.na(x))\nmean(is.na(y))\nmean(is.na(z))\n\nx / sum(x, na.rm = TRUE)\ny / sum(y, na.rm = TRUE)\nz / sum(z, na.rm = TRUE)\n\nround(x / sum(x, na.rm = TRUE) * 100, 1)\nround(y / sum(y, na.rm = TRUE) * 100, 1)\nround(z / sum(z, na.rm = TRUE) * 100, 1)\n\n(x - mean(x, na.rm = T))/sd(x, na.rm = T)\n(y - mean(y, na.rm = T))/sd(y, na.rm = T)\n(z - mean(z, na.rm = T))/sd(z, na.rm = T)\n\n\n\nCodeprop_missing <- function(x) mean(is.na(x))\n\nprop_total   <- function(x) x / sum(x, na.rm = T)\n\nround_prop   <- function(x) round(prop_total(x)*100, 1)\n\nz_score      <- function(x) (x - mean(x, na.rm = T))/sd(x, na.rm = T)"
  },
  {
    "objectID": "05-week5-workbook.html#data-frame-functions",
    "href": "05-week5-workbook.html#data-frame-functions",
    "title": "Week 5 Workbook",
    "section": "Data Frame Functions",
    "text": "Data Frame Functions\n\nAll the functions we’ve covered so far are vector functions (i.e. they input a vector, not a matrix or data frame and work well within mutate() calls)\nBut if you have a long string of dplyr verbs, it can be useful to put these into a data frame function where you provide a data frame input and flexible way of naming the columns\nI’m going to give a brief example here, but suggest that you check out more in R for Data Science.\nLet’s start with a brief example. To do it, let’s first make the bfi data frame long\n\n\nCodebfi_long <- bfi %>%\n  rownames_to_column(\"SID\") %>%\n  pivot_longer(\n    cols = c(-SID, -education, -gender, -age)\n    , names_to = c(\"trait\", \"item\")\n    , names_sep = -1\n    , values_to = \"value\"\n    , values_drop_na = T\n  )\nhead(bfi_long, 8)\n\n\n\n  \n\n\n\n\nSo maybe I want to get means for each of the Big Five from this, so I write a function like this:\n\n\nCodegrouped_mean <- function(df, group_var, mean_var) {\n  df %>%\n    group_by(group_var) %>%\n    summarize(mean(mean_var))\n}\n\nbfi_long %>% grouped_mean(trait, value)\n\n\n\nThat didn’t work because we can’t specify grouping variables like that\nTo get around it, we have to use something called embracing: We have to wrap the variables like this {{ trait }}\nThis just nudges the dplyr functions to look inside the data frame for the column you specify\n\n\nCodetidy_describe <- function(df, var) {\n  df %>%\n    summarize(\n      mean   = mean({{ var }},   na.rm = TRUE),\n      sd     = sd({{ var }},     na.rm = TRUE),\n      median = median({{ var }}, na.rm = TRUE),\n      min    = min({{ var }},    na.rm = TRUE),\n      max    = max({{ var }},    na.rm = TRUE),\n      n      = n(),\n      n_miss = sum(is.na({{ var }})),\n      .groups = \"drop\"\n      )\n}\n\nbfi_long %>% \n  group_by(trait) %>%\n  tidy_describe(value)\n\n\n\n  \n\n\n\n\nOr remember when I had us getting counts and proportions for continuous x categorical relationships?\n\n\nCodecount_prop <- function(df, var, sort = FALSE) {\n  df %>%\n    count({{ var }}, sort = sort) %>%\n    mutate(prop = n / sum(n))\n}\n\nbfi_long %>% \n  count_prop(education)"
  },
  {
    "objectID": "05-week5-workbook.html#wrap-up",
    "href": "05-week5-workbook.html#wrap-up",
    "title": "Week 5 Workbook",
    "section": "Wrap-Up",
    "text": "Wrap-Up\n\nThis is just a brief introduction to functions.\nFunctions are absolutely essential part of workflows, and you’ll see them pop up in every lesson from here on out (and you already saw them pop up in previous lessons)\nAs we continue to see them, I’ll ramp up their complexity, showing you how to write functions for estimating models, model predictions, figures, tables, and more"
  },
  {
    "objectID": "05-week5-workbook.html#iteration",
    "href": "05-week5-workbook.html#iteration",
    "title": "Week 5 Workbook",
    "section": "Iteration",
    "text": "Iteration\nIteration is everywhere. It underpins much of mathematics and statistics. If you’ve ever seen the \\(\\Sigma\\) symbol, then you’ve seen (and probably used) iteration.\nReasons for iteration:\n- reading in multiple files from a directory\n- running the same operation multiple times\n- running different combinations of the same model\n- creating similar figures / tables / outputs"
  },
  {
    "objectID": "05-week5-workbook.html#for-loops",
    "href": "05-week5-workbook.html#for-loops",
    "title": "Week 5 Workbook",
    "section": "\nfor loops",
    "text": "for loops\nEnter for loops. for loops are the “OG” form of iteration in computer science. The basic syntax is below. Basically, we can use a for loop to loop through and print a series of things.\n\nCodefor(i in letters[1:5]){\n  print(i)\n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n\nThe code above “loops” through 5 times, printing the iteration letter."
  },
  {
    "objectID": "05-week5-workbook.html#apply-family",
    "href": "05-week5-workbook.html#apply-family",
    "title": "Week 5 Workbook",
    "section": "\n_apply() family",
    "text": "_apply() family\n\nA somewhat faster version of for loops comes from the _apply() family of functions, including:\n\n\napply(), lapply(), sapply(), and mapply(). Unlike for loops, these are vectorized, which makes them more efficient.\n\n\n\n\nCodelapply(letters[1:5], print)\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] \"b\"\n\n[[3]]\n[1] \"c\"\n\n[[4]]\n[1] \"d\"\n\n[[5]]\n[1] \"e\"\n\nCodesapply(letters[1:5], print)\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n  a   b   c   d   e \n\"a\" \"b\" \"c\" \"d\" \"e\" \n\nCodemapply(print, letters[1:5])\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n  a   b   c   d   e \n\"a\" \"b\" \"c\" \"d\" \"e\""
  },
  {
    "objectID": "05-week5-workbook.html#purrr-and-_map_-functions",
    "href": "05-week5-workbook.html#purrr-and-_map_-functions",
    "title": "Week 5 Workbook",
    "section": "\npurrr and _map_() functions",
    "text": "purrr and _map_() functions\n\nToday, though, we’ll focus on the map() family of functions, which is the functions through which purrr iterates.\n\n\nCodemap(letters[1:5], print)\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] \"b\"\n\n[[3]]\n[1] \"c\"\n\n[[4]]\n[1] \"d\"\n\n[[5]]\n[1] \"e\"\n\n\n\nFor a more thorough comparison of for loops, the _apply() family, and _map_() functions, see https://jennybc.github.io/purrr-tutorial/"
  },
  {
    "objectID": "05-week5-workbook.html#purrr-and-_map_-predicates",
    "href": "05-week5-workbook.html#purrr-and-_map_-predicates",
    "title": "Week 5 Workbook",
    "section": "\npurrr and _map_() predicates",
    "text": "purrr and _map_() predicates\n\nToday, though, we’ll focus on the map() family of functions, which is the functions through which purrr iterates.\n\n\nCodemap(letters[1:5], print)\n\n\n\nNote that this returns a list, which we may not always want.\nWith purrr, we can change the kind of output of map() by adding a predicate, like lgl, dbl, chr, and df.\nSo in the example above, we may have wanted just the characters to print. To do that we’d call map_chr():\n\n\nCodemap_chr(letters[1:5], print)\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n\n\nNote that it also returns the concatenated character vector as well as printing each letter individually (i.e. iteratively)."
  },
  {
    "objectID": "05-week5-workbook.html#purrr-and-_map_-antecedents",
    "href": "05-week5-workbook.html#purrr-and-_map_-antecedents",
    "title": "Week 5 Workbook",
    "section": "\npurrr and _map_() antecedents",
    "text": "purrr and _map_() antecedents\n\nHow many mappings?\n\nSingle mapping: map_()\n\nParallel (2) mapping(s): map2_()\n\n3 or more mappings: pmap_()\n\n\n\n\n\nCodemap2_chr(letters[1:5], 1:5, paste)\n\n[1] \"a 1\" \"b 2\" \"c 3\" \"d 4\" \"e 5\"\n\n\n\nNote here that we can use map2() and pmap() with the predicates from above."
  },
  {
    "objectID": "05-week5-workbook.html#list-columns-and-the-power-of-purrr",
    "href": "05-week5-workbook.html#list-columns-and-the-power-of-purrr",
    "title": "Week 5 Workbook",
    "section": "List Columns and the Power of purrr\n",
    "text": "List Columns and the Power of purrr\n\n\nOn the previous slide, we saw a data frame inside of a data frame. This is called a list column within a nested data frame.\nIn this case, we created a list column using map, but one of the best things about purrr is how it combines with the nest() and unnest() functions from the tidyr package.\nWe’ll return to nest() later to demonstrate how anything you would iterate across is also something we can nest() by in long format data frames."
  },
  {
    "objectID": "05-week5-workbook.html#use-cases",
    "href": "05-week5-workbook.html#use-cases",
    "title": "Week 5 Workbook",
    "section": "Use Cases",
    "text": "Use Cases\n\nReading Data\n\nCleaning Data\nRunning Models\n\n(Plotting Figures - Week 8)\n\n(Creating Tables - Week 8)\n\nReading Data\nThere are a number of different cases where purrr and map() maybe useful for reading in data including:\n\nsubject-level files for an experimental task\n\nsubject- and task-level files for an experimental task\nEMA data\n\nlongitudinal data\n\nweb scraping and text mining\n\nReading Data: Participant-Level EMA\nFor this first example, I’ll show you how this would look with a for loop before I show you how it looks with purrr.\nAssuming you have all the data in a single folder and the format is reasonably similar, you have the following basic syntax:\n\nCodefiles <- list.files(data_path)\ndata <- list()\nfor(i in files){\n  data[[i]] <- read_csv(i)\n}\ndata <- combine(data)\n\n\n\nThis works fine in this simple case, but where purrr really shines in when you need to make modifications to your data before combining, whether this be recoding, removing missing cases, or renaming variables.\nBut first, the simple case of reading data. The code below will download a .zip file when you run it. Once, you do, navigate to your Desktop to unzip the folder. Open the R Project and you should be able to run the rest of the code\n\n\nCodedata_source <- \"https://github.com/emoriebeck/psc290-data-FQ23/raw/main/04-workshops/05-week5-purrr/05-week5-purrr.zip\"\ndata_dest <- \"~/Desktop/05-week5-purrr.zip\"\ndownload.file(data_source, data_dest)\n\n\n\nCodedf1 <- tibble(\n  ID = list.files(\"data/example_1\")\n  ) %>%\n  mutate(data = map(ID, ~read_csv(sprintf(\"data/example_1/%s\", .)))) %>%\n  unnest(data) \n\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 68 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 65 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 61 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 79 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 53 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 37 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 23 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 32 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 59 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 43 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 51 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 50 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 31 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 36 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 19 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 75 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 60 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 54 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 49 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 44 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nThe code above creates a list of ID’s from the data path (files named for each person), reads the data in using the map() function from purrr, removes the “.csv” from the ID variable, then unnests the data, resulting in a data frame for each person.\nNow, we’re going to combine with what we learned about last time with codebooks.\n\n\nCodecodebook <- read_csv(\"data/codebook_ex1.csv\")\n\nRows: 11 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): old_name, name, short_name, item_name, description, scale, reverse_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodecodebook\n\n\n\n  \n\n\n\n\nNow, that we have a codebook, what are the next steps?\n\n\nCodedf1 <- tibble(\n  ID = list.files(\"data/example_1\")\n  ) %>%\n  mutate(data = map(ID, ~read_csv(sprintf(\"data/example_1/%s\", .)))) %>%\n  unnest(data) \n\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 68 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 65 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 61 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 79 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 53 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 37 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 23 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 32 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 59 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 43 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 51 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 50 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 31 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 36 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 19 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 75 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 60 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 54 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 49 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 44 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodehead(df1, 6)\n\n\n\n  \n\n\n\n\npull old names in raw data from codebook\n\npull new names from codebook\n\nselect columns from codebook in loaded data\n\nrename columns with new names\n\n\nCodeold.names <- codebook$old_name # pull old names in raw data from codebook  \nnew.names <- codebook$item_name # pull new names from codebook  \ndf1 <- tibble(\n  ID = list.files(\"data/example_1\")\n  ) %>%\n  mutate(data = map(ID, ~read_csv(sprintf(\"data/example_1/%s\", .)))\n         , ID = str_remove_all(ID, \".csv\")) %>%\n  unnest(data) %>%\n  select(ID, count, all_of(old.names)) %>% # select columns from codebook in loaded data  \n  setNames(c(\"ID\", \"count\", new.names)) # rename columns with new names  \n\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 68 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 65 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 61 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 79 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 53 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 37 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 23 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 32 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 59 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 43 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 51 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 50 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 31 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 36 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 19 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 75 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 60 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 54 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 49 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 44 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodehead(df1, 6)\n\n\n\n  \n\n\n\nDescriptives: Within-Person Correlations\nWith these kinds of data, the first thing, we may want to do is look at within-person correlations, which we can do with purrr.\n\nCodecor_fun <- function(x) cor(x %>% select(-count), use = \"pairwise\")\n\nnested_r <- df1 %>%\n  group_by(ID) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(r = map(data, cor_fun))\nhead(nested_r, 6)\n\n\n\n  \n\n\n\nWe can access it like a list:\n\nCodenested_r$data[[1]]\n\n\n\n  \n\n\n\n\nBut we can’t easily (well, nicely) just unnest() a matrix\nWe’ve lost a lot of information along the way\nSo what do we do?\n\n\nCodenested_r %>%\n  select(-data) %>%\n  unnest(r)\n\n\n\n  \n\n\n\n\nWrite a function, of course!\n\n\nCodecor_fun <- function(x) {\n  r <- cor(x %>% select(-count), use = \"pairwise\")\n  r %>% \n    data.frame() %>%\n    rownames_to_column(\"var\") %>%\n    as_tibble()\n}\n\nnested_r <- nested_r %>%\n  mutate(r = map(data, cor_fun))\nhead(nested_r, 10)\n\n\n\n  \n\n\n\n\nLet’s try unnesting again:\n\n\nCodenested_r %>%\n  select(-data) %>%\n  unnest(r) %>% \n  arrange(desc(var))\n\n\n\n  \n\n\n\n\nThere’s more I would usually do here to format a correlation table for each participant and output the file as a PDF or html so I can post it on GitHub / OSF\nBut we’ll get there in Week 8/9!\nDescriptives: Means, sds, etc.\n\nCodetidy_describe <- function(df) {\n  df %>%\n    pivot_longer(\n      -count\n      , names_to = \"item\"\n      , values_to = \"value\"\n      , values_drop_na = T\n      ) %>%\n    group_by(item) %>%\n    summarize(\n      mean   = mean(value,   na.rm = TRUE),\n      sd     = sd(value,     na.rm = TRUE),\n      median = median(value, na.rm = TRUE),\n      min    = min(value,    na.rm = TRUE),\n      max    = max(value,    na.rm = TRUE),\n      n      = n(),\n      n_miss = sum(is.na(value)),\n      .groups = \"drop\"\n      )\n}\n\n\n\nCodenested_r <- nested_r %>%\n  mutate(desc = map(data, tidy_describe)) \nnested_r\n\n\n\n  \n\n\n\n\nCodenested_r %>%\n  select(-data, -r) %>%\n  unnest(desc)\n\n\n\n  \n\n\n\nModels\n\nWe can put essentially anything into a nested data frame.\nThe magic happens because everything is indexed by the other columns in the data frame, so we can keep track of it\nAnd unlike a normal list, we aren’t stuck with nested list structures that are really hard to parse and navigate through\nNext, I’m going to show you how to use purrr with models\nModeling is not a focus of this class, but I want to demonstrate this as a workflow because it completely revolutionized mine!\nBut first, we need to format our data:\n\n\nCodedf_long <- df1 %>%\n  select(-satisfaction) %>%\n  pivot_longer(\n    cols = c(-count, -ID)\n    , names_to = c(\"trait\", \"item\")\n    , names_sep = \"_\"\n    , values_to = \"value\"\n    , values_drop_na = T\n    )\ndf_long\n\n\n\n  \n\n\n\nTo create composites, we’ll:\n1. separate traits from items 2. group_by() trait, count, and ID 3. calculate the composites using summarize()\n\nCodedf_long <- df_long %>%\n  group_by(ID, count, trait) %>%\n  summarize(value = mean(value)) %>%\n  ungroup() %>%\n  left_join(df1 %>% select(ID, count, satisfaction))\n\n`summarise()` has grouped output by 'ID', 'count'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(ID, count)`\n\nCodedf_long\n\n\n\n  \n\n\n\nThen we’ll get within-person centered values using our own little function!\n\nCodecenter <- function(x) x - mean(x, na.rm = T)\n\ndf_long <- df_long %>%\n  group_by(ID, trait) %>%\n  mutate(value_c = center(value)) %>%\n  ungroup()\ndf_long\n\n\n\n  \n\n\n\nAnd grand-mean centered within-person averages\n\nCodedf_long <- df_long %>%\n  group_by(ID, trait) %>%\n  mutate(value_gmc = mean(value)) %>%\n  group_by(trait) %>%\n  mutate(value_gmc = center(value_gmc)) %>%\n  ungroup()\ndf_long\n\n\n\n  \n\n\n\nAnd now we are ready to run our models. But first, we’ll nest() our data.\n\nCodenested_mods <- df_long %>%\n  group_by(trait) %>%\n  nest() %>%\n  ungroup() \nnested_mods\n\n\n\n  \n\n\n\nAnd now run the models.\n\nCoderun_model <- function(d) lmer(satisfaction ~ value_c * value_gmc + (1 | ID), data = d)\n\nnested_mods <- df_long %>%\n  group_by(trait) %>%\n  nest() %>%\n  ungroup() %>% \n  mutate(model = map(data, run_model))\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\nCodenested_mods\n\n\n\n  \n\n\n\nAnd get data frames of the results:\n\nCodesprintfna <- function(x) ifelse(is.na(x), NA_character_, sprintf(\"%.2f\", x))\n\ntidy_tab <- function(m){\n  tidy(m, conf.int = T) %>%\n    mutate(pval = pnorm(abs(estimate/`std.error`), lower.tail = FALSE),\n           p = round(pval, digits = 3),\n           p = ifelse(pval < .001, \"p &lt; .001\", paste0(\"p = \", p))) %>%\n    mutate_at(vars(estimate, conf.low, conf.high), sprintfna) %>%\n    mutate(CI = ifelse(is.na(conf.low), \"\", sprintf(\"[%s,%s]\", conf.low, conf.high))) %>%\n    dplyr::select(term, estimate, CI, p)\n}\n\nnested_mods <- nested_mods %>%\n  mutate(tidy = map(model, tidy_tab))\nnested_mods\n\n\n\n  \n\n\n\nUnnesting\nWhich we can print into pretty data frames\n\nCodenested_mods %>%\n  select(trait, tidy) %>%\n  unnest(tidy)\n\n\n\n  \n\n\n\nUnnesting & Tabling\nWhich we can pretty easily turn into tables:\n\nCodemod_tab <- nested_mods %>%\n  select(trait, tidy) %>%\n  unnest(tidy) %>%\n  pivot_wider(\n    names_from = \"trait\"\n    , names_glue = \"{trait}_{.value}\"\n    , values_from = c(\"estimate\", \"CI\", \"p\")\n  ) %>%\n  select(term, starts_with(\"E\"), starts_with(\"A\"), starts_with(\"C\"), starts_with(\"N\"), starts_with(\"O\"))\n\nmod_tab\n\n\n\n  \n\n\n\n\nCodehdr <- c(1, rep(3, 5))\nnames(hdr) <- c(\" \", \"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Neuroticism\", \"Openness\")\n\nmod_tab <- mod_tab %>%\n  kable(.\n        , \"html\"\n        , escape = F\n        , col.names = c(\"Term\", rep(c(\"<em>b</em>\", \"CI\", \"<em>p</em>\"), times = 5))\n        , align = c(\"r\", rep(\"c\", 15))\n        , caption = \"<strong>Table 1</strong><br><em>Multilevel Model Estimates of Between- and Within-Person Big Five-State Satisfaction Associations\"\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\", font_size = 15) %>%\n  add_header_above(hdr)\nmod_tab\n\n\n\n\nTable 1Multilevel Model Estimates of Between- and Within-Person Big Five-State Satisfaction Associations\n\n \n\n\nExtraversion\nAgreeableness\nConscientiousness\nNeuroticism\nOpenness\n\n\n Term \n    b \n    CI \n    p \n    b \n    CI \n    p \n    b \n    CI \n    p \n    b \n    CI \n    p \n    b \n    CI \n    p \n  \n\n\n\n (Intercept) \n    2.98 \n    [2.93,3.03] \n    p < .001 \n    2.98 \n    [2.93,3.03] \n    p < .001 \n    2.97 \n    [2.92,3.02] \n    p < .001 \n    2.99 \n    [2.94,3.03] \n    p < .001 \n    2.97 \n    [2.93,3.02] \n    p < .001 \n  \n\n value_c \n    -0.07 \n    [-0.14,0.00] \n    p = 0.029 \n    -0.02 \n    [-0.10,0.05] \n    p = 0.279 \n    0.03 \n    [-0.05,0.10] \n    p = 0.231 \n    -0.01 \n    [-0.08,0.06] \n    p = 0.378 \n    0.02 \n    [-0.05,0.09] \n    p = 0.29 \n  \n\n value_gmc \n    0.17 \n    [0.03,0.32] \n    p = 0.008 \n    0.13 \n    [-0.05,0.31] \n    p = 0.081 \n    0.19 \n    [0.06,0.32] \n    p = 0.003 \n    -0.09 \n    [-0.19,0.01] \n    p = 0.032 \n    0.14 \n    [-0.04,0.33] \n    p = 0.067 \n  \n\n value_c:value_gmc \n    0.02 \n    [-0.17,0.22] \n    p = 0.399 \n    0.16 \n    [-0.12,0.44] \n    p = 0.127 \n    0.15 \n    [-0.04,0.34] \n    p = 0.061 \n    -0.19 \n    [-0.34,-0.03] \n    p = 0.009 \n    0.25 \n    [0.01,0.49] \n    p = 0.019 \n  \n\n sd__(Intercept) \n    0.00 \n     \n     \n    0.04 \n     \n     \n    0.00 \n     \n     \n    0.00 \n     \n     \n    0.03 \n     \n     \n  \n\n sd__Observation \n    1.16 \n     \n     \n    1.15 \n     \n     \n    1.17 \n     \n     \n    1.15 \n     \n     \n    1.16"
  },
  {
    "objectID": "05-week5-workbook.html#appendix-sourcing-functions",
    "href": "05-week5-workbook.html#appendix-sourcing-functions",
    "title": "Week 5 Workbook",
    "section": "Appendix: Sourcing Functions",
    "text": "Appendix: Sourcing Functions"
  },
  {
    "objectID": "final-project.html",
    "href": "final-project.html",
    "title": "Final Projects",
    "section": "",
    "text": "Due at 11:59 PM PST on November 19, 2022 on Canvas\n1-2 page (single spaced) proposal\n\nShort background (What are you hacking?)\nApproach (How you do plan to hack this? What “product” will you producing?)\nMotivation (Why are you hacking this particular problem / issue / question / procedure?)\nChallenges and barriers (what do you struggle with with bringing your project to life; are there specific barriers?)\n\n\nYou can also download this document here\n\n\nThe goal of this course is to build a set of tools for cleaning and managing your data. As discussed in class, a big part of that is to create a workflow for your research that is efficient, clear, and minimizes errors. The final project in the course aims to provide an opportunity for you to flesh out what that means for you and research. There’s no right or wrong way to build a sustainable workflow for yourself.\n\n\n\nThe project is open-ended, but some examples are:\n\nA preregistration template for your kind of data / research\nA quarto / Rmarkdown / R script / R Project template for the typical structure of research projects\nA data cleaning pipeline / template for your kind of data / research\nA set of functions meant to help you improve your research workflow\nWriting clear documentation for a new or previous data collection\nA data cleaning or analysis script that draws on different course concepts (e.g., directory structures, codebooks, functions, iteration / purrr, etc.)\nA “checks” / procedure checklist that documents robustness tests and aims to improve the accuracy and efficiency of your workflow\nAnything else that draws on things you learned in class and focuses on data cleaning, R, data management, procedures, GitHub, etc.\n\n\n\n\nFor the final project, you will be creating something to help improve your workflow. You can think of it sort of like a chance to “hack” your own work. I recommend choosing something you’ve been wanting to do but haven’t had time to prioritize and using this as an excuse to do so.\nAs mentioned above, this could be anything from finishing up cleaning some gnarly data you’ve had for awhile, redoing your documentation for a study that’s already been collected (or you plan to collect), building templates or procedures, etc. What I want from you for this project is something that is: * Useful to you, both short- and long-term * Challenging but doable (you’re better at this than you think!) * Something you feel proud of"
  }
]