---
title: "Week 10 - Review & Reflection"
author: "Emorie D Beck"
format: 
  revealjs:
    incremental: true
    code-tools: true
    code-copy: true
    code-line-numbers: true
    code-link: true
    preview-links: true
    slide-number: true
    self-contained: true
    fig-height: 4
    fig-width: 6
    fig-align: center
    css: custom.css
    theme: psc290-23
    # highlight-style: atom-one-dark
    margin-left: "0"
    margin-right: "0"
    width: 1400
    # height: 900
    footer: "PSC 290 - Data Management and Cleaning"
    logo: "https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/01-week1-intro/02-code/02-images/ucdavis_logo_blue.png"
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r, echo = F}
pkg <- c("knitr", "psych", "lavaan", "future", "plyr", "tidyverse", "furrr")
pkg <- pkg[!pkg %in% rownames(installed.packages())]
if(length(pkg) > 0) map(pkg, install.packages)

library(knitr)
library(psych)
library(lavaan)
library(future)
library(plyr)
library(tidyverse)
library(furrr) # note loading this last ONLY because it depends on tidyverse and will not mask it
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE,	
                      warning = FALSE,
                      results = 'show',
                      fig.width = 4, 
                      fig.height = 4, 
                      fig.retina = 3)
options(htmltools.dir.version = FALSE
        , knitr.kable.NA = "")
```

# Topics 

::::{.columns}
:::{.column}

1. Intro to Base R 
2. `dplyr`: Manipulating Data 
3. `tidyr`: Reshaping and Transforming Data
4. Codebooks 
5. `purrr` & Functions 

:::
:::{.column}

6. Review 
7. Strings, dates, & regex 
8. Functional Tables & Figures 
9. GitHub & Parallelization
10. Today 

:::
::::

# Lessons & Takeaways 

## Lesson 1 {.smaller}
### Always load tidyverse last 
Always load all packages **at the beginning of a script** 

```{r}
library(psych)
library(ggdist)
library(knitr)
library(kableExtra)
library(brms)
library(broom)
library(broom.mixed)
library(patchwork)
library(plyr)
library(tidyverse)
library(furrr)
```

- Note: `tidyverse` loads: `dplyr`, `forcats` (factors), `ggplot2`, `lubrdiate`, `purrr`, `readr`, `stringr`, `tibble`, and `tidyr` 
- This is good! It reduces the number of packages you have to load and ensures there's no order issues

---

### Deal with Conflicts 

- Use the `conflicts()` function to figure out what conflicts you have 
- Use `package::fnName()` to call a function directly without loading a package / to override conflicts 
  * e.g., `kableExtra` should be loaded before `tidyverse`, but then `tidyverse` masks `kableExtra::group_rows()`

:::{.fragment}

```{r, eval = F}
kable(tab) %>%
  kable_classic(html_font = "Times") %>%
  kableExtra::group_rows("Header", 1, 3)
```

:::

## Lesson 2 {.smaller}
### There is no single way to do anything 

The best way to do something is a way that you understand or you can introduce the mistakes you're trying to prevent 

::::{.columns}
:::{.column}

```{r, eval = F}
bfi %>%
  mutate(
    sid = 1:n(),
    E = rowMeans(pick(matches("E\\d")), na.rm = T), 
    A = rowMeans(pick(matches("A\\d")), na.rm = T), 
    C = rowMeans(pick(matches("C\\d")), na.rm = T), 
    N = rowMeans(pick(matches("N\\d")), na.rm = T), 
    O = rowMeans(pick(matches("O\\d")), na.rm = T)
  ) %>%
  ungroup() %>%
  select(sid, E:O)
```

:::
:::{.column}

```{r, eval = F}
bfi %>% 
  mutate(sid = 1:n()) %>%
  pivot_longer(
    cols = c(-sid, -gender, -education, -age)
    , names_to = c("trait", "item")
    , names_sep = -1
    , values_to = "value"
  ) %>%
  group_by(sid, trait) %>%
  summarize(value = mean(value, na.rm = T)) %>%
  pivot_wider(names_from = "trait", values_from = "value") %>%
  ungroup()
```

:::
::::

## Lesson 3
### Start at the end 
- What do you want your data to look like? 
- What do they look like now? 
- Now fill in the middle 

---

### Start at the end {.smaller}

HLM / MLM / MEM:  
RE ex: time, trial, stimuli, group, study, day, w/in person conditions 
FE ex: gender, baseline age, b/w subject conditions, country, etc. (can also be an RE) 

ID    |   RE1   |   RE2   |   DV    |   FE1   
----  |   ----  |   ----  |   ----  |   ----
1     |   1     |   1     |   4     |   3  
1     |   1     |   2     |   3     |   3  
1     |   2     |   1     |   2     |   3  
1     |   2     |   2     |   1     |   3  
2     |   1     |   1     |   5     |   1  
2     |   1     |   2     |   3     |   1  
2     |   2     |   1     |   1     |   1  
2     |   2     |   2     |   2     |   1  

--- 

### Start at the end {.smaller}

ID    |   RE_1_1   |   RE_1_2   |   RE_2_1   |   RE_2_2   |  FE2
----  |   ------  |   ------  |   ------  |   ------  |   ------  
1     |   4       |   3       |   2       |   1       |   3  
2     |   5       |   3       |   1       |   2       |   1

```{r, eval = F}
df %>% 
  pivot_longer(
    cols = RE_1_1:RE_2_2
    , names_to = c("RE1", "RE2")
    , names_sep = "_"
    , names_prefix = "RE_"
    , values_to = "DV"
  )
```

Note this is only possible / easy because of the naming scheme! If we had them named "RE1_1", this would not have been possible / would have been CONSIDERABLY more difficult 

## Lesson 4
### Don't be afraid to split your data into chunks 
- In alignment with starting at the end, a key strategy is knowing how you can **chunk** your data 
- No right or wrong way to chunk, but some examples are: 
  * items / values from the same scale / task (e.g., DV across trials / conditions)
  * baseline items (from other survey or from baseline wave)
  * outcome variables 
  * descriptive variables
  * item-level variables v. composites

## Lesson 5
### Joining data requires a key, so be thoughtful and you'll always be able to put the pieces together
- The most important thing when splitting data into chunks is to make sure you can put it back together 
- This requires one (e.g., participant ID) or more (e.g., participant ID, wave) keys that allows R to match the right values together
- This should be the last thing you do 
  * Please don't create mega datasets where you tack things on to the raw data as you go
  * This will eat RAM and make your life harder (and sometimes could end up in you accidentally sharing identifying information!!)

## Lesson 6 {.smaller}
### One of the most important skills is getting comfortable making data move flexibly from wide to long 
- Remember our example above? Without knowledge of how to do so, it would have been almost impossible 
- It's okay if it takes more than one step! That's better than manually moving stuff in excel and not creating a reproducible path!

```{r}
bfi %>% 
  mutate(sid = 1:n()) %>%
  pivot_longer(
    cols = c(-sid, -gender, -education, -age)
    , names_to = c("trait", "item")
    , names_sep = -1
    , values_to = "value"
  ) %>%
  group_by(sid, trait) %>%
  summarize(value = mean(value, na.rm = T)) %>%
  pivot_wider(names_from = "trait", values_from = "value") %>%
  ungroup()
```

## Lesson 7
### Establish a consistent naming scheme 
- label objects relative to a stage or research question
  * e.g., `nested_RQ1`, `RQ1_mods`, `raw_df`
  * this will help you clear your environment of clutter
- use temporary objects repeatedly 
  * e.g., if you need to use an object as an intermediary step, call it `tmp` and overwrite it as many times as is useful
  * You can always remove it using `rm(tmp)`
  
## Lesson 8
### You won't remember details about raw variables or variables you create, document them 
- Clearly document your raw data and planned transformations before (preregistration) or as (deviations or just reactive responses to aspects of the data) you clean your data 
- Clearly document all new variables you create, including their scale, etc. 

## Lesson 9 {.smaller}
### Reference data frames are great keys for ordering and renaming 
- Clearly documenting all new variables you create also creates the opportunity to create **reference data frames**, which can include variable names in the data, category information, longer names for the variables, descriptions of the scales, and the link function / column names e.g., 

cat         |   name        | scale   |   long_name           |   lab   
----        |   ----        | ----    |   ----                |   ----   
Outcome     |   dementia    |   0/1   |   Clinical Dementia   |   OR [CI] 
Outcome     |   braak       |   1-5   |   Braak Stage         |   est. [CI] 
Predictor   |   E           |   0-10  |   Extraversion        |   
Predictor   |   C           |   0-10  |   Conscientiousness   |   
Moderator   |   age         |   num   |   Baseline Age        |
Moderator   |   ses         |   1-7   |   Baseline SES        |

--- 

### Reference data frames are great keys for ordering and renaming 

```{r, eval = F}
out <- tribble(
  ~cat,     ~name,        ~scale,      ~long_name,              ~lab,   
"Outcome",  "dementia",   "0/1",       "Clinical Dementia",      "OR [CI]",
"Outcome",  "braak",      "1-5",       "Braak Stage",            "est. [CI]" 
)

tab %>%
  left_join(out %>% select(outcome = name, long_out = long_name, lab)) 
```


## Lesson 10
### Reorder everything using factors 

- Often, there are specific orders we want / need strings to be in; this is where factors come in 
- There's a whole package for this called [`forcats`](https://forcats.tidyverse.org). 
- Reference data frames are a great way to order your variables 

:::{.fragment}

```{r, eval = F}
out <- tribble(
  ~cat,     ~name,        ~scale,      ~long_name,              ~lab,   
"Outcome",  "dementia",   "0/1",       "Clinical Dementia",      "OR [CI]",
"Outcome",  "braak",      "1-5",       "Braak Stage",            "est. [CI]" 
)

tab %>%
  left_join(out %>% select(outcome = name, long_out = long_name, lab)) %>%
  mutate(long_out = factor(long_out, levels = out$long_name))
# mutate(long_out = factor(oucome, levels = out$name, labels = out$long_name))
```

:::

---

### Reorder everything using factors 

Remember this? 

```{r, eval = F}
terms <- tribble(
  ~path,     ~new,                         ~level
  "i~1",     "Intercept",                  "Fixed",
  "s~1",     "Slope",                      "Fixed",
  "i~~i",    "Intercept Variance",         "Random",
  "s~~s",    "Slope Variance",             "Random",
  "i~~s",    "Intercept-Slope Covariance", "Random"
)

extract_fun <- function(m, trait){
  p <- parameterEstimates(m) %>%
    data.frame()
  # saveRDS(p, file = sprintf("results/summary/%s.RDS", trait))
  p %>%
    unite(path, lhs, op, rhs, sep = "") %>%
    filter(path %in% terms$path) %>%
    left_join(terms) %>%
    select(term = new, est, ci.lower, ci.upper, pvalue) %>%
    mutate(term = factor(term, levels = terms$new)) %>%
    arrange(term)
}
```

## Lesson 11 {.smaller}
### Long lists of anything are asking for trouble 
- It took me way too long to even make the short examples above using `tribble()`
- Using a spreadsheet is an easier way to compile that information 
- The `googlesheets4` package is also a package dedicated to helping you to read, write, and parse Google Sheets
- It's easy to load files stored on GitHub
- Spreadsheets are user friendly even for those who aren't code literate
- It's way easier to reorder a spreadsheet (cut-insert cut rows) than to have to move rows around in an R script
- f%#*ing commas and quotes

## Lesson 12 {.smaller}
### File structure and organization are your most important data cleaning & management tools
- Your data will never be clean if you don't know where your files are! 
- No one wants to have to rerun things repeatedly 
  * Store large files (models, bootstrapped resamples, bayesian samples, etc.) using a clear, machine readable, parseable file structure (e.g., `dementia-E-age-unadj.RDS`)
  * These can then be read in like: 

:::{.fragment}

```{r, eval = F}
nested_res <- tibble(
  file = list.files("models"),
  mod = map(file, \(x) readRDS(sprintf("models/%s", x)))
  ) %>%
  separate(file, c("outcome", "trait", "moderator", "adj"), sep = "-")
```

:::

---

### File structure and organization are your most important data cleaning & management tools

- Same thing goes for smaller objects
- Save those small ones, like summaries (e.g., from `broom::tidy()`, `coef()`, etc.), predicted values, random effects, etc. using the same file structure, and you always have everything at your fingertips
- Plus you can merge them more easily! 
- This organization also transfers to GitHub for easy loading via raw links!

## Lesson 13 
### Some things / functions are portable across projects, some need modification 

- Some functions are portable: 

:::{.fragment}

```{r}
z_scale <- function(x) (x - mean(x, na.rm = T))/sd(x, na.rm = T)
pomp_score <- function(x){
  rng <- range(x, na.rm = T)
  (x - rng[1])/(rng[2] - rng[1])*100
}
```

:::

---

### Some things / functions are portable across projects, some need modification 

- Some are not: 
- This function works for `lavaan`. With slight modifications, it could also work for `broom::tidy()` output

:::{.fragment}

```{r, eval = F}
format_fun <- function(d){
  d %>%
    mutate(sig = ifelse(pvalue < .05, "sig", "ns")) %>%
    rowwise() %>%
    mutate_at(vars(est, ci.lower, ci.upper), round_fun) %>%
    mutate_at(vars(pvalue), pround_fun) %>%
    ungroup() %>%
    mutate(CI = sprintf("[%s,%s]", ci.lower, ci.upper)) %>%
    mutate_at(vars(est, CI, pvalue), ~ifelse(sig == "sig" & !is.na(sig), sprintf("<strong>%s</strong>", .), .)) 
}
```

:::

---

### Some things / functions are portable across projects, some need modification 
- One possibility is to create an `.R` script that you can "source" (`source("custom_functions.R")`) 
  * You could have general functions (e.g., `z_scale()`, `pomp_score()`) and use case specific ones (e.g., `lavaan_format_fun()` or `broom_format_fun()`) 
  * I often like to copy these into my R workflow because it means that everything is included in the scripts (even though the `.R` script can be included in the repo) 

## Lesson 14
### Resources are finite: Be aware of how you use them 

::::{.columns}
:::{.column}

- Using grid view in your Environment tab is a great way to track resources 
- The environment below came after running 95 separate models, all of which were held in memory 

:::
:::{.column}

![](images/full-environment.png)

:::
::::

---

### Resources are finite: Be aware of how you use them 

::::{.columns}
:::{.column}

:::{.nonincremental}

- Using grid view in your Environment tab is a great way to track resources 

:::

- The environment below came from reloading smaller summary objects rather than keeping all the models in working memory

:::
:::{.column}

![](images/tidy-environment.png)

:::
::::

---

### Resources are finite: Be aware of how you use them 

- Activity Monitor (Mac) or Process Monitor (Windows) is another great way to track general system usage across many programs, not just `R`
- I use this in particular when I'm doing parallelization 
  * Sometimes threads stall (drop to 0% CPU or Memory)
  * Sometimes threads use way too much memory (and you start using swap)
  * It's great to track this so you can interrupt 
  

## Lesson 15
### Data frames are your friend

- They are the easiest objects to work with in R because of the number of dedicated tools and functions for working with them 
- But they can get unwieldy (e.g., printing a data frame with hundreds of columns and thousands of rows)
- `tibbles` help with this but don't always play nice 
  * You can't go from some classes to tibble directly
  * Instead go data.frame -> tibble

```{r, eval = F}
r <- cor(df, use = "pairwise")
r[upper.tri(r, diag = T)] <- NA 
r %>%
  data.frame() %>%
  as_tibble()
```

## Lesson 16
### RStudio / Pivot Cheat Sheets 

[![Posit Cheatsheets](images/cheatsheets.png)](https://posit.co/resources/cheatsheets/)

---

### RStudio / Pivot Cheat Sheets 

- [Quarto](https://rstudio.github.io/cheatsheets/quarto.pdf) 
- [RStudio](https://rstudio.github.io/cheatsheets/rstudio-ide.pdf) 
- [RMarkdown](https://rstudio.github.io/cheatsheets/rmarkdown.pdf) 
- [lubridate](https://rstudio.github.io/cheatsheets/lubridate.pdf) 
- [stringr](https://rstudio.github.io/cheatsheets/strings.pdf) 
- [purrr](https://rstudio.github.io/cheatsheets/purrr.pdf) 
- [readr](https://rstudio.github.io/cheatsheets/data-import.pdf) 
- [tidyr](https://rstudio.github.io/cheatsheets/tidyr.pdf) 
- [dplyr](https://rstudio.github.io/cheatsheets/data-transformation.pdf) 
- [ggplot2](https://rstudio.github.io/cheatsheets/data-visualization.pdf) 
  
## Hacks

-   [The option / alt key and other shortcuts](https://support.posit.co/hc/en-us/articles/200711853-Keyboard-Shortcuts-in-the-RStudio-IDE)
-   The tab key: "attempt completion"
-   [R templates](https://quarto.org/docs/extensions/starter-templates.html)
-   [GitHub Pages](https://pages.github.com)
-   [Functions without inputs](https://bookdown.org/rdpeng/rprogdatascience/functions.html)




