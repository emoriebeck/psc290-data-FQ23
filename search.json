[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Cleaning and Management (Fall 2023)",
    "section": "",
    "text": "Mondays, 2:10-5 PM (October 2-December 4, 2023)\n166 Young Hall\nPsychology Department\nUniversity of California, Davis"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Data Cleaning and Management (Fall 2023)",
    "section": "Course Description",
    "text": "Course Description\n\n\n\nIn graduate education, training on research (and statistical) methods and conceptual frameworks far outpaces training on key technical skills that underpin all research, empirical or otherwise. On average, researchers spend about 80% of their (analytic) time on data cleaning, but we spend comparatively little teaching those skills. This course aims to fill that gap by helping researchers to (1) build their reproducible research workflow and (2) improve their data cleaning and general statistical programming skills. To that end, each session will be split to address each of these goals, with the beginning of class focused on conceptual ideas about best practices in building a workflow and the latter half focused on technical training on programming and cleaning data in R. This course will be set up as a “bring your own data” course to allow students to anticipate specific challenges that face different types of research. \nThis course is not a “pure” data science (i.e. we won’t be working with databases, etc.) because it focuses on the skills and tools most common within the social sciences. Science is a collaborative enterprise, and these tools are widely used among many social scientists, which promotes an open, equitable workflow by using tools available and most commonly used by the majority of our peers."
  },
  {
    "objectID": "index.html#navigating-this-site",
    "href": "index.html#navigating-this-site",
    "title": "Data Cleaning and Management (Fall 2023)",
    "section": "Navigating This Site:",
    "text": "Navigating This Site:\n1. Weekly assignments are under Problem Sets. These are due at 12:01 AM on the day of class.\n2. Reading list (and links) and links to workshop slides are under Schedule and on Canvas. I recommend bookmarking this site to allow you access to all materials in perpetuity.\n3. Final Project Information will be under Final Project. The proposal instructions (Due November 19 at 11:59 PM PST) is now posted. More details about the final project will be posted by November 7, and a rubric for the final project will be posted by November 28.\n4. The most updated version of the syllabus will be on the Syllabus page and can be downloaded there as well."
  },
  {
    "objectID": "index.html#course-zoom-link",
    "href": "index.html#course-zoom-link",
    "title": "Data Cleaning and Management (Fall 2023)",
    "section": "Course Zoom Link",
    "text": "Course Zoom Link\nThis course is in person, but you may access it on Zoom due to illness, exposure, travel, etc."
  },
  {
    "objectID": "10-week10-workbook.html",
    "href": "10-week10-workbook.html",
    "title": "Week 10 Workbook",
    "section": "",
    "text": "This is lavaan 0.6-15\nlavaan is FREE software! Please report any bugs.\n\n\n\nAttaching package: 'lavaan'\n\n\nThe following object is masked from 'package:psych':\n\n    cor2cov\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()     masks psych::%+%()\n✖ ggplot2::alpha()   masks psych::alpha()\n✖ dplyr::arrange()   masks plyr::arrange()\n✖ purrr::compact()   masks plyr::compact()\n✖ dplyr::count()     masks plyr::count()\n✖ dplyr::desc()      masks plyr::desc()\n✖ dplyr::failwith()  masks plyr::failwith()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::id()        masks plyr::id()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::mutate()    masks plyr::mutate()\n✖ dplyr::rename()    masks plyr::rename()\n✖ dplyr::summarise() masks plyr::summarise()\n✖ dplyr::summarize() masks plyr::summarize()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-1",
    "href": "10-week10-workbook.html#lesson-1",
    "title": "Week 10 Workbook",
    "section": "Lesson 1",
    "text": "Lesson 1\nAlways load tidyverse last\n\nAlways load all packages at the beginning of a script\n\n\n\nCodelibrary(psych)\nlibrary(ggdist)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(brms)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(patchwork)\nlibrary(plyr)\nlibrary(tidyverse)\nlibrary(furrr)\n\n\n\nNote: tidyverse loads: dplyr, forcats (factors), ggplot2, lubrdiate, purrr, readr, stringr, tibble, and tidyr\n\nThis is good! It reduces the number of packages you have to load and ensures there’s no order issues\n\nDeal with Conflicts\n\nUse the conflicts() function to figure out what conflicts you have\nUse package::fnName() to call a function directly without loading a package / to override conflicts\n\ne.g., kableExtra should be loaded before tidyverse, but then tidyverse masks kableExtra::group_rows()\n\n\n\n\n\nCodekable(tab) %>%\n  kable_classic(html_font = \"Times\") %>%\n  kableExtra::group_rows(\"Header\", 1, 3)"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-2",
    "href": "10-week10-workbook.html#lesson-2",
    "title": "Week 10 Workbook",
    "section": "Lesson 2",
    "text": "Lesson 2\nThere is no single way to do anything\n\nThe best way to do something is a way that you understand or you can introduce the mistakes you’re trying to prevent\n\n\nCodebfi %>%\n  mutate(\n    sid = 1:n(),\n    E = rowMeans(pick(matches(\"E\\\\d\")), na.rm = T), \n    A = rowMeans(pick(matches(\"A\\\\d\")), na.rm = T), \n    C = rowMeans(pick(matches(\"C\\\\d\")), na.rm = T), \n    N = rowMeans(pick(matches(\"N\\\\d\")), na.rm = T), \n    O = rowMeans(pick(matches(\"O\\\\d\")), na.rm = T)\n  ) %>%\n  ungroup() %>%\n  select(sid, E:O)\n\n\n\n  \n\n\n\n\nCodebfi %>% \n  mutate(sid = 1:n()) %>%\n  pivot_longer(\n    cols = c(-sid, -gender, -education, -age)\n    , names_to = c(\"trait\", \"item\")\n    , names_sep = -1\n    , values_to = \"value\"\n  ) %>%\n  group_by(sid, trait) %>%\n  summarize(value = mean(value, na.rm = T)) %>%\n  pivot_wider(names_from = \"trait\", values_from = \"value\") %>%\n  ungroup()"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-3",
    "href": "10-week10-workbook.html#lesson-3",
    "title": "Week 10 Workbook",
    "section": "Lesson 3",
    "text": "Lesson 3\nStart at the end\n\nWhat do you want your data to look like?\nWhat do they look like now?\nNow fill in the middle\n\nHLM / MLM / MEM: RE ex: time, trial, stimuli, group, study, day, w/in person conditions FE ex: gender, baseline age, b/w subject conditions, country, etc. (can also be an RE)\nID | RE1 | RE2 | DV | FE1\n1 | 1 | 1 | 4 | 3\n1 | 1 | 2 | 3 | 3\n1 | 2 | 1 | 2 | 3\n1 | 2 | 2 | 1 | 3\n2 | 1 | 1 | 5 | 1\n2 | 1 | 2 | 3 | 1\n2 | 2 | 1 | 1 | 1\n2 | 2 | 2 | 2 | 1\nStart at the end\nID | RE_1_1 | RE_1_2 | RE+2_1 | RE_2_2 | FE2 1 | 4 | 3 | 2 | 1 | 3\n2 | 5 | 3 | 1 | 2 | 1\n\n\npivot_longer(): RE_1_1:RE_2_2\n\nnames_to = c(“RE1”, “RE2”)\nvalues_to = “DV”\nnames_sep = “_”\nnames_prefix = “RE_”\n\n\nNote this is only possible / easy because of the naming scheme! If we had them named “RE1_1”, this would not have been possible / would have been CONSIDERABLY more difficult"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-4",
    "href": "10-week10-workbook.html#lesson-4",
    "title": "Week 10 Workbook",
    "section": "Lesson 4",
    "text": "Lesson 4\nDon’t be afraid to split your data into chunks\n\nIn alignment with starting at the end, a key strategy is knowing how you can chunk your data\nNo right or wrong way to chunk, but some examples are:\n\nitems / values from the same scale / task (e.g., DV across trials / conditions)\nbaseline items (from other survey or from baseline wave)\noutcome variables\ndescriptive variables\nitem-level variables v. composites"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-5",
    "href": "10-week10-workbook.html#lesson-5",
    "title": "Week 10 Workbook",
    "section": "Lesson 5",
    "text": "Lesson 5\nJoining data requires a key, so be thoughtful and you’ll always be able to put the pieces together\n\nThe most important thing when splitting data into chunks is to make sure you can put it back together\nThis requires one (e.g., participant ID) or more (e.g., participant ID, wave) keys that allows R to match the right values together\nThis should be the last thing you do\n\nPlease don’t create mega datasets where you tack things on to the raw data as you go\nThis will eat RAM and make your life harder (and sometimes could end up in you accidentally sharing identifying information!!)"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-6",
    "href": "10-week10-workbook.html#lesson-6",
    "title": "Week 10 Workbook",
    "section": "Lesson 6",
    "text": "Lesson 6\nOne of the most important skills is getting comfortable making data move flexibly from wide to long\n\nRemember our example above? Without knowledge of how to do so, it would have been almost impossible\nIt’s okay if it takes more than one step! That’s better than manually moving stuff in excel and not creating a reproducible path!\n\n\nCodebfi %>% \n  mutate(sid = 1:n()) %>%\n  pivot_longer(\n    cols = c(-sid, -gender, -education, -age)\n    , names_to = c(\"trait\", \"item\")\n    , names_sep = -1\n    , values_to = \"value\"\n  ) %>%\n  group_by(sid, trait) %>%\n  summarize(value = mean(value, na.rm = T)) %>%\n  pivot_wider(names_from = \"trait\", values_from = \"value\") %>%\n  ungroup()"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-7",
    "href": "10-week10-workbook.html#lesson-7",
    "title": "Week 10 Workbook",
    "section": "Lesson 7",
    "text": "Lesson 7\nEstablish a consistent naming scheme\n\nlabel objects relative to a stage or research question\n\ne.g., nested_RQ1, RQ1_mods, raw_df\n\nthis will help you clear your environment of clutter\n\n\nuse temporary objects repeatedly\n\ne.g., if you need to use an object as an intermediary step, call it tmp and overwrite it as many times as is useful\nYou can always remove it using rm(tmp)"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-8",
    "href": "10-week10-workbook.html#lesson-8",
    "title": "Week 10 Workbook",
    "section": "Lesson 8",
    "text": "Lesson 8\nYou won’t remember details about raw variables or variables you create, document them\n\nClearly document your raw data and planned transformations before (preregistration) or as (deviations or just reactive responses to aspects of the data) you clean your data\nClearly document all new variables you create, including their scale, etc."
  },
  {
    "objectID": "10-week10-workbook.html#lesson-9",
    "href": "10-week10-workbook.html#lesson-9",
    "title": "Week 10 Workbook",
    "section": "Lesson 9",
    "text": "Lesson 9\nReference data frames are great keys for ordering and renaming\n\nClearly documenting all new variables you create also creates the opportunity to create reference data frames, which can include variable names in the data, category information, longer names for the variables, descriptions of the scales, and the link function / column names e.g.,\n\ncat | name | scale | long_name | lab\nOutcome | dementia | 0/1 | Clinical Dementia | OR [CI] Outcome | braak | 1-5 | Braak Stage | est. [CI] Predictor | E | 0-10 | Extraversion |\nPredictor | C | 0-10 | Conscientiousness |\nModerator | age | num | Baseline Age | Moderator | ses | 1-7 | Baseline SES |\nReference data frames are great keys for ordering and renaming\n\nCodeout <- tribble(\n  ~cat,     ~name,        ~scale,      ~long_name,              ~lab,   \n\"Outcome\",  \"dementia\",   \"0/1\",       \"Clinical Dementia\",      \"OR [CI]\",\n\"Outcome\",  \"braak\",      \"1-5\",       \"Braak Stage\",            \"est. [CI]\" \n)\n\ntab %>%\n  left_join(out %>% select(outcome = name, long_out = long_name, lab))"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-10",
    "href": "10-week10-workbook.html#lesson-10",
    "title": "Week 10 Workbook",
    "section": "Lesson 10",
    "text": "Lesson 10\nReorder everything using factors\n\nOften, there are specific orders we want / need strings to be in; this is where factors come in\nThere’s a whole package for this called forcats.\nReference data frames are a great way to order your variables\n\n\nCodeout <- tribble(\n  ~cat,     ~name,        ~scale,      ~long_name,              ~lab,   \n\"Outcome\",  \"dementia\",   \"0/1\",       \"Clinical Dementia\",      \"OR [CI]\",\n\"Outcome\",  \"braak\",      \"1-5\",       \"Braak Stage\",            \"est. [CI]\" \n)\n\ntab %>%\n  left_join(out %>% select(outcome = name, long_out = long_name, lab)) %>%\n  mutate(long_out = factor(long_out, levels = out$long_name))\n# mutate(long_out = factor(oucome, levels = out$name, labels = out$long_name))\n\n\nReorder everything using factors\n\nRemember this?\n\n\nCodeterms <- tribble(\n  ~path,     ~new,                         ~level\n  \"i~1\",     \"Intercept\",                  \"Fixed\",\n  \"s~1\",     \"Slope\",                      \"Fixed\",\n  \"i~~i\",    \"Intercept Variance\",         \"Random\",\n  \"s~~s\",    \"Slope Variance\",             \"Random\",\n  \"i~~s\",    \"Intercept-Slope Covariance\", \"Random\"\n)\n\nextract_fun <- function(m, trait){\n  p <- parameterEstimates(m) %>%\n    data.frame()\n  # saveRDS(p, file = sprintf(\"results/summary/%s.RDS\", trait))\n  p %>%\n    unite(path, lhs, op, rhs, sep = \"\") %>%\n    filter(path %in% terms$path) %>%\n    left_join(terms) %>%\n    select(term = new, est, ci.lower, ci.upper, pvalue) %>%\n    mutate(term = factor(term, levels = terms$new)) %>%\n    arrange(term)\n}"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-11",
    "href": "10-week10-workbook.html#lesson-11",
    "title": "Week 10 Workbook",
    "section": "Lesson 11",
    "text": "Lesson 11\nLong lists of anything are asking for trouble\n\nIt took me way too long to even make the short examples above using tribble()\n\nUsing a spreadsheet is an easier way to compile that information\nThe googlesheets4 package is also a package dedicated to helping you to read, write, and parse Google Sheets\nIt’s easy to load files stored on GitHub\nSpreadsheets are user friendly even for those who aren’t code literate\nIt’s way easier to reorder a spreadsheet (cut-insert cut rows) than to have to move rows around in an R script\nf%#*ing commas and quotes"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-12",
    "href": "10-week10-workbook.html#lesson-12",
    "title": "Week 10 Workbook",
    "section": "Lesson 12",
    "text": "Lesson 12\nFile structure and organization are your most important data cleaning & management tools\n\nYour data will never be clean if you don’t know where your files are!\nNo one wants to have to rerun things repeatedly\n\nStore large files (models, bootstrapped resamples, bayesian samples, etc.) using a clear, machine readable, parseable file structure (e.g., dementia-E-age-unadj.RDS)\nThese can then be read in like:\n\n\n\n\nCodenested_res <- tibble(\n  file = list.files(\"models\"),\n  mod = map(file, \\(x) readRDS(sprintf(\"models/%s\", x)))\n  ) %>%\n  separate(file, c(\"outcome\", \"trait\", \"moderator\", \"adj\"), sep = \"-\")\n\n\nFile structure and organization are your most important data cleaning & management tools\n\nSame thing goes for smaller objects\nSave those small ones, like summaries (e.g., from broom::tidy(), coef(), etc.), predicted values, random effects, etc. using the same file structure, and you always have everything at your fingertips\nPlus you can merge them more easily!\nThis organization also transfers to GitHub for easy loading via raw links!"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-13",
    "href": "10-week10-workbook.html#lesson-13",
    "title": "Week 10 Workbook",
    "section": "Lesson 13",
    "text": "Lesson 13\nSome things / functions are portable across projects, some need modification\n\nSome functions are portable:\n\n\nCodez_scale <- function(x) (x - mean(x, na.rm = T))/sd(x, na.rm = T)\npomp_score <- function(x){\n  rng <- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])*100\n}\n\n\nSome things / functions are portable across projects, some need modification\n\nSome are not:\nThis function works for lavaan. With slight modifications, it could also work for broom::tidy() output\n\n\nCodeformat_fun <- function(d){\n  d %>%\n    mutate(sig = ifelse(pvalue < .05, \"sig\", \"ns\")) %>%\n    rowwise() %>%\n    mutate_at(vars(est, ci.lower, ci.upper), round_fun) %>%\n    mutate_at(vars(pvalue), pround_fun) %>%\n    ungroup() %>%\n    mutate(CI = sprintf(\"[%s,%s]\", ci.lower, ci.upper)) %>%\n    mutate_at(vars(est, CI, pvalue), ~ifelse(sig == \"sig\" & !is.na(sig), sprintf(\"<strong>%s</strong>\", .), .)) \n}\n\n\nSome things / functions are portable across projects, some need modification\n\nOne possibility is to create an .R script that you can “source” (source(\"custom_functions.R\"))\n\nYou could have general functions (e.g., z_scale(), pomp_score()) and use case specific ones (e.g., lavaan_format_fun() or broom_format_fun())\nI often like to copy these into my R workflow because it means that everything is included in the scripts (even though the .R script can be included in the repo)"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-14",
    "href": "10-week10-workbook.html#lesson-14",
    "title": "Week 10 Workbook",
    "section": "Lesson 14",
    "text": "Lesson 14\nResources are finite, so be aware of how you’re using them\n\nUsing grid view in your Environment tab is a great way to track resources\nThe environment below came after running 95 separate models, all of which were held in memory\n\n\nResources are finite, so be aware of how you’re using them\n\n\nUsing grid view in your Environment tab is a great way to track resources\n\n\n\nThe environment below came from reloading smaller summary objects rather than keeping all the models in working memory\n\n\nResources are finite, so be aware of how you’re using them\n\nActivity Monitor (Mac) or Process Monitor (Windows) is another great way to track general system usage across many programs, not just R\n\nI use this in particular when I’m doing parallelization\n\nSometimes threads stall (drop to 0% CPU or Memory)\nSometimes threads use way too much memory (and you start using swap)\nIt’s great to track this so you can interrupt"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-15",
    "href": "10-week10-workbook.html#lesson-15",
    "title": "Week 10 Workbook",
    "section": "Lesson 15",
    "text": "Lesson 15\nData frames are your friend\n\nThey are the easiest objects to work with in R because of the number of dedicated tools and functions for working with them\nBut they can get unwieldy (e.g., printing a data frame with hundreds of columns and thousands of rows)\n\ntibbles help with this but don’t always play nice\n\nYou can’t go from some classes to tibble directly\nInstead go data.frame -> tibble\n\n\n\n\nCoder <- cor(df, use = \"pairwise\")\nr[upper.tri(r, diag = T)] <- NA \nr %>%\n  data.frame() %>%\n  as_tibble()"
  },
  {
    "objectID": "10-week10-workbook.html#lesson-16",
    "href": "10-week10-workbook.html#lesson-16",
    "title": "Week 10 Workbook",
    "section": "Lesson 16",
    "text": "Lesson 16\nRStudio / Pivot Cheat Sheets\n\n\nPosit Cheatsheets\n\n\nRStudio / Pivot Cheat Sheets\n\nQuarto\nRStudio\nRMarkdown\nlubridate\nstringr\npurrr\nreadr\ntidyr\ndplyr\nggplot2"
  },
  {
    "objectID": "10-week10-workbook.html#hacks",
    "href": "10-week10-workbook.html#hacks",
    "title": "Week 10 Workbook",
    "section": "Hacks",
    "text": "Hacks\n\nThe option / alt key and other shortcuts\nThe tab key: “attempt completion”\nR templates\nGitHub Pages\nFunctions without inputs"
  },
  {
    "objectID": "03-week3-slides.html",
    "href": "03-week3-slides.html",
    "title": "Week 3 Slides",
    "section": "",
    "text": "Welcome to Week 3! This week, we’ll talk about data quality, descriptive statistics / exploratory data analysis, and tidyr."
  },
  {
    "objectID": "05-week5-workbook.html#why-write-your-own-functions",
    "href": "05-week5-workbook.html#why-write-your-own-functions",
    "title": "Week 5 Workbook",
    "section": "Why Write Your Own Functions?",
    "text": "Why Write Your Own Functions?\n\nAutomate your workflow\nPrevent copy-paste errors (only need to update the function, not update the same task in a bunch of areas)\nSaves you time by providing tools that port to new projects\nFunctions make you think, which improves the quality of our work"
  },
  {
    "objectID": "05-week5-workbook.html#when-should-you-write-your-own-functions",
    "href": "05-week5-workbook.html#when-should-you-write-your-own-functions",
    "title": "Week 5 Workbook",
    "section": "When Should You Write Your Own Functions?",
    "text": "When Should You Write Your Own Functions?\n\nHadley Wickham’s rule of thumb (which is common in a lot of CS and DS circles) is that if you have to copy-paste something more than twice, you should write a function\nBut really you can write one whenever you’d like\nI often write functions when a link of code starts getting because it’s usually a good signal that I’m trying to do too much at once."
  },
  {
    "objectID": "05-week5-workbook.html#types-of-functions",
    "href": "05-week5-workbook.html#types-of-functions",
    "title": "Week 5 Workbook",
    "section": "Types of Functions",
    "text": "Types of Functions\n\nVector functions take one or more vectors as input and return a vector as output.\nData frame functions take a data frame as input and return a data frame as output.\nPlot functions that take a data frame as input and return a plot as output.\nand so on…\nWe’ll talk extensively about this again when we talk about building figures and tables into your workflow in Week 8"
  },
  {
    "objectID": "05-week5-workbook.html#vector-functions",
    "href": "05-week5-workbook.html#vector-functions",
    "title": "Week 5 Workbook",
    "section": "Vector Functions",
    "text": "Vector Functions\n\nIn my work, I often want to POMP (Percentage Of Maximum Possible) score or z-score (standardize) variables, which allows me to get data on a standard and interpretable scale (percentage or standard deviations)\nI often want to do this with lots of variables, so to do each one separately would take forever and introduce chances for error.\n\n\nCodebfi %>%\n  select(matches(\"1$\")) %>%\n  mutate(\n    E1 = (E1 - min(E1, na.rm = T))/(max(E1, na.rm = T) - min(E1, na.rm = T))*100\n    , A1 = (A1 - min(A1, na.rm = T))/(max(A1, na.rm = T) - min(A1, na.rm = T))*100\n    , C1 = (C1 - min(C1, na.rm = T))/(max(C1, na.rm = T) - min(C1, na.rm = T))*100\n    , N1 = (N1 - min(N1, na.rm = T))/(max(N1, na.rm = T) - min(N1, na.rm = T))*100\n    , O1 = (O1 - min(O1, na.rm = T))/(max(O1, na.rm = T) - min(O1, na.rm = T))*100\n  ) %>%\n  head(10)\n\n\n\n  \n\n\n\n\nInstead we could make this a function because I don’t want to talk about how long it took me to do that copy-pasting 😭\n\n\nCodepomp <- function(x, na) (x - min(x, na.rm = na))/(max(x, na.rm = na) - min(x, na.rm = na))*100\nbfi %>%\n  select(matches(\"1$\")) %>%\n  mutate(\n    E1 = pomp(E1, T)\n    , A1 =  pomp(A1, T)\n    , C1 =  pomp(C1, T)\n    , N1 =  pomp(N1, T)\n    , O1 =  pomp(O1, T)\n  ) %>%\n  head(10)\n\n\n\n  \n\n\n\n\nAnd remember, we could simplify this even more with mutate_at()!\nAnd I know in this example it saved us about four lines of code, but having a function like this that you can just reference, especially in conjunction with mutate_at() saves you time in the short and long run.1\n\n\n\nCodepomp <- function(x, na) (x - min(x, na.rm = na))/(max(x, na.rm = na) - min(x, na.rm = na))*100\nbfi %>%\n  select(matches(\"1$\")) %>%\n  mutate_at(vars(matches(\"1$\")), pomp, na = T) %>%\n  head(10)"
  },
  {
    "objectID": "05-week5-workbook.html#writing-functions",
    "href": "05-week5-workbook.html#writing-functions",
    "title": "Week 5 Workbook",
    "section": "Writing Functions",
    "text": "Writing Functions\n\nThe first step in writing a function is to find the pattern:\nLet’s look back at the code above:\n\n\nCodeE1 = (E1 - min(E1, na.rm = T))/(max(E1, na.rm = T) - min(E1, na.rm = T))*100\nA1 = (A1 - min(A1, na.rm = T))/(max(A1, na.rm = T) - min(A1, na.rm = T))*100\nC1 = (C1 - min(C1, na.rm = T))/(max(C1, na.rm = T) - min(C1, na.rm = T))*100\nN1 = (N1 - min(N1, na.rm = T))/(max(N1, na.rm = T) - min(N1, na.rm = T))*100\nO1 = (O1 - min(O1, na.rm = T))/(max(O1, na.rm = T) - min(O1, na.rm = T))*100\n\n\n\nDo you see the pattern?\n\n\nCode(█ - min(█, na.rm = TRUE)) / (max(█, na.rm = TRUE) - min(█, na.rm = TRUE))\n\n\nWe need three things to turn this into a function:\n\n\nA name. Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1.\n\nThe arguments. The arguments are things that vary across calls and our analysis above tells us that we have just one. We’ll call it x because this is the conventional name for a numeric vector.\n\nThe body. The body is the code that’s repeated across all the calls.\n\n\nThe template of a function looks something like this:\n\n\nCodename <- function(arguments){\n  body\n}\n\n\n\nAlthough for local functions, I suggest something like this:\n\n\nCodename <- function(\n    argument1 # description of first argument\n    , argument2 # description of second argument\n    ){\n  body\n}\n\n\n\nThat’s not going to fit well on my slides, so I will minimally do it in class.\nFollowing this, we return to our function:\n\n\nCodepomp <- function(x, na) {\n  (x - min(x, na.rm = na))/(max(x, na.rm = na) - min(x, na.rm = na))*100\n}"
  },
  {
    "objectID": "05-week5-workbook.html#function-efficiency",
    "href": "05-week5-workbook.html#function-efficiency",
    "title": "Week 5 Workbook",
    "section": "Function Efficiency",
    "text": "Function Efficiency\n\nFor most of our purposes, we may not care that much about how fast our code is\nUnless you’re working with huge data, things typically run fast\nBut with a function like POMP, I may need to run it on 100+ variables each with 10,000+ observations, so speed may be a consideration\nSo how do you speed up your functions?\n\nAvoid computing something twice\nTry to anticipate where things could go wrong\n\n\nSo in our function, we calculate min() twice and min() once\nWe could just calculate range() instead, which reduces that computation time\nWatch (note you have to run this in your console to see the difference)!\n\n\n\n\nCodepomp <- function(x) {\n  (x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T))*100\n}\n\nstart <- Sys.time()\ntmp <- bfi %>%\n  select(matches(\"1$\")) %>%\n  mutate_at(vars(matches(\"1$\")), pomp)\n(diff1 <- Sys.time() - start)\n\nTime difference of 0.01409316 secs\n\n\n\n\n\nCodepomp <- function(x) {\n  rng <- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])*100\n}\n\nstart <- Sys.time()\ntmp <- bfi %>%\n  select(matches(\"1$\")) %>%\n  mutate_at(vars(matches(\"1$\")), pomp)\n(diff2 <- Sys.time() - start)\n\nTime difference of 0.01456594 secs\n\n\n\n::::\n\nBut those differences are really minimal right? Just -5^{-4} seconds.\nBut across 150 variables that’s -0.0709 seconds."
  },
  {
    "objectID": "05-week5-workbook.html#cautionary-note-on-outputs",
    "href": "05-week5-workbook.html#cautionary-note-on-outputs",
    "title": "Week 5 Workbook",
    "section": "Cautionary note on outputs",
    "text": "Cautionary note on outputs\n\nThe function we wrote above could be a “mutate()” function because the output is the same length / size as the input.\nIf you write a function that drops cases, you will get an error because R can’t make the resulting vector fit back into the data frame\nOr, if you write a function that returns length 1 within mutate, it will just repeat that value for every row\nSo, just be careful to think about what the output is and make sure that it matches the use case you are working with!"
  },
  {
    "objectID": "05-week5-workbook.html#more-notes-on-outputs",
    "href": "05-week5-workbook.html#more-notes-on-outputs",
    "title": "Week 5 Workbook",
    "section": "More notes on outputs",
    "text": "More notes on outputs\n\nYou can output anything you want from a function!\n\ndata frames\nvectors\nsingle values\nstrings\nlists (very common for base R and package functions!!)\nmodel objects\nplots\ntables\netc.!"
  },
  {
    "objectID": "05-week5-workbook.html#you-try",
    "href": "05-week5-workbook.html#you-try",
    "title": "Week 5 Workbook",
    "section": "You Try:",
    "text": "You Try:\nTry turning the following into functions:\n\nCodemean(is.na(x))\nmean(is.na(y))\nmean(is.na(z))\n\nx / sum(x, na.rm = TRUE)\ny / sum(y, na.rm = TRUE)\nz / sum(z, na.rm = TRUE)\n\nround(x / sum(x, na.rm = TRUE) * 100, 1)\nround(y / sum(y, na.rm = TRUE) * 100, 1)\nround(z / sum(z, na.rm = TRUE) * 100, 1)\n\n(x - mean(x, na.rm = T))/sd(x, na.rm = T)\n(y - mean(y, na.rm = T))/sd(y, na.rm = T)\n(z - mean(z, na.rm = T))/sd(z, na.rm = T)\n\n\n\nCodeprop_missing <- function(x) mean(is.na(x))\n\nprop_total   <- function(x) x / sum(x, na.rm = T)\n\nround_prop   <- function(x) round(prop_total(x)*100, 1)\n\nz_score      <- function(x) (x - mean(x, na.rm = T))/sd(x, na.rm = T)"
  },
  {
    "objectID": "05-week5-workbook.html#data-frame-functions",
    "href": "05-week5-workbook.html#data-frame-functions",
    "title": "Week 5 Workbook",
    "section": "Data Frame Functions",
    "text": "Data Frame Functions\n\nAll the functions we’ve covered so far are vector functions (i.e. they input a vector, not a matrix or data frame and work well within mutate() calls)\nBut if you have a long string of dplyr verbs, it can be useful to put these into a data frame function where you provide a data frame input and flexible way of naming the columns\nI’m going to give a brief example here, but suggest that you check out more in R for Data Science.\nLet’s start with a brief example. To do it, let’s first make the bfi data frame long\n\n\nCodebfi_long <- bfi %>%\n  rownames_to_column(\"SID\") %>%\n  pivot_longer(\n    cols = c(-SID, -education, -gender, -age)\n    , names_to = c(\"trait\", \"item\")\n    , names_sep = -1\n    , values_to = \"value\"\n    , values_drop_na = T\n  )\nhead(bfi_long, 8)\n\n\n\n  \n\n\n\n\nSo maybe I want to get means for each of the Big Five from this, so I write a function like this:\n\n\nCodegrouped_mean <- function(df, group_var, mean_var) {\n  df %>%\n    group_by(group_var) %>%\n    summarize(mean(mean_var))\n}\n\nbfi_long %>% grouped_mean(trait, value)\n\n\n\nThat didn’t work because we can’t specify grouping variables like that\nTo get around it, we have to use something called embracing: We have to wrap the variables like this {{ trait }}\nThis just nudges the dplyr functions to look inside the data frame for the column you specify\n\n\nCodetidy_describe <- function(df, var) {\n  df %>%\n    summarize(\n      mean   = mean({{ var }},   na.rm = TRUE),\n      sd     = sd({{ var }},     na.rm = TRUE),\n      median = median({{ var }}, na.rm = TRUE),\n      min    = min({{ var }},    na.rm = TRUE),\n      max    = max({{ var }},    na.rm = TRUE),\n      n      = n(),\n      n_miss = sum(is.na({{ var }})),\n      .groups = \"drop\"\n      )\n}\n\nbfi_long %>% \n  group_by(trait) %>%\n  tidy_describe(value)\n\n\n\n  \n\n\n\n\nOr remember when I had us getting counts and proportions for continuous x categorical relationships?\n\n\nCodecount_prop <- function(df, var, sort = FALSE) {\n  df %>%\n    count({{ var }}, sort = sort) %>%\n    mutate(prop = n / sum(n))\n}\n\nbfi_long %>% \n  count_prop(education)"
  },
  {
    "objectID": "05-week5-workbook.html#wrap-up",
    "href": "05-week5-workbook.html#wrap-up",
    "title": "Week 5 Workbook",
    "section": "Wrap-Up",
    "text": "Wrap-Up\n\nThis is just a brief introduction to functions.\nFunctions are absolutely essential part of workflows, and you’ll see them pop up in every lesson from here on out (and you already saw them pop up in previous lessons)\nAs we continue to see them, I’ll ramp up their complexity, showing you how to write functions for estimating models, model predictions, figures, tables, and more"
  },
  {
    "objectID": "05-week5-workbook.html#iteration",
    "href": "05-week5-workbook.html#iteration",
    "title": "Week 5 Workbook",
    "section": "Iteration",
    "text": "Iteration\nIteration is everywhere. It underpins much of mathematics and statistics. If you’ve ever seen the \\(\\Sigma\\) symbol, then you’ve seen (and probably used) iteration.\nReasons for iteration:\n- reading in multiple files from a directory\n- running the same operation multiple times\n- running different combinations of the same model\n- creating similar figures / tables / outputs"
  },
  {
    "objectID": "05-week5-workbook.html#for-loops",
    "href": "05-week5-workbook.html#for-loops",
    "title": "Week 5 Workbook",
    "section": "\nfor loops",
    "text": "for loops\nEnter for loops. for loops are the “OG” form of iteration in computer science. The basic syntax is below. Basically, we can use a for loop to loop through and print a series of things.\n\nCodefor(i in letters[1:5]){\n  print(i)\n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n\nThe code above “loops” through 5 times, printing the iteration letter."
  },
  {
    "objectID": "05-week5-workbook.html#apply-family",
    "href": "05-week5-workbook.html#apply-family",
    "title": "Week 5 Workbook",
    "section": "\n_apply() family",
    "text": "_apply() family\n\nA somewhat faster version of for loops comes from the _apply() family of functions, including:\n\n\napply(), lapply(), sapply(), and mapply(). Unlike for loops, these are vectorized, which makes them more efficient.\n\n\n\n\nCodelapply(letters[1:5], print)\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] \"b\"\n\n[[3]]\n[1] \"c\"\n\n[[4]]\n[1] \"d\"\n\n[[5]]\n[1] \"e\"\n\nCodesapply(letters[1:5], print)\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n  a   b   c   d   e \n\"a\" \"b\" \"c\" \"d\" \"e\" \n\nCodemapply(print, letters[1:5])\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n  a   b   c   d   e \n\"a\" \"b\" \"c\" \"d\" \"e\""
  },
  {
    "objectID": "05-week5-workbook.html#purrr-and-_map_-functions",
    "href": "05-week5-workbook.html#purrr-and-_map_-functions",
    "title": "Week 5 Workbook",
    "section": "\npurrr and _map_() functions",
    "text": "purrr and _map_() functions\n\nToday, though, we’ll focus on the map() family of functions, which is the functions through which purrr iterates.\n\n\nCodemap(letters[1:5], print)\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] \"b\"\n\n[[3]]\n[1] \"c\"\n\n[[4]]\n[1] \"d\"\n\n[[5]]\n[1] \"e\"\n\n\n\nFor a more thorough comparison of for loops, the _apply() family, and _map_() functions, see https://jennybc.github.io/purrr-tutorial/"
  },
  {
    "objectID": "05-week5-workbook.html#purrr-and-_map_-predicates",
    "href": "05-week5-workbook.html#purrr-and-_map_-predicates",
    "title": "Week 5 Workbook",
    "section": "\npurrr and _map_() predicates",
    "text": "purrr and _map_() predicates\n\nToday, though, we’ll focus on the map() family of functions, which is the functions through which purrr iterates.\n\n\nCodemap(letters[1:5], print)\n\n\n\nNote that this returns a list, which we may not always want.\nWith purrr, we can change the kind of output of map() by adding a predicate, like lgl, dbl, chr, and df.\nSo in the example above, we may have wanted just the characters to print. To do that we’d call map_chr():\n\n\nCodemap_chr(letters[1:5], print)\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n\n\nNote that it also returns the concatenated character vector as well as printing each letter individually (i.e. iteratively)."
  },
  {
    "objectID": "05-week5-workbook.html#purrr-and-_map_-antecedents",
    "href": "05-week5-workbook.html#purrr-and-_map_-antecedents",
    "title": "Week 5 Workbook",
    "section": "\npurrr and _map_() antecedents",
    "text": "purrr and _map_() antecedents\n\nHow many mappings?\n\nSingle mapping: map_()\n\nParallel (2) mapping(s): map2_()\n\n3 or more mappings: pmap_()\n\n\n\n\n\nCodemap2_chr(letters[1:5], 1:5, paste)\n\n[1] \"a 1\" \"b 2\" \"c 3\" \"d 4\" \"e 5\"\n\n\n\nNote here that we can use map2() and pmap() with the predicates from above."
  },
  {
    "objectID": "05-week5-workbook.html#list-columns-and-the-power-of-purrr",
    "href": "05-week5-workbook.html#list-columns-and-the-power-of-purrr",
    "title": "Week 5 Workbook",
    "section": "List Columns and the Power of purrr\n",
    "text": "List Columns and the Power of purrr\n\n\nOn the previous slide, we saw a data frame inside of a data frame. This is called a list column within a nested data frame.\nIn this case, we created a list column using map, but one of the best things about purrr is how it combines with the nest() and unnest() functions from the tidyr package.\nWe’ll return to nest() later to demonstrate how anything you would iterate across is also something we can nest() by in long format data frames."
  },
  {
    "objectID": "05-week5-workbook.html#use-cases",
    "href": "05-week5-workbook.html#use-cases",
    "title": "Week 5 Workbook",
    "section": "Use Cases",
    "text": "Use Cases\n\nReading Data\n\nCleaning Data\nRunning Models\n\n(Plotting Figures - Week 8)\n\n(Creating Tables - Week 8)\n\nReading Data\nThere are a number of different cases where purrr and map() maybe useful for reading in data including:\n\nsubject-level files for an experimental task\n\nsubject- and task-level files for an experimental task\nEMA data\n\nlongitudinal data\n\nweb scraping and text mining\n\nReading Data: Participant-Level EMA\nFor this first example, I’ll show you how this would look with a for loop before I show you how it looks with purrr.\nAssuming you have all the data in a single folder and the format is reasonably similar, you have the following basic syntax:\n\nCodefiles <- list.files(data_path)\ndata <- list()\nfor(i in files){\n  data[[i]] <- read_csv(i)\n}\ndata <- combine(data)\n\n\n\nThis works fine in this simple case, but where purrr really shines in when you need to make modifications to your data before combining, whether this be recoding, removing missing cases, or renaming variables.\nBut first, the simple case of reading data. The code below will download a .zip file when you run it. Once, you do, navigate to your Desktop to unzip the folder. Open the R Project and you should be able to run the rest of the code\n\n\nCodedata_source <- \"https://github.com/emoriebeck/psc290-data-FQ23/raw/main/04-workshops/05-week5-purrr/05-week5-purrr.zip\"\ndata_dest <- \"~/Desktop/05-week5-purrr.zip\"\ndownload.file(data_source, data_dest)\n\n\n\nCodedf1 <- tibble(\n  ID = list.files(\"data/example_1\")\n  ) %>%\n  mutate(data = map(ID, ~read_csv(sprintf(\"data/example_1/%s\", .)))) %>%\n  unnest(data) \n\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 68 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 65 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 61 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 79 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 53 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 37 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 23 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 32 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 59 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 43 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 51 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 50 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 31 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 36 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 19 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 75 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 60 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 54 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 49 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 44 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nThe code above creates a list of ID’s from the data path (files named for each person), reads the data in using the map() function from purrr, removes the “.csv” from the ID variable, then unnests the data, resulting in a data frame for each person.\nNow, we’re going to combine with what we learned about last time with codebooks.\n\n\nCodecodebook <- read_csv(\"data/codebook_ex1.csv\")\n\nRows: 11 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): old_name, name, short_name, item_name, description, scale, reverse_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodecodebook\n\n\n\n  \n\n\n\n\nNow, that we have a codebook, what are the next steps?\n\n\nCodedf1 <- tibble(\n  ID = list.files(\"data/example_1\")\n  ) %>%\n  mutate(data = map(ID, ~read_csv(sprintf(\"data/example_1/%s\", .)))) %>%\n  unnest(data) \n\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 68 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 65 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 61 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 79 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 53 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 37 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 23 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 32 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 59 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 43 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 51 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 50 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 31 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 36 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 19 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 75 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 60 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 54 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 49 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 44 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodehead(df1, 6)\n\n\n\n  \n\n\n\n\npull old names in raw data from codebook\n\npull new names from codebook\n\nselect columns from codebook in loaded data\n\nrename columns with new names\n\n\nCodeold.names <- codebook$old_name # pull old names in raw data from codebook  \nnew.names <- codebook$item_name # pull new names from codebook  \ndf1 <- tibble(\n  ID = list.files(\"data/example_1\")\n  ) %>%\n  mutate(data = map(ID, ~read_csv(sprintf(\"data/example_1/%s\", .)))\n         , ID = str_remove_all(ID, \".csv\")) %>%\n  unnest(data) %>%\n  select(ID, count, all_of(old.names)) %>% # select columns from codebook in loaded data  \n  setNames(c(\"ID\", \"count\", new.names)) # rename columns with new names  \n\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 68 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 65 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 61 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 79 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 53 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 37 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 23 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 39 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 32 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 59 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 43 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 51 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 45 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 50 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 31 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 40 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 46 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 36 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 19 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 42 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 75 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 60 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 54 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 48 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 64 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 49 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): E_Assert, N_Depr, N_EmoVol, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, E_En...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 55 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, A_Rspct, A_Cmpn, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 44 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 25 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, N_Depr, N_EmoVol, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 47 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, O_IntCur, C_Org, A_Rspct, C_Rspnbl, A_Cmpn, O...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): O_AesSens, E_Assert, N_Depr, N_EmoVol, O_IntCur, C_Org, C_Rspnbl, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodehead(df1, 6)\n\n\n\n  \n\n\n\nDescriptives: Within-Person Correlations\nWith these kinds of data, the first thing, we may want to do is look at within-person correlations, which we can do with purrr.\n\nCodecor_fun <- function(x) cor(x %>% select(-count), use = \"pairwise\")\n\nnested_r <- df1 %>%\n  group_by(ID) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(r = map(data, cor_fun))\nhead(nested_r, 6)\n\n\n\n  \n\n\n\nWe can access it like a list:\n\nCodenested_r$data[[1]]\n\n\n\n  \n\n\n\n\nBut we can’t easily (well, nicely) just unnest() a matrix\nWe’ve lost a lot of information along the way\nSo what do we do?\n\n\nCodenested_r %>%\n  select(-data) %>%\n  unnest(r)\n\n\n\n  \n\n\n\n\nWrite a function, of course!\n\n\nCodecor_fun <- function(x) {\n  r <- cor(x %>% select(-count), use = \"pairwise\")\n  r %>% \n    data.frame() %>%\n    rownames_to_column(\"var\") %>%\n    as_tibble()\n}\n\nnested_r <- nested_r %>%\n  mutate(r = map(data, cor_fun))\nhead(nested_r, 10)\n\n\n\n  \n\n\n\n\nLet’s try unnesting again:\n\n\nCodenested_r %>%\n  select(-data) %>%\n  unnest(r) %>% \n  arrange(desc(var))\n\n\n\n  \n\n\n\n\nThere’s more I would usually do here to format a correlation table for each participant and output the file as a PDF or html so I can post it on GitHub / OSF\nBut we’ll get there in Week 8/9!\nDescriptives: Means, sds, etc.\n\nCodetidy_describe <- function(df) {\n  df %>%\n    pivot_longer(\n      -count\n      , names_to = \"item\"\n      , values_to = \"value\"\n      , values_drop_na = T\n      ) %>%\n    group_by(item) %>%\n    summarize(\n      mean   = mean(value,   na.rm = TRUE),\n      sd     = sd(value,     na.rm = TRUE),\n      median = median(value, na.rm = TRUE),\n      min    = min(value,    na.rm = TRUE),\n      max    = max(value,    na.rm = TRUE),\n      n      = n(),\n      n_miss = sum(is.na(value)),\n      .groups = \"drop\"\n      )\n}\n\n\n\nCodenested_r <- nested_r %>%\n  mutate(desc = map(data, tidy_describe)) \nnested_r\n\n\n\n  \n\n\n\n\nCodenested_r %>%\n  select(-data, -r) %>%\n  unnest(desc)\n\n\n\n  \n\n\n\nModels\n\nWe can put essentially anything into a nested data frame.\nThe magic happens because everything is indexed by the other columns in the data frame, so we can keep track of it\nAnd unlike a normal list, we aren’t stuck with nested list structures that are really hard to parse and navigate through\nNext, I’m going to show you how to use purrr with models\nModeling is not a focus of this class, but I want to demonstrate this as a workflow because it completely revolutionized mine!\nBut first, we need to format our data:\n\n\nCodedf_long <- df1 %>%\n  select(-satisfaction) %>%\n  pivot_longer(\n    cols = c(-count, -ID)\n    , names_to = c(\"trait\", \"item\")\n    , names_sep = \"_\"\n    , values_to = \"value\"\n    , values_drop_na = T\n    )\ndf_long\n\n\n\n  \n\n\n\nTo create composites, we’ll:\n1. separate traits from items 2. group_by() trait, count, and ID 3. calculate the composites using summarize()\n\nCodedf_long <- df_long %>%\n  group_by(ID, count, trait) %>%\n  summarize(value = mean(value)) %>%\n  ungroup() %>%\n  left_join(df1 %>% select(ID, count, satisfaction))\n\n`summarise()` has grouped output by 'ID', 'count'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(ID, count)`\n\nCodedf_long\n\n\n\n  \n\n\n\nThen we’ll get within-person centered values using our own little function!\n\nCodecenter <- function(x) x - mean(x, na.rm = T)\n\ndf_long <- df_long %>%\n  group_by(ID, trait) %>%\n  mutate(value_c = center(value)) %>%\n  ungroup()\ndf_long\n\n\n\n  \n\n\n\nAnd grand-mean centered within-person averages\n\nCodedf_long <- df_long %>%\n  group_by(ID, trait) %>%\n  mutate(value_gmc = mean(value)) %>%\n  group_by(trait) %>%\n  mutate(value_gmc = center(value_gmc)) %>%\n  ungroup()\ndf_long\n\n\n\n  \n\n\n\nAnd now we are ready to run our models. But first, we’ll nest() our data.\n\nCodenested_mods <- df_long %>%\n  group_by(trait) %>%\n  nest() %>%\n  ungroup() \nnested_mods\n\n\n\n  \n\n\n\nAnd now run the models.\n\nCoderun_model <- function(d) lmer(satisfaction ~ value_c * value_gmc + (1 | ID), data = d)\n\nnested_mods <- df_long %>%\n  group_by(trait) %>%\n  nest() %>%\n  ungroup() %>% \n  mutate(model = map(data, run_model))\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\nCodenested_mods\n\n\n\n  \n\n\n\nAnd get data frames of the results:\n\nCodesprintfna <- function(x) ifelse(is.na(x), NA_character_, sprintf(\"%.2f\", x))\n\ntidy_tab <- function(m){\n  tidy(m, conf.int = T) %>%\n    mutate(pval = pnorm(abs(estimate/`std.error`), lower.tail = FALSE),\n           p = round(pval, digits = 3),\n           p = ifelse(pval < .001, \"p &lt; .001\", paste0(\"p = \", p))) %>%\n    mutate_at(vars(estimate, conf.low, conf.high), sprintfna) %>%\n    mutate(CI = ifelse(is.na(conf.low), \"\", sprintf(\"[%s,%s]\", conf.low, conf.high))) %>%\n    dplyr::select(term, estimate, CI, p)\n}\n\nnested_mods <- nested_mods %>%\n  mutate(tidy = map(model, tidy_tab))\nnested_mods\n\n\n\n  \n\n\n\nUnnesting\nWhich we can print into pretty data frames\n\nCodenested_mods %>%\n  select(trait, tidy) %>%\n  unnest(tidy)\n\n\n\n  \n\n\n\nUnnesting & Tabling\nWhich we can pretty easily turn into tables:\n\nCodemod_tab <- nested_mods %>%\n  select(trait, tidy) %>%\n  unnest(tidy) %>%\n  pivot_wider(\n    names_from = \"trait\"\n    , names_glue = \"{trait}_{.value}\"\n    , values_from = c(\"estimate\", \"CI\", \"p\")\n  ) %>%\n  select(term, starts_with(\"E\"), starts_with(\"A\"), starts_with(\"C\"), starts_with(\"N\"), starts_with(\"O\"))\n\nmod_tab\n\n\n\n  \n\n\n\n\nCodehdr <- c(1, rep(3, 5))\nnames(hdr) <- c(\" \", \"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Neuroticism\", \"Openness\")\n\nmod_tab <- mod_tab %>%\n  kable(.\n        , \"html\"\n        , escape = F\n        , col.names = c(\"Term\", rep(c(\"<em>b</em>\", \"CI\", \"<em>p</em>\"), times = 5))\n        , align = c(\"r\", rep(\"c\", 15))\n        , caption = \"<strong>Table 1</strong><br><em>Multilevel Model Estimates of Between- and Within-Person Big Five-State Satisfaction Associations\"\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\", font_size = 15) %>%\n  add_header_above(hdr)\nmod_tab\n\n\n\n\nTable 1Multilevel Model Estimates of Between- and Within-Person Big Five-State Satisfaction Associations\n\n \n\n\nExtraversion\nAgreeableness\nConscientiousness\nNeuroticism\nOpenness\n\n\n Term \n    b \n    CI \n    p \n    b \n    CI \n    p \n    b \n    CI \n    p \n    b \n    CI \n    p \n    b \n    CI \n    p \n  \n\n\n\n (Intercept) \n    2.98 \n    [2.93,3.03] \n    p < .001 \n    2.98 \n    [2.93,3.03] \n    p < .001 \n    2.97 \n    [2.92,3.02] \n    p < .001 \n    2.99 \n    [2.94,3.03] \n    p < .001 \n    2.97 \n    [2.93,3.02] \n    p < .001 \n  \n\n value_c \n    -0.07 \n    [-0.14,0.00] \n    p = 0.029 \n    -0.02 \n    [-0.10,0.05] \n    p = 0.279 \n    0.03 \n    [-0.05,0.10] \n    p = 0.231 \n    -0.01 \n    [-0.08,0.06] \n    p = 0.378 \n    0.02 \n    [-0.05,0.09] \n    p = 0.29 \n  \n\n value_gmc \n    0.17 \n    [0.03,0.32] \n    p = 0.008 \n    0.13 \n    [-0.05,0.31] \n    p = 0.081 \n    0.19 \n    [0.06,0.32] \n    p = 0.003 \n    -0.09 \n    [-0.19,0.01] \n    p = 0.032 \n    0.14 \n    [-0.04,0.33] \n    p = 0.067 \n  \n\n value_c:value_gmc \n    0.02 \n    [-0.17,0.22] \n    p = 0.399 \n    0.16 \n    [-0.12,0.44] \n    p = 0.127 \n    0.15 \n    [-0.04,0.34] \n    p = 0.061 \n    -0.19 \n    [-0.34,-0.03] \n    p = 0.009 \n    0.25 \n    [0.01,0.49] \n    p = 0.019 \n  \n\n sd__(Intercept) \n    0.00 \n     \n     \n    0.04 \n     \n     \n    0.00 \n     \n     \n    0.00 \n     \n     \n    0.03 \n     \n     \n  \n\n sd__Observation \n    1.16 \n     \n     \n    1.15 \n     \n     \n    1.17 \n     \n     \n    1.15 \n     \n     \n    1.16"
  },
  {
    "objectID": "05-week5-workbook.html#appendix-sourcing-functions",
    "href": "05-week5-workbook.html#appendix-sourcing-functions",
    "title": "Week 5 Workbook",
    "section": "Appendix: Sourcing Functions",
    "text": "Appendix: Sourcing Functions"
  },
  {
    "objectID": "ps3-week3.html",
    "href": "ps3-week3.html",
    "title": "Problem Set Week 3",
    "section": "",
    "text": "Due Date: Monday, October 23, 12:01 AM PST.\nDownload your problem set for week 3 below or on Canvas.\nAnswers will be posted after the due date."
  },
  {
    "objectID": "ps5-week5.html",
    "href": "ps5-week5.html",
    "title": "Problem Set Week 5",
    "section": "",
    "text": "Due Date: Monday, November 6, 12:01 AM PST.\nDownload your problem set for week 5 below or on Canvas.\nAnswers will be posted after the due date."
  },
  {
    "objectID": "09-week9-slides.html",
    "href": "09-week9-slides.html",
    "title": "Week 9 Slides",
    "section": "",
    "text": "Welcome to Week 9! This week, we’ll talk about building Git, GitHub, parallelization, future, and furrr."
  },
  {
    "objectID": "07-week7-workbook.html",
    "href": "07-week7-workbook.html",
    "title": "Week 7 Workbook",
    "section": "",
    "text": "Loading required package: Matrix\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr   1.1.3     ✔ readr   2.1.4\n✔ forcats 1.0.0     ✔ stringr 1.5.0\n✔ ggplot2 3.4.2     ✔ tibble  3.2.1\n✔ purrr   1.0.2     ✔ tidyr   1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()      masks psych::%+%()\n✖ ggplot2::alpha()    masks psych::alpha()\n✖ dplyr::arrange()    masks plyr::arrange()\n✖ purrr::compact()    masks plyr::compact()\n✖ dplyr::count()      masks plyr::count()\n✖ dplyr::desc()       masks plyr::desc()\n✖ tidyr::expand()     masks Matrix::expand()\n✖ dplyr::failwith()   masks plyr::failwith()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::group_rows() masks kableExtra::group_rows()\n✖ dplyr::id()         masks plyr::id()\n✖ dplyr::lag()        masks stats::lag()\n✖ dplyr::mutate()     masks plyr::mutate()\n✖ tidyr::pack()       masks Matrix::pack()\n✖ dplyr::rename()     masks plyr::rename()\n✖ dplyr::summarise()  masks plyr::summarise()\n✖ dplyr::summarize()  masks plyr::summarize()\n✖ tidyr::unpack()     masks Matrix::unpack()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
  },
  {
    "objectID": "07-week7-workbook.html#dataset-we-will-use",
    "href": "07-week7-workbook.html#dataset-we-will-use",
    "title": "Week 7 Workbook",
    "section": "Dataset we will use",
    "text": "Dataset we will use\nWe will use rtweet to pull Twitter data from the PAC-12 universities. We will use the university admissions Twitter handle if there is one, or the main Twitter handle for the university if there isn’t one:\n\n\n\n\nCode# Load previously pulled Twitter data\np12_full_df <- readRDS(\"week7-data.RDS\")\nglimpse(p12_full_df)\n\nRows: 328\nColumns: 90\n$ user_id                 <chr> \"22080148\", \"22080148\", \"22080148\", \"22080148\"…\n$ status_id               <chr> \"1254177694599675904\", \"1253431405993840646\", …\n$ created_at              <dttm> 2020-04-25 22:37:18, 2020-04-23 21:11:49, 202…\n$ screen_name             <chr> \"WSUPullman\", \"WSUPullman\", \"WSUPullman\", \"WSU…\n$ text                    <chr> \"Big Dez is headed to Indy!\\n\\n#GoCougs | #NFL…\n$ source                  <chr> \"Twitter for iPhone\", \"Twitter Web App\", \"Twit…\n$ display_text_width      <dbl> 125, 58, 246, 83, 56, 64, 156, 271, 69, 140, 4…\n$ reply_to_status_id      <chr> NA, NA, NA, NA, NA, NA, NA, NA, \"1252615862659…\n$ reply_to_user_id        <chr> NA, NA, NA, NA, NA, NA, NA, NA, \"22080148\", NA…\n$ reply_to_screen_name    <chr> NA, NA, NA, NA, NA, NA, NA, NA, \"WSUPullman\", …\n$ is_quote                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ is_retweet              <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ favorite_count          <int> 0, 322, 30, 55, 186, 53, 22, 44, 11, 0, 69, 42…\n$ retweet_count           <int> 230, 32, 1, 5, 0, 3, 2, 6, 2, 6, 3, 4, 5, 5, 2…\n$ quote_count             <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ reply_count             <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hashtags                <list> <\"GoCougs\", \"NFLDraft2020\", \"NFLCougs\">, <\"WS…\n$ symbols                 <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ urls_url                <list> NA, NA, NA, NA, NA, NA, NA, \"commencement.wsu…\n$ urls_t.co               <list> NA, NA, NA, NA, NA, NA, NA, \"https://t.co/RR4…\n$ urls_expanded_url       <list> NA, NA, NA, NA, NA, NA, NA, \"https://commence…\n$ media_url               <list> \"http://pbs.twimg.com/ext_tw_video_thumb/1254…\n$ media_t.co              <list> \"https://t.co/NdGsvXnij7\", \"https://t.co/0OWG…\n$ media_expanded_url      <list> \"https://twitter.com/WSUCougarFB/status/12541…\n$ media_type              <list> \"photo\", \"photo\", \"photo\", \"photo\", \"photo\", …\n$ ext_media_url           <list> \"http://pbs.twimg.com/ext_tw_video_thumb/1254…\n$ ext_media_t.co          <list> \"https://t.co/NdGsvXnij7\", \"https://t.co/0OWG…\n$ ext_media_expanded_url  <list> \"https://twitter.com/WSUCougarFB/status/12541…\n$ ext_media_type          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ mentions_user_id        <list> <\"1250265324\", \"1409024796\", \"180884045\">, NA…\n$ mentions_screen_name    <list> <\"WSUCougarFB\", \"dadpat7\", \"Colts\">, NA, \"WSU…\n$ lang                    <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\"…\n$ quoted_status_id        <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"12529…\n$ quoted_text             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"My WS…\n$ quoted_created_at       <dttm> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2020-…\n$ quoted_source           <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Twitt…\n$ quoted_favorite_count   <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 209, N…\n$ quoted_retweet_count    <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 6, NA,…\n$ quoted_user_id          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"43947…\n$ quoted_screen_name      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"maddd…\n$ quoted_name             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Maddy…\n$ quoted_followers_count  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 629, N…\n$ quoted_friends_count    <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 382, N…\n$ quoted_statuses_count   <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 8881, …\n$ quoted_location         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Seatt…\n$ quoted_description      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"WSU A…\n$ quoted_verified         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, FALSE,…\n$ retweet_status_id       <chr> \"1254159118996127746\", NA, NA, NA, NA, NA, NA,…\n$ retweet_text            <chr> \"Big Dez is headed to Indy!\\n\\n#GoCougs | #NFL…\n$ retweet_created_at      <dttm> 2020-04-25 21:23:29, NA, NA, NA, NA, NA, NA, …\n$ retweet_source          <chr> \"Twitter for iPhone\", NA, NA, NA, NA, NA, NA, …\n$ retweet_favorite_count  <int> 1402, NA, NA, NA, NA, NA, NA, NA, NA, 26, NA, …\n$ retweet_retweet_count   <int> 230, NA, NA, NA, NA, NA, NA, NA, NA, 6, NA, NA…\n$ retweet_user_id         <chr> \"1250265324\", NA, NA, NA, NA, NA, NA, NA, NA, …\n$ retweet_screen_name     <chr> \"WSUCougarFB\", NA, NA, NA, NA, NA, NA, NA, NA,…\n$ retweet_name            <chr> \"Washington State Football\", NA, NA, NA, NA, N…\n$ retweet_followers_count <int> 77527, NA, NA, NA, NA, NA, NA, NA, NA, 996, NA…\n$ retweet_friends_count   <int> 1448, NA, NA, NA, NA, NA, NA, NA, NA, 316, NA,…\n$ retweet_statuses_count  <int> 15363, NA, NA, NA, NA, NA, NA, NA, NA, 1666, N…\n$ retweet_location        <chr> \"Pullman, WA\", NA, NA, NA, NA, NA, NA, NA, NA,…\n$ retweet_description     <chr> \"Official Twitter home of Washington State Cou…\n$ retweet_verified        <lgl> TRUE, NA, NA, NA, NA, NA, NA, NA, NA, FALSE, N…\n$ place_url               <chr> NA, NA, NA, NA, NA, \"https://api.twitter.com/1…\n$ place_name              <chr> NA, NA, NA, NA, NA, \"Pullman\", NA, NA, NA, NA,…\n$ place_full_name         <chr> NA, NA, NA, NA, NA, \"Pullman, WA\", NA, NA, NA,…\n$ place_type              <chr> NA, NA, NA, NA, NA, \"city\", NA, NA, NA, NA, \"c…\n$ country                 <chr> NA, NA, NA, NA, NA, \"United States\", NA, NA, N…\n$ country_code            <chr> NA, NA, NA, NA, NA, \"US\", NA, NA, NA, NA, \"US\"…\n$ geo_coords              <list> <NA, NA>, <NA, NA>, <NA, NA>, <NA, NA>, <NA, …\n$ coords_coords           <list> <NA, NA>, <NA, NA>, <NA, NA>, <NA, NA>, <NA, …\n$ bbox_coords             <list> <NA, NA, NA, NA, NA, NA, NA, NA>, <NA, NA, NA…\n$ status_url              <chr> \"https://twitter.com/WSUPullman/status/1254177…\n$ name                    <chr> \"WSU Pullman\", \"WSU Pullman\", \"WSU Pullman\", \"…\n$ location                <chr> \"Pullman, Washington USA\", \"Pullman, Washingto…\n$ description             <chr> \"We are an award-winning research university i…\n$ url                     <chr> \"http://t.co/VxKZH9BuMS\", \"http://t.co/VxKZH9B…\n$ protected               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ followers_count         <int> 43914, 43914, 43914, 43914, 43914, 43914, 4391…\n$ friends_count           <int> 9717, 9717, 9717, 9717, 9717, 9717, 9717, 9717…\n$ listed_count            <int> 556, 556, 556, 556, 556, 556, 556, 556, 556, 5…\n$ statuses_count          <int> 15234, 15234, 15234, 15234, 15234, 15234, 1523…\n$ favourites_count        <int> 20124, 20124, 20124, 20124, 20124, 20124, 2012…\n$ account_created_at      <dttm> 2009-02-26 23:39:34, 2009-02-26 23:39:34, 200…\n$ verified                <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ profile_url             <chr> \"http://t.co/VxKZH9BuMS\", \"http://t.co/VxKZH9B…\n$ profile_expanded_url    <chr> \"http://www.wsu.edu\", \"http://www.wsu.edu\", \"h…\n$ account_lang            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ profile_banner_url      <chr> \"https://pbs.twimg.com/profile_banners/2208014…\n$ profile_background_url  <chr> \"http://abs.twimg.com/images/themes/theme5/bg.…\n$ profile_image_url       <chr> \"http://pbs.twimg.com/profile_images/576502906…\n\nCodep12_df <- p12_full_df |> \n  select(user_id, created_at, screen_name, text, location)\nhead(p12_df)"
  },
  {
    "objectID": "07-week7-workbook.html#creating-strings",
    "href": "07-week7-workbook.html#creating-strings",
    "title": "Week 7 Workbook",
    "section": "Creating Strings",
    "text": "Creating Strings\nCreating string using single quotes\nNotice how R stores strings using double quotes internally:\n\nCodemy_string <- 'This is a string'\nmy_string\n\n[1] \"This is a string\"\n\n\nCreating string using double quotes\n\nCodemy_string <- \"Strings can also contain numbers: 123\"\nmy_string\n\n[1] \"Strings can also contain numbers: 123\"\n\n\nChecking class and type of strings\n\nCodeclass(my_string)\n\n[1] \"character\"\n\nCodetypeof(my_string)\n\n[1] \"character\"\n\n\nQuotes in quotes\nNote: To include quotes as part of the string, we can either use the other type of quotes to surround the string (i.e., ' or \") or escape the quote using a backslash (\\).\n\nCode# Include quote by using the other type of quotes to surround the string \nmy_string <- \"There's no issues with this string.\"\nmy_string\n\n[1] \"There's no issues with this string.\"\n\n\n\nCode# Include quote of the same type by escaping it with a backslash\nmy_string <- 'There\\'s no issues with this string.'\nmy_string\n\n[1] \"There's no issues with this string.\"\n\n\n\nCode# This would not work\nmy_string <- 'There's an issue with this string.'\nmy_string"
  },
  {
    "objectID": "07-week7-workbook.html#str_length",
    "href": "07-week7-workbook.html#str_length",
    "title": "Week 7 Workbook",
    "section": "str_length()",
    "text": "str_length()\nThe str_length() function: - Function: Find string length\n\nCode?str_length\n\n\n\nCode# SYNTAX\nstr_length(string)\n\n\n\nArguments:\n\n\nstring: Character vector (or vector coercible to character)\n\n\nNote that str_length() calculates the length of a string, whereas the length() function (which is not part of stringr package) calculates the number of elements in an object\n\nUsing str_length() on string\n\nCodestr_length(\"cats\")\n\n[1] 4\n\n\nCompare to length(), which treats the string as a single object:\n\nCodelength(\"cats\")\n\n[1] 1\n\n\nstr_length() on character vector\n\nCodestr_length(c(\"cats\", \"in\", \"hat\"))\n\n[1] 4 2 3\n\n\nCompare to length(), which finds the number of elements in the vector:\n\nCodelength(c(\"cats\", \"in\", \"hat\"))\n\n[1] 3\n\n\nUsing str_length() on other vectors coercible to character\n\n\nLogical vectors can be coerced to character vectors:\n\n\nCodestr_length(c(TRUE, FALSE))\n\n[1] 4 5\n\n\n\n\nNumeric vectors can be coerced to character vectors:\n\n\nCodestr_length(c(1, 2.5, 3000))\n\n[1] 1 3 4\n\n\n\n\nInteger vectors can be coerced to character vectors:\n\n\nCodestr_length(c(2L, 100L))\n\n[1] 1 3\n\n\nUsing str_length() on dataframe column\nRecall that the columns in a dataframe are just vectors, so we can use str_length() as long as the vector is coercible to character type.\n\nCodestr_length(p12_df$screen_name[1:20])\n\n [1] 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n\n\n\nCodep12_df %>% select(screen_name) %>% unique() %>% \n  mutate(screen_name_len = str_length(screen_name))"
  },
  {
    "objectID": "07-week7-workbook.html#str_c",
    "href": "07-week7-workbook.html#str_c",
    "title": "Week 7 Workbook",
    "section": "str_c()",
    "text": "str_c()\nThe str_c() function:\n\nFunction: Concatenate strings between vectors (element-wise)\n\n\nCode?str_c\n\n# SYNTAX AND DEFAULT VALUES\nstr_c(..., sep = \"\", collapse = NULL)\n\n\n\nArguments:\n\nThe input is one or more character vectors (or vectors coercible to character)\n\nZero length arguments are removed\nShort arguments are recycled to the length of the longest\n\n\n\nsep: String to insert between input vectors\n\ncollapse: Optional string used to combine input vectors into single string\n\n\n\nUsing str_c() on one vector\nSince we only provided one input vector, it has nothing to concatenate with, so str_c() will just return the same vector:\n\nCodestr_c(c(\"a\", \"b\", \"c\"))\n\n[1] \"a\" \"b\" \"c\"\n\n\nUsing str_c() on one vector\nNote that specifying the sep argument will also not have any effect because we only have one input vector, and sep is the separator between multiple vectors:\n\nCodestr_c(c(\"a\", \"b\", \"c\"), sep = \"~\")\n\n[1] \"a\" \"b\" \"c\"\n\nCode# Check length: Output is the original vector of 3 elements\nstr_c(c(\"a\", \"b\", \"c\")) %>% length()\n\n[1] 3\n\n\nUsing str_c() on one vector\n\nAs seen on the previous slide, str_c() returns a vector by default (because the default value for the collapse argument is NULL).\nBut we can specify a string for collapse in order to collapse the elements of the output vector into a single string:\n\n\nCodestr_c(c(\"a\", \"b\", \"c\"), collapse = \"|\")\n\n[1] \"a|b|c\"\n\nCode# Check length: Output vector of length 3 is collapsed into a single string\nstr_c(c(\"a\", \"b\", \"c\"), collapse = \"|\") %>% length()\n\n[1] 1\n\nCode# Check str_length: This gives the length of the collapsed string, which is 5 characters long\nstr_c(c(\"a\", \"b\", \"c\"), collapse = \"|\") %>% str_length()\n\n[1] 5\n\n\nUsing str_c() on more than one vector\nWhen we provide multiple input vectors, we can see that the vectors get concatenated element-wise (i.e., 1st element from each vector are concatenated, 2nd element from each vector are concatenated, etc):\n\nCodestr_c(c(\"a\", \"b\", \"c\"), c(\"x\", \"y\", \"z\"), c(\"!\", \"?\", \";\"))\n\n[1] \"ax!\" \"by?\" \"cz;\"\n\n\nUsing str_c() on more than one vector\nThe default separator for each element-wise concatenation is an empty string (\"\"), but we can customize that by specifying the sep argument:\n\nCodestr_c(c(\"a\", \"b\", \"c\"), c(\"x\", \"y\", \"z\"), c(\"!\", \"?\", \";\"), sep = \"~\")\n\n[1] \"a~x~!\" \"b~y~?\" \"c~z~;\"\n\nCode# Check length: Output vector is same length as input vectors\nstr_c(c(\"a\", \"b\", \"c\"), c(\"x\", \"y\", \"z\"), c(\"!\", \"?\", \";\"), sep = \"~\") %>% length()\n\n[1] 3\n\n\nUsing str_c() on more than one vector\nAgain, we can specify the collapse argument in order to collapse the elements of the output vector into a single string:\n\nCodestr_c(c(\"a\", \"b\", \"c\"), c(\"x\", \"y\", \"z\"), c(\"!\", \"?\", \";\"), collapse = \"|\")\n\n[1] \"ax!|by?|cz;\"\n\nCode# Check length: Output vector of length 3 is collapsed into a single string\nstr_c(c(\"a\", \"b\", \"c\"), c(\"x\", \"y\", \"z\"), c(\"!\", \"?\", \";\"), collapse = \"|\") %>% length()\n\n[1] 1\n\nCode# Specifying both `sep` and `collapse`\nstr_c(c(\"a\", \"b\", \"c\"), c(\"x\", \"y\", \"z\"), c(\"!\", \"?\", \";\"), sep = \"~\", collapse = \"|\")\n\n[1] \"a~x~!|b~y~?|c~z~;\""
  },
  {
    "objectID": "07-week7-workbook.html#str_sub",
    "href": "07-week7-workbook.html#str_sub",
    "title": "Week 7 Workbook",
    "section": "str_sub()",
    "text": "str_sub()\nThe str_sub() function:\n\nFunction: Subset strings\nArguments:\n\n\nstring: Character vector (or vector coercible to character)\n\nstart: Position of first character to be included in substring (default: 1)\n\nend: Position of last character to be included in substring (default: -1)\n\nNegative index = counting backwards\n\n\n\nomit_na: If TRUE, missing values in any of the arguments provided will result in an unchanged input\n\n\n\n\nCode?str_sub\n\n# SYNTAX AND DEFAULT VALUES\nstr_sub(string, start = 1L, end = -1L)\nstr_sub(string, start = 1L, end = -1L, omit_na = FALSE) <- value\n\n\n\nWhen str_sub() is used in the assignment form, you can replace the subsetted part of the string with a value of your choice\n\nIf an element in the vector is too short to meet the subset specification, the replacement value will be concatenated to the end of that element\nNote that this modifies your input vector directly, so you must have the vector saved to a variable (see example below)\n\n\n\nUsing str_sub() to subset strings\nIf no start and end positions are specified, str_sub() will by default return the entire (original) string:\n\nCodestr_sub(string = c(\"abcdefg\", 123, TRUE))\n\n[1] \"abcdefg\" \"123\"     \"TRUE\"   \n\n\nNote that if an element is shorter than the specified end (i.e., 123 in the example below), it will just include all the available characters that it does have:\n\nCodestr_sub(string = c(\"abcdefg\", 123, TRUE), start = 2, end = 4)\n\n[1] \"bcd\" \"23\"  \"RUE\"\n\n\nRemember we can also use negative index to count the position starting from the back:\n\nCodestr_sub(c(\"abcdefg\", 123, TRUE), start = 2, end = -2)\n\n[1] \"bcdef\" \"2\"     \"RU\"   \n\n\nUsing str_sub() to replace strings\nIf no start and end positions are specified, str_sub() will by default return the original string, so the entire string would be replaced:\n\nCodev <- c(\"A\", \"AB\", \"ABC\", \"ABCD\", \"ABCDE\")\nstr_sub(v, start = 1,end =-1)\n\n[1] \"A\"     \"AB\"    \"ABC\"   \"ABCD\"  \"ABCDE\"\n\nCodestr_sub(v, start = 1,end =-1) <- \"*\"\nv\n\n[1] \"*\" \"*\" \"*\" \"*\" \"*\"\n\n\nUsing str_sub() on dataframe column\nWe can use as.character() to turn the created_at value to a string, then use str_sub() to extract out various date/time components from the string:\n\nCodep12_datetime_df <- p12_df %>% select(created_at) %>%\n  mutate(\n      dt_chr = as.character(created_at),\n      date_chr = str_sub(dt_chr, 1, 10),\n      yr_chr = str_sub(dt_chr, 1, 4),\n      mth_chr = str_sub(dt_chr, 6, 7),\n      day_chr = str_sub(dt_chr, 9, 10),\n      hr_chr = str_sub(dt_chr, -8, -7),\n      min_chr = str_sub(dt_chr, -5, -4),\n      sec_chr = str_sub(dt_chr, -2, -1)\n    )\n\np12_datetime_df"
  },
  {
    "objectID": "07-week7-workbook.html#other-stringr-functions",
    "href": "07-week7-workbook.html#other-stringr-functions",
    "title": "Week 7 Workbook",
    "section": "Other stringr functions",
    "text": "Other stringr functions\nOther useful stringr functions:\n\n\nstr_to_upper(): Turn strings to uppercase\n\nstr_to_lower(): Turn strings to lowercase\n\nstr_sort(): Sort a character vector\n\nstr_trim(): Trim whitespace from strings (including \\n, \\t, etc.)\n\nstr_pad(): Pad strings with specified character\n\nUsing str_to_upper() to turn strings to uppercase\nTurn column names of p12_df to uppercase:\n\nCode# Column names are originally lowercase\nnames(p12_df)\n\n[1] \"user_id\"     \"created_at\"  \"screen_name\" \"text\"        \"location\"   \n\nCode# Turn column names to uppercase\nnames(p12_df) <- str_to_upper(names(p12_df))\nnames(p12_df)\n\n[1] \"USER_ID\"     \"CREATED_AT\"  \"SCREEN_NAME\" \"TEXT\"        \"LOCATION\"   \n\n\nUsing str_to_lower() to turn strings to lowercase\nTurn column names of p12_df to lowercase:\n\nCode# Column names are originally uppercase\nnames(p12_df)\n\n[1] \"USER_ID\"     \"CREATED_AT\"  \"SCREEN_NAME\" \"TEXT\"        \"LOCATION\"   \n\nCode# Turn column names to lowercase\nnames(p12_df) <- str_to_lower(names(p12_df))\nnames(p12_df)\n\n[1] \"user_id\"     \"created_at\"  \"screen_name\" \"text\"        \"location\"   \n\n\nUsing str_sort() to sort character vector\nSort the vector of p12_df column names:\n\nCode# Before sort\nnames(p12_df)\n\n[1] \"user_id\"     \"created_at\"  \"screen_name\" \"text\"        \"location\"   \n\nCode# Sort alphabetically (default)\nstr_sort(names(p12_df))\n\n[1] \"created_at\"  \"location\"    \"screen_name\" \"text\"        \"user_id\"    \n\nCode# Sort reverse alphabetically\nstr_sort(names(p12_df), decreasing = TRUE)\n\n[1] \"user_id\"     \"text\"        \"screen_name\" \"location\"    \"created_at\" \n\n\nUsing str_trim() to trim whitespace from string\n\nCode# Trim whitespace from both left and right sides (default)\nstr_trim(c(\"\\nABC \", \" XYZ\\t\"))\n\n[1] \"ABC\" \"XYZ\"\n\nCode# Trim whitespace from left side\nstr_trim(c(\"\\nABC \", \" XYZ\\t\"), side = \"left\")\n\n[1] \"ABC \"  \"XYZ\\t\"\n\nCode# Trim whitespace from right side\nstr_trim(c(\"\\nABC \", \" XYZ\\t\"), side = \"right\")\n\n[1] \"\\nABC\" \" XYZ\" \n\n\nUsing str_pad() to pad string with character\nLet’s say we have a vector of zip codes that has lost all leading 0’s. We can use str_pad() to add that back in:\n\nCode# Pad the left side of strings with \"0\" until width of 5 is reached\nstr_pad(c(95035, 90024, 5009, 5030), width = 5, side = \"left\", pad = \"0\")\n\n[1] \"95035\" \"90024\" \"05009\" \"05030\""
  },
  {
    "objectID": "07-week7-workbook.html#example-of-using-regular-expression-in-action",
    "href": "07-week7-workbook.html#example-of-using-regular-expression-in-action",
    "title": "Week 7 Workbook",
    "section": "Example of using regular expression in action:",
    "text": "Example of using regular expression in action:\n\nHow can we match all occurrences of times in the following string? (i.e., 10 AM and 1 PM)\n\n\"Class starts at 10 AM and ends at 1 PM.\"\n\n\nThe regular expression \\d+ [AP]M can!\n\n\nCodemy_string = \"Class starts at 10 AM and ends at 1 PM.\"\nmy_regex = \"\\\\d+ [AP]M\"\n\n# The escaped string \"\\\\d\" results in the regex \\d\nprint(my_regex)\n\n[1] \"\\\\d+ [AP]M\"\n\nCodewriteLines(my_regex)\n\n\\d+ [AP]M\n\nCode# View matches for our regular expression\nstr_view_all(string = my_string, pattern = my_regex)\n\n[1] │ Class starts at <10 AM> and ends at <1 PM>.\n\n\n\nHow the regular expression \\d+ [AP]M works:\n\n\n\\d+ matches 1 or more digits in a row\n\n\n\\d means match all numeric digits (i.e., 0-9)\n\n+ means match 1 or more of\n\n\nmatches a literal space\n\n[AP]M matches either AM or PM\n\n\n[AP] means match either an A or P at that position\n\nM means match a literal M"
  },
  {
    "objectID": "07-week7-workbook.html#some-common-regular-expression-patterns-include-not-inclusive",
    "href": "07-week7-workbook.html#some-common-regular-expression-patterns-include-not-inclusive",
    "title": "Week 7 Workbook",
    "section": "Some common regular expression patterns include (not inclusive):",
    "text": "Some common regular expression patterns include (not inclusive):\n\nCharacter classes\nQuantifiers\nAnchors\n\nSets and ranges\nGroups and backreferences\n\nCredit: DaveChild Regular Expression Cheat Sheet\nCharacter classes\n\n\nSTRING\nREGEX\nMATCHES\n\n\n\n\"\\\\d\"\n\\d\nany digit\n\n\n\"\\\\D\"\n\\D\nany non-digit\n\n\n\"\\\\s\"\n\\s\nany whitespace\n\n\n\"\\\\S\"\n\\S\nany non-whitespace\n\n\n\"\\\\w\"\n\\w\nany word character\n\n\n\"\\\\W\"\n\\W\nany non-word character\n\n\n\nCredit: Working with strings in stringr Cheat sheet\n\n\nThere are certain character classes in regular expression that have special meaning. For example:\n\n\n\\d is used to match any digit (i.e., number)\n\n\\s is used to match any whitespace (i.e., space, tab, or newline character)\n\n\\w is used to match any word character (i.e., alphanumeric character or underscore)\n\n\n“But wait… there’s more! Before a regex is interpreted as a regular expression, it is also interpreted by R as a string. And backslash is used to escape there as well. So, in the end, you need to preprend two backslashes…”\nCredit: Escaping sequences from Stat 545\nThis means in R, when we want to use regular expression patterns \"\\d\",\"\\s\", \"\\w\", etc. to match to strings, we must write out the regex patterns as \"\\\\d\",\"\\\\s\", \"\\\\w\", etc.\nUsing \\d & \\D to match digits & non-digits\n\nGoal: write a regular expression pattern that matches to any digit in the string p12_df$text[119]\n\nWe can use \\d to match all instances of a digit (i.e., number):\n\n\nCode# Match any instances of a digit\nstr_view_all(string = p12_df$text[119], pattern = \"\\\\d\")\n\n[1] │ \"I stand with my colleagues at @UW and America's leading research universities as they take fight to Covid-<1><9> in our labs and hospitals.\"\n    │ \n    │ #ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/<4>YSf<4>SpPe<0>\n\n\nKEY POINT WITH REGEX\n\nOur regular expression is the value we specify for the pattern argument above; this is our “regex object”\nWe want our regex object to include the regular expression \\d, which matches to any digit\nWe specify our regex object as \"\\\\d\" rather than \"\\d\"\n\nUse regular expression \\D to match all instances of a non-digit character:\n\nCode# Match any instances of a non-digit\nstr_view_all(string = p12_df$text[119], pattern = \"\\\\D\")\n\n[1] │ <\"><I>< ><s><t><a><n><d>< ><w><i><t><h>< ><m><y>< ><c><o><l><l><e><a><g><u><e><s>< ><a><t>< ><@><U><W>< ><a><n><d>< ><A><m><e><r><i><c><a><'><s>< ><l><e><a><d><i><n><g>< ><r><e><s><e><a><r><c><h>< ><u><n><i><v><e><r><s><i><t><i><e><s>< ><a><s>< ><t><h><e><y>< ><t><a><k><e>< ><f><i><g><h><t>< ><t><o>< ><C><o><v><i><d><->19< ><i><n>< ><o><u><r>< ><l><a><b><s>< ><a><n><d>< ><h><o><s><p><i><t><a><l><s><.><\"><\n    │ ><\n    │ ><#><P><r><o><u><d><T><o><B><e><O><n><T><h><e><i><r><T><e><a><m>< ><x>< ><#><A><l><w><a><y><s><C><o><m><p><e><t><e>< ><x>< ><#><G><o><H><u><s><k><i><e><s>< ><h><t><t><p><s><:></></><t><.><c><o></>4<Y><S><f>4<S><p><P><e>0\n\n\nMatch to all instances of a digit followed by a non-digit character:\n\nCodestr_view_all(string = p12_df$text[119], pattern = \"\\\\d\\\\D\")\n\n[1] │ \"I stand with my colleagues at @UW and America's leading research universities as they take fight to Covid-1<9 >in our labs and hospitals.\"\n    │ \n    │ #ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/<4Y>Sf<4S>pPe0\n\n\nUsing \\s & \\S to match whitespace & non-whitespace\nWe can use \\s to match all instances of a whitespace (i.e., space, tab, or newline character):\n\nCode# Match any instances of a whitespace\nstr_view_all(\n  string = p12_df$text[119]\n  , pattern = \"\\\\s\"\n  )\n\n[1] │ \"I< >stand< >with< >my< >colleagues< >at< >@UW< >and< >America's< >leading< >research< >universities< >as< >they< >take< >fight< >to< >Covid-19< >in< >our< >labs< >and< >hospitals.\"<\n    │ ><\n    │ >#ProudToBeOnTheirTeam< >x< >#AlwaysCompete< >x< >#GoHuskies< >https://t.co/4YSf4SpPe0\n\n\nWe can use \\S to match all instances of a non-whitespace character:\n\nCode# Match any instances of a non-whitespace\nstr_view_all(\n  string = p12_df$text[119]\n  , pattern = \"\\\\S\"\n  )\n\n[1] │ <\"><I> <s><t><a><n><d> <w><i><t><h> <m><y> <c><o><l><l><e><a><g><u><e><s> <a><t> <@><U><W> <a><n><d> <A><m><e><r><i><c><a><'><s> <l><e><a><d><i><n><g> <r><e><s><e><a><r><c><h> <u><n><i><v><e><r><s><i><t><i><e><s> <a><s> <t><h><e><y> <t><a><k><e> <f><i><g><h><t> <t><o> <C><o><v><i><d><-><1><9> <i><n> <o><u><r> <l><a><b><s> <a><n><d> <h><o><s><p><i><t><a><l><s><.><\">\n    │ \n    │ <#><P><r><o><u><d><T><o><B><e><O><n><T><h><e><i><r><T><e><a><m> <x> <#><A><l><w><a><y><s><C><o><m><p><e><t><e> <x> <#><G><o><H><u><s><k><i><e><s> <h><t><t><p><s><:></></><t><.><c><o></><4><Y><S><f><4><S><p><P><e><0>\n\n\nUsing \\w & \\W to match words & non-words\nWe can use \\w to match all instances of a word character (i.e., alphanumeric character or underscore):\n\nCode# Match any instances of a word character\nstr_view_all(\n  string = p12_df$text[119]\n  , pattern = \"\\\\w\"\n  )\n\n[1] │ \"<I> <s><t><a><n><d> <w><i><t><h> <m><y> <c><o><l><l><e><a><g><u><e><s> <a><t> @<U><W> <a><n><d> <A><m><e><r><i><c><a>'<s> <l><e><a><d><i><n><g> <r><e><s><e><a><r><c><h> <u><n><i><v><e><r><s><i><t><i><e><s> <a><s> <t><h><e><y> <t><a><k><e> <f><i><g><h><t> <t><o> <C><o><v><i><d>-<1><9> <i><n> <o><u><r> <l><a><b><s> <a><n><d> <h><o><s><p><i><t><a><l><s>.\"\n    │ \n    │ #<P><r><o><u><d><T><o><B><e><O><n><T><h><e><i><r><T><e><a><m> <x> #<A><l><w><a><y><s><C><o><m><p><e><t><e> <x> #<G><o><H><u><s><k><i><e><s> <h><t><t><p><s>://<t>.<c><o>/<4><Y><S><f><4><S><p><P><e><0>\n\n\nWe can use \\W to match all instances of a non-word character:\n\nCode# Match any instances of a non-word character\nstr_view_all(\n  string = p12_df$text[119]\n  , pattern = \"\\\\W\"\n  )\n\n[1] │ <\">I< >stand< >with< >my< >colleagues< >at< ><@>UW< >and< >America<'>s< >leading< >research< >universities< >as< >they< >take< >fight< >to< >Covid<->19< >in< >our< >labs< >and< >hospitals<.><\"><\n    │ ><\n    │ ><#>ProudToBeOnTheirTeam< >x< ><#>AlwaysCompete< >x< ><#>GoHuskies< >https<:></></>t<.>co</>4YSf4SpPe0\n\n\nThis matches all instances of 3-letter words:\n\nCodestr_view_all(\n  string = p12_df$text[119]\n  , pattern = \"\\\\W\\\\w\\\\w\\\\w\\\\W\"\n  )\n\n[1] │ \"I stand with my colleagues at @UW< and >America's leading research universities as they take fight to Covid-19 in< our >labs< and >hospitals.\"\n    │ \n    │ #ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/4YSf4SpPe0\n\n\nWrap-Up: Character Classes\n\nThe second half of the table above shows other regular expressions involving backslashes\nThis includes special characters like \\n and \\t, as well as using backslash to escape characters that have special meanings in regex, like . or ? (as we will soon see.\nSo to match a literal period or question mark, we need to use the regex \\. and \\?, or strings \"\\\\.\" and \"\\\\?\" in R.\n\n\n\nSTRING\nREGEX\nMATCHES\n\n\n\n\"\\\\n\"\n\\n\nnewline\n\n\n\"\\\\t\"\n\\t\ntab\n\n\n\"\\\\\\\\\"\n\\\\\n\\\n\n\n\"\\\\.\"\n\\.\n.\n\n\n\"\\\\?\"\n\\?\n?\n\n\n\"\\\\(\"\n\\(\n(\n\n\n\"\\\\)\"\n\\)\n)\n\n\n\"\\\\{\"\n\\{\n{\n\n\n\"\\\\}\"\n\\}\n}"
  },
  {
    "objectID": "07-week7-workbook.html#quantifiers",
    "href": "07-week7-workbook.html#quantifiers",
    "title": "Week 7 Workbook",
    "section": "Quantifiers",
    "text": "Quantifiers\n\n\nCharacter\nDescription\n\n\n\n*\n0 or more\n\n\n?\n0 or 1\n\n\n+\n1 or more\n\n\n{3}\nExactly 3\n\n\n{3,}\n3 or more\n\n\n{3,5}\n3, 4, or 5\n\n\n\n\nWe can use quantifiers to specify the amount of a certain character or expression to match.\nThe quantifier should directly follow the pattern you want to quantify.\nFor example, s? matches 0 or 1 s and \\d{4} matches exactly 4 digits.\n\nUsing the *, ?, and + quantifiers\nWe can use * to match 0 or more of a pattern:\n\nCode# Matches all instances of `s` followed by 0 or more non-word character\nstr_view_all(string = p12_df$text[119], pattern = \"s\\\\W*\")\n\n[1] │ \"I <s>tand with my colleague<s >at @UW and America'<s >leading re<s>earch univer<s>itie<s >a<s >they take fight to Covid-19 in our lab<s >and ho<s>pital<s.\"\n    │ \n    │ #>ProudToBeOnTheirTeam x #Alway<s>Compete x #GoHu<s>kie<s >http<s://>t.co/4YSf4SpPe0\n\n\nUsing the *, ?, and + quantifiers\nWe can use ? to match 0 or 1 of a pattern:\n\nCode# Matches all instances of `s` followed by 0 or 1 non-word character\nstr_view_all(string = p12_df$text[119], pattern = \"s\\\\W?\")\n\n[1] │ \"I <s>tand with my colleague<s >at @UW and America'<s >leading re<s>earch univer<s>itie<s >a<s >they take fight to Covid-19 in our lab<s >and ho<s>pital<s.>\"\n    │ \n    │ #ProudToBeOnTheirTeam x #Alway<s>Compete x #GoHu<s>kie<s >http<s:>//t.co/4YSf4SpPe0\n\n\nUsing the *, ?, and + quantifiers\nWe can use + to match 1 or more of a pattern:\n\nCode# Matches all instances of `s` followed by 1 or more non-word character\nstr_view_all(string = p12_df$text[119], pattern = \"s\\\\W+\")\n\n[1] │ \"I stand with my colleague<s >at @UW and America'<s >leading research universitie<s >a<s >they take fight to Covid-19 in our lab<s >and hospital<s.\"\n    │ \n    │ #>ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskie<s >http<s://>t.co/4YSf4SpPe0\n\nCode# Matche all twitter hashtags\n  # hashtag defined as hashtag character # followed by 1 or more word characters\nstr_view_all(string = p12_df$text[119], pattern = \"#\\\\w+\")\n\n[1] │ \"I stand with my colleagues at @UW and America's leading research universities as they take fight to Covid-19 in our labs and hospitals.\"\n    │ \n    │ <#ProudToBeOnTheirTeam> x <#AlwaysCompete> x <#GoHuskies> https://t.co/4YSf4SpPe0\n\n\nUsing {...} to specify how many occurrences to match\nWe can use {n} to specify the exact number of characters or expressions to match:\n\nCode# Matches words with exactly 3 letters\nstr_view_all(string = p12_df$text[119], pattern = \"\\\\s\\\\w{3}\\\\s\")\n\n[1] │ \"I stand with my colleagues at @UW< and >America's leading research universities as they take fight to Covid-19 in< our >labs< and >hospitals.\"\n    │ \n    │ #ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/4YSf4SpPe0\n\n\nUsing {...} to specify how many occurrences to match\nWe can use {n,} to specify n as the minimum amount to match:\n\nCode# Matches words with 3 or more letters\nstr_view_all(string = p12_df$text[119], pattern = \"\\\\s\\\\w{3,}\\\\s\")\n\n[1] │ \"I< stand >with my< colleagues >at @UW< and >America's< leading >research< universities >as< they >take< fight >to Covid-19 in< our >labs< and >hospitals.\"\n    │ \n    │ #ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/4YSf4SpPe0\n\n\nUsing {...} to specify how many occurrences to match\nWe can use {n,m} to specify we want to match between n and m amount (inclusive):\n\nCode# Matches words with between 3 to 5 letters (inclusive)\nstr_view_all(string = p12_df$text[119], pattern = \"\\\\s\\\\w{3,5}\\\\s\")\n\n[1] │ \"I< stand >with my colleagues at @UW< and >America's leading research universities as< they >take< fight >to Covid-19 in< our >labs< and >hospitals.\"\n    │ \n    │ #ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/4YSf4SpPe0"
  },
  {
    "objectID": "07-week7-workbook.html#anchors",
    "href": "07-week7-workbook.html#anchors",
    "title": "Week 7 Workbook",
    "section": "Anchors",
    "text": "Anchors\n\nWe can use anchors to indicate which part of the string to match.\nFor example, ^ matches the start of the string, $ matches the end of the string (Notice how we do not need to escape these characters).\n\n\\b can be used to help detect word boundaries, and \\B can be used to help match characters within a word.\n\n\n\n\n\n\n\n\nString\nCharacter\nDescription\n\n\n\n\"^\"\n^\nStart of string, or start of line in multi-line pattern\n\n\n\"$\"\n$\nEnd of string, or end of line in multi-line pattern\n\n\n\"\\\\b\"\n\\b\nWord boundary\n\n\n\"\\\\B\"\n\\B\nNon-word boundary\n\n\n\nUsing ^ & $ to match start & end of string\nWe can use ^ to match the start of a string:\n\nCode# Matches only the quotation mark at the start of the text and not the end quote\nstr_view_all(string = p12_df$text[119], pattern = '^\"')\n\n[1] │ <\">I stand with my colleagues at @UW and America's leading research universities as they take fight to Covid-19 in our labs and hospitals.\"\n    │ \n    │ #ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/4YSf4SpPe0\n\n\nUsing ^ & $ to match start & end of string\nWe can use $ to match the end of a string:\n\nCode# Matches only the number at the end of the text and not any other numbers\nstr_view_all(string = p12_df$text[119], pattern = \"\\\\d$\")\n\n[1] │ \"I stand with my colleagues at @UW and America's leading research universities as they take fight to Covid-19 in our labs and hospitals.\"\n    │ \n    │ #ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/4YSf4SpPe<0>\n\n\nUsing \\b & \\B to match word boundary & non-word boundary\nWe can use \\b to help detect word boundary:\n\nCode# Match to all word bounraries\nstr_view_all(string = p12_df$text[119], pattern = \"\\\\b\")\n\n[1] │ \"<>I<> <>stand<> <>with<> <>my<> <>colleagues<> <>at<> @<>UW<> <>and<> <>America<>'<>s<> <>leading<> <>research<> <>universities<> <>as<> <>they<> <>take<> <>fight<> <>to<> <>Covid<>-<>19<> <>in<> <>our<> <>labs<> <>and<> <>hospitals<>.\"\n    │ \n    │ #<>ProudToBeOnTheirTeam<> <>x<> #<>AlwaysCompete<> <>x<> #<>GoHuskies<> <>https<>://<>t<>.<>co<>/<>4YSf4SpPe0<>\n\n\n\nCode# Matches words with 3 or more letters using \\b\nstr_view_all(string = p12_df$text[119], pattern = \"\\\\b\\\\w{3,}\\\\b\")\n\n[1] │ \"I <stand> <with> my <colleagues> at @UW <and> <America>'s <leading> <research> <universities> as <they> <take> <fight> to <Covid>-19 in <our> <labs> <and> <hospitals>.\"\n    │ \n    │ #<ProudToBeOnTheirTeam> x #<AlwaysCompete> x #<GoHuskies> <https>://t.co/<4YSf4SpPe0>\n\n\nNotice how this is much flexible than trying to use whitespace (\\s) to determine word boundary:\n\nCode# Matches words with 3 or more letters using \\s\nstr_view_all(string = p12_df$text[119], pattern = \"\\\\s\\\\w{3,}\\\\s\")\n\n[1] │ \"I< stand >with my< colleagues >at @UW< and >America's< leading >research< universities >as< they >take< fight >to Covid-19 in< our >labs< and >hospitals.\"\n    │ \n    │ #ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/4YSf4SpPe0\n\n\nRegular expression \\B matches to “non-word boundary”; what does that mean?\n\nCodestr_view_all(string = p12_df$text[119], pattern = \"\\\\B\")\n\n[1] │ <>\"I s<>t<>a<>n<>d w<>i<>t<>h m<>y c<>o<>l<>l<>e<>a<>g<>u<>e<>s a<>t <>@U<>W a<>n<>d A<>m<>e<>r<>i<>c<>a's l<>e<>a<>d<>i<>n<>g r<>e<>s<>e<>a<>r<>c<>h u<>n<>i<>v<>e<>r<>s<>i<>t<>i<>e<>s a<>s t<>h<>e<>y t<>a<>k<>e f<>i<>g<>h<>t t<>o C<>o<>v<>i<>d-1<>9 i<>n o<>u<>r l<>a<>b<>s a<>n<>d h<>o<>s<>p<>i<>t<>a<>l<>s.<>\"<>\n    │ <>\n    │ <>#P<>r<>o<>u<>d<>T<>o<>B<>e<>O<>n<>T<>h<>e<>i<>r<>T<>e<>a<>m x <>#A<>l<>w<>a<>y<>s<>C<>o<>m<>p<>e<>t<>e x <>#G<>o<>H<>u<>s<>k<>i<>e<>s h<>t<>t<>p<>s:<>/<>/t.c<>o/4<>Y<>S<>f<>4<>S<>p<>P<>e<>0\n\n\nWe can use \\B to help match characters within a word:\n\nCode# Matches only the letter `s` within a word and not at the start or end\nstr_view_all(string = p12_df$text[119], pattern = \"\\\\Bs\\\\B\")\n\n[1] │ \"I stand with my colleagues at @UW and America's leading re<s>earch univer<s>ities as they take fight to Covid-19 in our labs and ho<s>pitals.\"\n    │ \n    │ #ProudToBeOnTheirTeam x #Alway<s>Compete x #GoHu<s>kies https://t.co/4YSf4SpPe0"
  },
  {
    "objectID": "07-week7-workbook.html#sets-and-ranges",
    "href": "07-week7-workbook.html#sets-and-ranges",
    "title": "Week 7 Workbook",
    "section": "Sets and ranges",
    "text": "Sets and ranges\n\n\nCharacter\nDescription\n\n\n\n.\nMatch any character except newline (\\n)\n\n\na|b\nMatch a or b\n\n\n\n[abc]\nMatch either a, b, or c\n\n\n\n[^abc]\nMatch anything except a, b, or c\n\n\n\n[a-z]\nMatch range of lowercase letters from a to z\n\n\n\n[A-Z]\nMatch range of uppercase letters from A to Z\n\n\n\n[0-9]\nMatch range of numbers from 0 to 9\n\n\n\n\n\nThe table lists some more ways regular expression offers us flexibility and option in what we want to match.\nThe period . acts as a wildcard to match any character except newline.\nThe vertical bar | is similar to an OR operator. Square brackets [...] can be used to specify a set or range of characters to match (or not to match).\n\nUsing . as a wildcard\nWe can use . to match any character except newline (\\n):\n\nCode# Matches any character except newline\nstr_view_all(string = p12_df$text[119], pattern = \".\")\n\n[1] │ <\"><I>< ><s><t><a><n><d>< ><w><i><t><h>< ><m><y>< ><c><o><l><l><e><a><g><u><e><s>< ><a><t>< ><@><U><W>< ><a><n><d>< ><A><m><e><r><i><c><a><'><s>< ><l><e><a><d><i><n><g>< ><r><e><s><e><a><r><c><h>< ><u><n><i><v><e><r><s><i><t><i><e><s>< ><a><s>< ><t><h><e><y>< ><t><a><k><e>< ><f><i><g><h><t>< ><t><o>< ><C><o><v><i><d><-><1><9>< ><i><n>< ><o><u><r>< ><l><a><b><s>< ><a><n><d>< ><h><o><s><p><i><t><a><l><s><.><\">\n    │ \n    │ <#><P><r><o><u><d><T><o><B><e><O><n><T><h><e><i><r><T><e><a><m>< ><x>< ><#><A><l><w><a><y><s><C><o><m><p><e><t><e>< ><x>< ><#><G><o><H><u><s><k><i><e><s>< ><h><t><t><p><s><:></></><t><.><c><o></><4><Y><S><f><4><S><p><P><e><0>\n\n\nWe can confirm there is a newline in the tweet above by using writeLines() or print():\n\nCodewriteLines(p12_df$text[119])\n\n\"I stand with my colleagues at @UW and America's leading research universities as they take fight to Covid-19 in our labs and hospitals.\"\n\n#ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/4YSf4SpPe0\n\nCodeprint(p12_df$text[119])\n\n[1] \"\\\"I stand with my colleagues at @UW and America's leading research universities as they take fight to Covid-19 in our labs and hospitals.\\\"\\n\\n#ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/4YSf4SpPe0\"\n\n\nUsing | as an OR operator\nWe can use | to match either one of multiple patterns:\n\nCode# Matches `research`, `fight`, or `labs`\nstr_view_all(string = p12_df$text[119], pattern = \"research|fight|labs\")\n\n[1] │ \"I stand with my colleagues at @UW and America's leading <research> universities as they take <fight> to Covid-19 in our <labs> and hospitals.\"\n    │ \n    │ #ProudToBeOnTheirTeam x #AlwaysCompete x #GoHuskies https://t.co/4YSf4SpPe0\n\nCode# Matches hashtags or handles\nstr_view_all(string = p12_df$text[119], pattern = \"@\\\\w+|#\\\\w+\")\n\n[1] │ \"I stand with my colleagues at <@UW> and America's leading research universities as they take fight to Covid-19 in our labs and hospitals.\"\n    │ \n    │ <#ProudToBeOnTheirTeam> x <#AlwaysCompete> x <#GoHuskies> https://t.co/4YSf4SpPe0"
  },
  {
    "objectID": "07-week7-workbook.html#using-...-to-match-or-not-match-a-set-or-range-of-characters",
    "href": "07-week7-workbook.html#using-...-to-match-or-not-match-a-set-or-range-of-characters",
    "title": "Week 7 Workbook",
    "section": "#Using [...] to match (or not match) a set or range of characters",
    "text": "#Using [...] to match (or not match) a set or range of characters\nWe can use [...] to match any set of characters:\n\nCode# Matches hashtags or handles\nstr_view_all(string = p12_df$text[119], pattern = \"[@#]\\\\w+\")\n\n[1] │ \"I stand with my colleagues at <@UW> and America's leading research universities as they take fight to Covid-19 in our labs and hospitals.\"\n    │ \n    │ <#ProudToBeOnTheirTeam> x <#AlwaysCompete> x <#GoHuskies> https://t.co/4YSf4SpPe0\n\nCode# Matches any 2 consecutive vowels\nstr_view_all(string = p12_df$text[119], pattern = \"[aeiouAEIOU]{2}\")\n\n[1] │ \"I stand with my coll<ea>g<ue>s at @UW and America's l<ea>ding res<ea>rch universit<ie>s as they take fight to Covid-19 in <ou>r labs and hospitals.\"\n    │ \n    │ #Pr<ou>dToB<eO>nTh<ei>rT<ea>m x #AlwaysCompete x #GoHusk<ie>s https://t.co/4YSf4SpPe0\n\n\nWe can also use [...] to match any range of alpha or numeric characters:\n\nCode# Matches only lowercase x through z or uppercase A through C\nstr_view_all(string = p12_df$text[119], pattern = \"[x-zA-C]\")\n\n[1] │ \"I stand with m<y> colleagues at @UW and <A>merica's leading research universities as the<y> take fight to <C>ovid-19 in our labs and hospitals.\"\n    │ \n    │ #ProudTo<B>eOnTheirTeam <x> #<A>lwa<y>s<C>ompete <x> #GoHuskies https://t.co/4YSf4SpPe0\n\nCode# Matches only numbers 1 through 4 or the pound sign\nstr_view_all(string = p12_df$text[119], pattern = \"[1-4#]\")\n\n[1] │ \"I stand with my colleagues at @UW and America's leading research universities as they take fight to Covid-<1>9 in our labs and hospitals.\"\n    │ \n    │ <#>ProudToBeOnTheirTeam x <#>AlwaysCompete x <#>GoHuskies https://t.co/<4>YSf<4>SpPe0\n\n\nWe can use [^...] to indicate we do not want to match the provided set or range of characters:\n\nCode# Matches any vowels\nstr_view_all(string = p12_df$text[119], pattern = \"[aeiouAEIOU]\")\n\n[1] │ \"<I> st<a>nd w<i>th my c<o>ll<e><a>g<u><e>s <a>t @<U>W <a>nd <A>m<e>r<i>c<a>'s l<e><a>d<i>ng r<e>s<e><a>rch <u>n<i>v<e>rs<i>t<i><e>s <a>s th<e>y t<a>k<e> f<i>ght t<o> C<o>v<i>d-19 <i>n <o><u>r l<a>bs <a>nd h<o>sp<i>t<a>ls.\"\n    │ \n    │ #Pr<o><u>dT<o>B<e><O>nTh<e><i>rT<e><a>m x #<A>lw<a>ysC<o>mp<e>t<e> x #G<o>H<u>sk<i><e>s https://t.c<o>/4YSf4SpP<e>0\n\nCode# Matches anything except vowels\nstr_view_all(string = p12_df$text[119], pattern = \"[^aeiouAEIOU]\")\n\n[1] │ <\">I< ><s><t>a<n><d>< ><w>i<t><h>< ><m><y>< ><c>o<l><l>ea<g>ue<s>< >a<t>< ><@>U<W>< >a<n><d>< >A<m>e<r>i<c>a<'><s>< ><l>ea<d>i<n><g>< ><r>e<s>ea<r><c><h>< >u<n>i<v>e<r><s>i<t>ie<s>< >a<s>< ><t><h>e<y>< ><t>a<k>e< ><f>i<g><h><t>< ><t>o< ><C>o<v>i<d><-><1><9>< >i<n>< >ou<r>< ><l>a<b><s>< >a<n><d>< ><h>o<s><p>i<t>a<l><s><.><\"><\n    │ ><\n    │ ><#><P><r>ou<d><T>o<B>eO<n><T><h>ei<r><T>ea<m>< ><x>< ><#>A<l><w>a<y><s><C>o<m><p>e<t>e< ><x>< ><#><G>o<H>u<s><k>ie<s>< ><h><t><t><p><s><:></></><t><.><c>o</><4><Y><S><f><4><S><p><P>e<0>\n\nCode# Matches anything that's not uppercase letters\nstr_view_all(string = p12_df$text[119], pattern = \"[^A-Z]+\")\n\n[1] │ <\">I< stand with my colleagues at @>UW< and >A<merica's leading research universities as they take fight to >C<ovid-19 in our labs and hospitals.\"\n    │ \n    │ #>P<roud>T<o>B<e>O<n>T<heir>T<eam x #>A<lways>C<ompete x #>G<o>H<uskies https://t.co/4>YS<f4>S<p>P<e0>\n\n\nNotice that [...] only matches a single character (see second to last example above). We need to use quantifiers if we want to match a stretch of characters (see last example above)."
  },
  {
    "objectID": "07-week7-workbook.html#creating-datetime-objects",
    "href": "07-week7-workbook.html#creating-datetime-objects",
    "title": "Week 7 Workbook",
    "section": "Creating date/time objects",
    "text": "Creating date/time objects\nFunctions that create date/time objects by parsing character or numeric input:\n\nCreate Date object: ymd(), ydm(), mdy(), myd(), dmy(), dym()\n\n\ny stands for year, m stands for month, d stands for day\nSelect the function that represents the order in which your date input is formatted, and the function will be able to parse your input and create a Date object\n\n\n\nCreating POSIXct objects\n\nCreate POSIXct object: ymd_h(), ymd_hm(), ymd_hms(), etc.\n\n\nh stands for hour, m stands for minute, s stands for second\nFor any of the previous 6 date functions, you can append h, hm, or hms if you want to provide additional time information in order to create a POSIXct object\nTo force a POSIXct object without providing any time information, you can just provide a timezone (using tz) to one of the date functions and it will assume midnight as the time\nYou can use Sys.timezone() to get the timezone for your location\n\n\nCreating Date object from character or numeric input\nThe lubridate functions are flexible and can parse dates in various formats:\n\nCoded <- mdy(\"1/1/2020\"); d\n\n[1] \"2020-01-01\"\n\nCoded <- mdy(\"1-1-2020\"); d\n\n[1] \"2020-01-01\"\n\nCoded <- mdy(\"Jan. 1, 2020\"); d\n\n[1] \"2020-01-01\"\n\nCoded <- ymd(20200101); d\n\n[1] \"2020-01-01\"\n\n\nCreating Date object from character or numeric input\nInvestigate the Date object:\n\nCodeclass(d)\n\n[1] \"Date\"\n\nCodetypeof(d)\n\n[1] \"double\"\n\nCode# Number of days since January 1, 1970\nas.numeric(d)\n\n[1] 18262\n\n\nCreating POSIXct object from character or numeric input\nThe lubridate functions are flexible and can parse AM/PM in various formats:\n\nCodedt <- mdy_h(\"12/31/2019 11pm\"); dt\n\n[1] \"2019-12-31 23:00:00 UTC\"\n\nCodedt <- mdy_hm(\"12/31/2019 11:59 pm\"); dt\n\n[1] \"2019-12-31 23:59:00 UTC\"\n\nCodedt <- mdy_hms(\"12/31/2019 11:59:59 PM\"); dt\n\n[1] \"2019-12-31 23:59:59 UTC\"\n\nCodedt <- ymd_hms(20191231235959); dt\n\n[1] \"2019-12-31 23:59:59 UTC\"\n\n\nInvestigate the POSIXct object:\n\nCodeclass(dt)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nCodetypeof(dt)\n\n[1] \"double\"\n\nCode# Number of seconds since January 1, 1970\nas.numeric(dt)\n\n[1] 1577836799\n\n\nWe can also create a POSIXct object from a date function by providing a timezone. The time would default to midnight:\n\nCodedt <- mdy(\"1/1/2020\", tz = \"UTC\")\ndt\n\n[1] \"2020-01-01 UTC\"\n\nCode# Number of seconds since January 1, 1970\nas.numeric(dt)  # Note that this is indeed 1 sec after the previous example\n\n[1] 1577836800\n\n\nCreating Date objects from dataframe column\nUsing the p12_datetime_df we created earlier, we can create Date objects from the date_chr column:\n\nCode# Use `ymd()` to parse the string stored in the `date_chr` column\np12_datetime_df %>% select(created_at, dt_chr, date_chr) %>%\n  mutate(date_ymd = ymd(date_chr))\n\n\n\n  \n\n\n\nCreating POSIXct objects from dataframe column\nUsing the p12_datetime_df we created earlier, we can recreate the created_at column (class POSIXct) from the dt_chr column (class character):\n\nCode# Use `ymd_hms()` to parse the string stored in the `dt_chr` column\np12_datetime_df %>% select(created_at, dt_chr) %>%\n  mutate(datetime_ymd_hms = ymd_hms(dt_chr))\n\n\n\n  \n\n\n\nCreating date/time objects from individual components\nFunctions that create date/time objects from various date/time components:\n\nCreate Date object: make_date()\n\nSyntax and default values: make_date(year = 1970L, month = 1L, day = 1L)\n\nAll inputs are coerced to integer\n\n\nCreate POSIXct object: make_datetime()\n\nSyntax and default values: make_datetime(year = 1970L, month = 1L, day = 1L, hour = 0L, min = 0L, sec = 0, tz = \"UTC\")\n\n\n\n\nThere are various ways to pass in the inputs to create the same Date object:\n\nCoded <- make_date(2020, 1, 1); d\n\n[1] \"2020-01-01\"\n\nCode# Characters can be coerced to integers\nd <- make_date(\"2020\", \"01\", \"01\"); d\n\n[1] \"2020-01-01\"\n\nCode# Remember that the default values for month and day would be 1L\nd <- make_date(2020); d\n\n[1] \"2020-01-01\"\n\n\nCreating POSIXct object from individual components\n\nCode# Inputs should be numeric\nd <- make_datetime(2019, 12, 31, 23, 59, 59)\nd\n\n[1] \"2019-12-31 23:59:59 UTC\"\n\n\nCreating Date objects from dataframe columns\nUsing the p12_datetime_df we created earlier, we can create Date objects from the various date component columns:\n\nCode# Use `make_date()` to create a `Date` object from the `yr_chr`, `mth_chr`, `day_chr` fields\np12_datetime_df %>% select(created_at, dt_chr, yr_chr, mth_chr, day_chr) %>%\n  mutate(date_make_date = make_date(year = yr_chr, month = mth_chr, day = day_chr))\n\n\n\n  \n\n\n\nCreating POSIXct objects from dataframe columns\nUsing the p12_datetime_df we created earlier, we can recreate the created_at column (class POSIXct) from the various date and time component columns (class character):\n\nCode# Use `make_datetime()` to create a `POSIXct` object from the `yr_chr`, `mth_chr`, `day_chr`, `hr_chr`, `min_chr`, `sec_chr` fields\n# Convert inputs to integers first\np12_datetime_df %>%\n  mutate(datetime_make_datetime = make_datetime(\n    as.integer(yr_chr), as.integer(mth_chr), as.integer(day_chr), \n    as.integer(hr_chr), as.integer(min_chr), as.integer(sec_chr)\n  )) %>%\n  select(datetime_make_datetime, yr_chr, mth_chr, day_chr, hr_chr, min_chr, sec_chr)"
  },
  {
    "objectID": "07-week7-workbook.html#datetime-object-components",
    "href": "07-week7-workbook.html#datetime-object-components",
    "title": "Week 7 Workbook",
    "section": "Date/time object components",
    "text": "Date/time object components\nStoring data using date/time objects makes it easier to get and set the various date/time components.\nBasic accessor functions:\n\ndate(): Date component\nyear(): Year\nmonth(): Month\nday(): Day\nhour(): Hour\nminute(): Minute\nsecond(): Second\nweek(): Week of the year\nwday(): Day of the week (1 for Sunday to 7 for Saturday)\nam(): Is it in the am? (returns TRUE or FALSE)\npm(): Is it in the pm? (returns TRUE or FALSE)\n\nTo get a date/time component, you can simply pass a date/time object to the function\n\nSyntax: accessor_function(<date/time_object>)\n\n\n\n\nTo set a date/time component, you can assign into the accessor function to change the component\n\nSyntax: accessor_function(<date/time_object>) <- \"new_component\"\n\nNote that am() and pm() can’t be set. Modify the time components instead.\n\n\n\n\nCode# Create datetime for New Year's Eve\ndt <- make_datetime(2019, 12, 31, 23, 59, 59)\ndt\n\n[1] \"2019-12-31 23:59:59 UTC\"\n\nCodedt %>% class()\n\n[1] \"POSIXct\" \"POSIXt\" \n\nCodedate(dt) # Get date\n\n[1] \"2019-12-31\"\n\nCodehour(dt) # Get hour\n\n[1] 23\n\nCodepm(dt)   # Is it pm?\n\n[1] TRUE\n\nCodewday(dt) # Day of the week (3 = Tuesday)\n\n[1] 3\n\nCodeyear(dt) # Get year\n\n[1] 2019"
  },
  {
    "objectID": "07-week7-workbook.html#setting-datetime-components",
    "href": "07-week7-workbook.html#setting-datetime-components",
    "title": "Week 7 Workbook",
    "section": "Setting date/time components",
    "text": "Setting date/time components\n\nCodeweek(dt) # Get week of year\n\n[1] 53\n\nCode# Set week of year (move back 1 week)\nweek(dt) <- week(dt) - 1\ndt\n\n[1] \"2019-12-24 23:59:59 UTC\"\n\nCodeday(dt) <- 25 # Set day to Christmas Day\ndt\n\n[1] \"2019-12-25 23:59:59 UTC\""
  },
  {
    "objectID": "07-week7-workbook.html#getting-datetime-components-from-dataframe-column",
    "href": "07-week7-workbook.html#getting-datetime-components-from-dataframe-column",
    "title": "Week 7 Workbook",
    "section": "Getting date/time components from dataframe column",
    "text": "Getting date/time components from dataframe column\nUsing the p12_datetime_df we created earlier, we can isolate the various date/time components from the POSIXct object in the created_at column:\n\nCode# The extracted date/time components will be of numeric type\np12_datetime_df %>% select(created_at) %>%\n  mutate(\n    yr_num = year(created_at),\n    mth_num = month(created_at),\n    day_num = day(created_at),\n    hr_num = hour(created_at),\n    min_num = minute(created_at),\n    sec_num = second(created_at),\n    ampm = ifelse(am(created_at), 'AM', 'PM')  # am()/pm() returns TRUE/FALSE\n  )"
  },
  {
    "objectID": "07-week7-workbook.html#time-spans",
    "href": "07-week7-workbook.html#time-spans",
    "title": "Week 7 Workbook",
    "section": "Time spans",
    "text": "Time spans\n3 ways to represent time spans (From lubridate cheatsheet)\n\n\nIntervals represent specific intervals of the timeline, bounded by start and end date-times\n\nExample: People with birthdays between the interval October 23 to November 22 are Scorpios\n\n\n\nPeriods track changes in clock times, which ignore time line irregularities\n\nExample: Daylight savings time ends at the beginning of November and we gain an hour - this extra hour is ignored when determining the period between October 23 to November 22\n\n\n\nDurations track the passage of physical time, which deviates from clock time when irregularities occur\n\nExample: Daylight savings time ends at the beginning of November and we gain an hour - this extra hour is added when determining the duration between October 23 to November 22\n\n\n\nUsing the lubridate package for time spans:\n\n\nInterval\n\nCreate an interval using interval() or %--%\n\nSyntax: interval(<date/time_object1>, <date/time_object2>) or <date/time_object1> %--% <date/time_object2>\n\n\n\n\n\n\nTime spans using lubridate: Periods\n\n“Periods are time spans but don’t have a fixed length in seconds, instead they work with ‘human’ times, like days and months.” (From R for Data Science)\nCreate periods using functions whose name is the time unit pluralized (e.g., years(), months(), weeks(), days(), hours(), minutes(), seconds())\nYou can add and subtract periods\nYou can also use as.period() to get period of an interval\n\n\nCodedays(1)\n\n[1] \"1d 0H 0M 0S\"\n\n\nTime spans using lubridate: Durations\n\nDurations keep track of the physical amount of time elapsed, so it is “stored as seconds, the only time unit with a consistent length” (From lubridate cheatsheet)\nCreate durations using functions whose name is the time unit prefixed with a d (e.g., dyears(), dweeks(), ddays(), dhours(), dminutes(), dseconds())\nExample: ddays(1) creates a duration of 86400s, using the standard conversion of 60 seconds in an minute, 60 minutes in an hour, and 24 hours in a day:\n\n\nCodeddays(1)\n\n[1] \"86400s (~1 days)\"\n\n\nNotice that the output says this is equivalent to approximately 1 day, since it acknowledges that not all days have 24 hours.\nIn the case of daylight savings, one particular day may have 25 hours, so the duration of that day should be represented as:\n\nCodeddays(1) + dhours(1)\n\n[1] \"90000s (~1.04 days)\"\n\n\n\nYou can add and subract durations\nYou can also use as.duration() to get duration of an interval"
  },
  {
    "objectID": "07-week7-workbook.html#working-with-interval",
    "href": "07-week7-workbook.html#working-with-interval",
    "title": "Week 7 Workbook",
    "section": "Working with interval",
    "text": "Working with interval\n\nCode# Use `Sys.timezone()` to get timezone for your location (time is midnight by default)\nscorpio_start <- ymd(\"2019-10-23\", tz = Sys.timezone())\nscorpio_end <- ymd(\"2019-11-22\", tz = Sys.timezone())\n\nscorpio_start\n\n[1] \"2019-10-23 PDT\"\n\nCode# These datetime objects have class `POSIXct`\nclass(scorpio_start)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nCode# Create interval for the datetimes\nscorpio_interval <- scorpio_start %--% scorpio_end  # or `interval(scorpio_start, scorpio_end)`\nscorpio_interval <- interval(scorpio_start, scorpio_end)\nscorpio_interval\n\n[1] 2019-10-23 PDT--2019-11-22 PST\n\nCode# The object has class `Interval`\nclass(scorpio_interval)\n\n[1] \"Interval\"\nattr(,\"package\")\n[1] \"lubridate\"\n\nCodeas.numeric(scorpio_interval)\n\n[1] 2595600"
  },
  {
    "objectID": "07-week7-workbook.html#working-with-period",
    "href": "07-week7-workbook.html#working-with-period",
    "title": "Week 7 Workbook",
    "section": "Working with period",
    "text": "Working with period\nIf we use as.period() to get the period of scorpio_interval, we see that it is a period of 30 days. We do not worry about the extra 1 hour gained due to daylight savings ending:\n\nCode# Period is 30 days\nscorpio_period <- as.period(scorpio_interval)\nscorpio_period\n\n[1] \"30d 0H 0M 0S\"\n\nCode# The object has class `Period`\nclass(scorpio_period)\n\n[1] \"Period\"\nattr(,\"package\")\n[1] \"lubridate\"\n\n\nBecause periods work with “human” times like days, it is more intuitive. For example, if we add a period of 30 days to the scorpio_start datetime object, we get the expected end datetime that is 30 days later:\n\nCode# Start datetime for Scorpio birthdays (time is midnight)\nscorpio_start\n\n[1] \"2019-10-23 PDT\"\n\nCode# After adding 30 day period, we get the expected end datetime (time is midnight)\nscorpio_start + days(30)\n\n[1] \"2019-11-22 PST\""
  },
  {
    "objectID": "07-week7-workbook.html#working-with-duration",
    "href": "07-week7-workbook.html#working-with-duration",
    "title": "Week 7 Workbook",
    "section": "Working with duration",
    "text": "Working with duration\nIf we use as.duration() to get the duration of scorpio_interval, we see that it is a duration of 2595600 seconds. It takes into account the extra 1 hour gained due to daylight savings ending:\n\nCode# Duration is 2595600 seconds, which is equivalent to 30 24-hr days + 1 additional hour\nscorpio_duration <- as.duration(scorpio_interval)\nscorpio_duration\n\n[1] \"2595600s (~4.29 weeks)\"\n\nCode# The object has class `Duration`\nclass(scorpio_duration)\n\n[1] \"Duration\"\nattr(,\"package\")\n[1] \"lubridate\"\n\nCode# Using the standard 60s/min, 60min/hr, 24hr/day conversion,\n# confirm duration is slightly more than 30 \"standard\" (ie. 24-hr) days\n2595600 / (60 * 60 * 24)\n\n[1] 30.04167\n\nCode# Specifically, it is 30 days + 1 hour, if we define a day to have 24 hours\nseconds_to_period(scorpio_duration)\n\n[1] \"30d 1H 0M 0S\"\n\n\nBecause durations work with physical time, when we add a duration of 30 days to the scorpio_start datetime object, we do not get the end datetime we’d expect:\n\nCode# Start datetime for Scorpio birthdays (time is midnight)\nscorpio_start\n\n[1] \"2019-10-23 PDT\"\n\nCode# After adding 30 day duration, we do not get the expected end datetime\n# `ddays(30)` adds the number of seconds in 30 standard 24-hr days, but one of the days has 25 hours\nscorpio_start + ddays(30)\n\n[1] \"2019-11-21 23:00:00 PST\"\n\nCode# We need to add the additional 1 hour of physical time that elapsed during this time span\nscorpio_start + ddays(30) + dhours(1)\n\n[1] \"2019-11-22 PST\""
  },
  {
    "objectID": "06-week6-slides.html",
    "href": "06-week6-slides.html",
    "title": "Week 6 Slides",
    "section": "",
    "text": "Welcome to Week 4! This week, we’ll talk about data documentation, readable codebooks, and data cleaning workflows."
  },
  {
    "objectID": "ps4-week4.html",
    "href": "ps4-week4.html",
    "title": "Problem Set Week 4",
    "section": "",
    "text": "Due Date: Monday, October 30, 12:01 AM PST.\nDownload your problem set for week 4 below or on Canvas.\nAnswers will be posted after the due date."
  },
  {
    "objectID": "03-week3-workbook.html",
    "href": "03-week3-workbook.html",
    "title": "Week 3 Workbook",
    "section": "",
    "text": "Codelibrary(knitr)\nlibrary(psych)\nlibrary(plyr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()     masks psych::%+%()\n✖ ggplot2::alpha()   masks psych::alpha()\n✖ dplyr::arrange()   masks plyr::arrange()\n✖ purrr::compact()   masks plyr::compact()\n✖ dplyr::count()     masks plyr::count()\n✖ dplyr::desc()      masks plyr::desc()\n✖ dplyr::failwith()  masks plyr::failwith()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::id()        masks plyr::id()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::mutate()    masks plyr::mutate()\n✖ dplyr::rename()    masks plyr::rename()\n✖ dplyr::summarise() masks plyr::summarise()\n✖ dplyr::summarize() masks plyr::summarize()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
  },
  {
    "objectID": "03-week3-workbook.html#outline",
    "href": "03-week3-workbook.html#outline",
    "title": "Week 3 Workbook",
    "section": "Outline",
    "text": "Outline\n\nWelcome & Q’s on homework\nPart 1: Data Quality and Descriptives\nPart 2: tidyr\n\nProblem set & Q time"
  },
  {
    "objectID": "03-week3-workbook.html#what-is-data-quality",
    "href": "03-week3-workbook.html#what-is-data-quality",
    "title": "Week 3 Workbook",
    "section": "What is data quality",
    "text": "What is data quality\nIBM’s definition of data quality:\n\n“Data quality measures how well a dataset meets criteria for accuracy, completeness, validity, consistency, uniqueness, timeliness, and fitness for purpose”"
  },
  {
    "objectID": "03-week3-workbook.html#aspects-of-data-quality",
    "href": "03-week3-workbook.html#aspects-of-data-quality",
    "title": "Week 3 Workbook",
    "section": "Aspects of data quality",
    "text": "Aspects of data quality\n\n\nAccuracy: Do the data reflect reality / truth?\n\nCompleteness: Are the data usable or complete (no missing people, values, etc. beyond random)\n\n\nUniqueness: There is no duplicated data\n\nValidity: Do the data have the correct properties (values, ranges, etc.)\n\nConsistency: When integrating across multiple data sources, information should converge across sources and match reality\n\nTimeliness: Can the data be maintained and distributed within a specified time frame\n\nFitness for purpose: Do the data meet your research need?"
  },
  {
    "objectID": "03-week3-workbook.html#why-should-we-care-about-data-quality",
    "href": "03-week3-workbook.html#why-should-we-care-about-data-quality",
    "title": "Week 3 Workbook",
    "section": "Why should we care about data quality",
    "text": "Why should we care about data quality\n\nYou aren’t responsible for poor quality data you receive, but you are responsible for the data products you work with – that is, you are responsible for improving data quality\nPoor quality data threatens scientific integrity\n\nPoor quality data are a pain for you to work with and for others to work with"
  },
  {
    "objectID": "03-week3-workbook.html#what-can-data-quality-do-for-my-career",
    "href": "03-week3-workbook.html#what-can-data-quality-do-for-my-career",
    "title": "Week 3 Workbook",
    "section": "What can data quality do for my career?",
    "text": "What can data quality do for my career?\n\nThe virtuous cycle of data cleaning\n\nSome people get a reputation for getting their data, analyses, etc. right\nThis is important for publications, grant funding, etc.\nIt tends to be inter-generational – you inherit some of your reputation on this from your advisor\nStart paying it forward now to build your own career, whether it’s in academia or industry"
  },
  {
    "objectID": "03-week3-workbook.html#how-do-i-ensure-data-quality",
    "href": "03-week3-workbook.html#how-do-i-ensure-data-quality",
    "title": "Week 3 Workbook",
    "section": "How do I ensure data quality?",
    "text": "How do I ensure data quality?\nThe Towards Data Science website has a nice definition of EDA:\n\n“Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics”\n\n\nSo EDA is basically a fancy word for the descriptive statistics you’ve been learning about for years\n\nI think about “exploratory data analysis for data quality”\n\nInvestigating values and patterns of variables from “input data”\nIdentifying and cleaning errors or values that need to be changed\nCreating analysis variables\nChecking values of analysis variables against values of input variables"
  },
  {
    "objectID": "03-week3-workbook.html#how-i-will-teach-exploratory-data-analysis",
    "href": "03-week3-workbook.html#how-i-will-teach-exploratory-data-analysis",
    "title": "Week 3 Workbook",
    "section": "How I will teach exploratory data analysis",
    "text": "How I will teach exploratory data analysis\nWill teach exploratory data analysis (EDA) in two sub-sections:\n\nProvide “Guidelines for EDA”\n\nLess about coding, more about practices you should follow and mentality necessary to ensure high data quality\n\n\nIntroduce “Tools of EDA”:\n\nDemonstrate code to investigate variables and relationship between variables\nMost of these tools are just the application of programming skills you have already learned (or will learn soon!)"
  },
  {
    "objectID": "03-week3-workbook.html#guidelines-for-eda-for-data-quality",
    "href": "03-week3-workbook.html#guidelines-for-eda-for-data-quality",
    "title": "Week 3 Workbook",
    "section": "Guidelines for “EDA for data quality”",
    "text": "Guidelines for “EDA for data quality”\nAssume that your goal in “EDA for data quality” is to investigate “input” data sources and create “analysis variables”\n\nUsually, your analysis dataset will incorporate multiple sources of input data, including data you collect (primary data) and/or data collected by others (secondary data)\n\nEDA is not a linear process, and the process will vary across people and projects Some broad steps:\n\nUnderstand how input data sources were created\n\ne.g., when working with survey data, have survey questionnaire and codebooks on hand (watch out for skip patterns!!!)\n\n\nFor each input data source, identify the “unit of analysis” and which combination of variables uniquely identify observations\nInvestigate patterns in input variables\nCreate analysis variable from input variable(s)\nVerify that analysis variable is created correctly through descriptive statistics that compare values of input variable(s) against values of the analysis variable\n\n\nIt is critically important to step through EDA processes at multiple points during data cleaning, from the input / raw data to the output / analysis / clean data.\nAlways be aware of missing values\nThey will not always be coded as NA in input variables (e.g., some projects code them as 99, 99999, negative values, etc.)"
  },
  {
    "objectID": "03-week3-workbook.html#unit-of-analysis-and-which-variables-uniquely-identify-observations",
    "href": "03-week3-workbook.html#unit-of-analysis-and-which-variables-uniquely-identify-observations",
    "title": "Week 3 Workbook",
    "section": "“Unit of analysis” and which variables uniquely identify observations",
    "text": "“Unit of analysis” and which variables uniquely identify observations\n“Unit of analysis” refers to “what does each observation represent” in an input data source\n\nIf each obs represents a trial in an experiment, you have “trial level data”\nIf each obs represents a participant, you have “participant level data”\nIf each obs represents a sample, you have “sample-level data”\nIf each obs represents a year, you have “year level data” (i.e. longitudinal)\n\nHow to identify unit of analysis\n\ndata documentation\ninvestigating the data set\nThis is very important because we often conduct analyses that span multiple units of analysis (e.g., between- v within-person, person- v stimuli-level, etc.)\nWe have to be careful and thoughtful about identifiers that let us do that (important for joining data together, which will be the focus on our R workshop today)"
  },
  {
    "objectID": "03-week3-workbook.html#rules-for-creating-new-variables",
    "href": "03-week3-workbook.html#rules-for-creating-new-variables",
    "title": "Week 3 Workbook",
    "section": "Rules for creating new variables",
    "text": "Rules for creating new variables\nRules I follow for variable creation\n\nNever modify “input variable”; instead create new variable based on input variable(s)\n\nAlways keep input variables used to create new variables\n\n\nInvestigate input variable(s) and relationship between input variables\nDeveloping a plan for creation of analysis variable\n\ne.g., for each possible value of input variables, what should value of analysis variable be?\n\n\nWrite code to create analysis variable\nRun descriptive checks to verify new variables are constructed correctly\n\nCan “comment out” these checks, but don’t delete them\n\n\nDocument new variables with notes and labels"
  },
  {
    "objectID": "03-week3-workbook.html#data-we-will-use",
    "href": "03-week3-workbook.html#data-we-will-use",
    "title": "Week 3 Workbook",
    "section": "Data we will use",
    "text": "Data we will use\nUse read_csv() function from readr (loaded with tidyverse) to import .csv dataset into R.\n\nCodelibrary(plyr)\nlibrary(tidyverse)\nsoep_long <- read_csv(file=\"https://github.com/emoriebeck/psc290-data-FQ23/raw/main/04-workshops/03-week3-tidyr/gsoep.csv\")\nsoep_long\n\n\nLet’s examine the data [you must run this code chunk]\n\nCodesoep_long %>% names()\nsoep_long %>% names() %>% str()\n\nstr(soep_long) # ugh\n\nstr(soep_long$LifeEvent__Married)\nattributes(soep_long$LifeEvent__Married)\ntypeof(soep_long$LifeEvent__Married)\nclass(soep_long$LifeEvent__Married)"
  },
  {
    "objectID": "03-week3-workbook.html#rule-1",
    "href": "03-week3-workbook.html#rule-1",
    "title": "Week 3 Workbook",
    "section": "Rule 1",
    "text": "Rule 1\n\nNever modify “input variable”; instead create new variable based on input variable(s)\n\nAlways keep input variables used to create new variables\n\n\n\n\nI already did this before the data were loaded in. I renamed all the input variables with interpretable names and reshaped them so the time variable (year) is long and the other variables are wide"
  },
  {
    "objectID": "03-week3-workbook.html#rule-2",
    "href": "03-week3-workbook.html#rule-2",
    "title": "Week 3 Workbook",
    "section": "Rule 2",
    "text": "Rule 2\n\nInvestigate input variable(s) and relationship between input variables\n\n\nWe’ll talk more about this in a bit when we discuss different kinds of descriptives, but briefly let’s look at basic descriptives + zero-order correlations\n\n\nCodedescribe(soep_long)\n\n\n\n  \n\n\n\nThis doesn’t look great because we’ve negative values where we shouldn’t, which represent flags for different kinds of missing variables. We’ll have to fix that\n\nI’ll show you a better way later, but we haven’t learned everything to do it nicely yet. So instead, we’ll use cor.plot() from the psych package to make a simple heat map of the correlations.\nWe shouldn’t see that many negative correlations, which flags that we need to reverse score some items\n\n\nCodesoep_2005 <- soep_long %>% filter(year == 2005) %>% select(-year)\ncor.plot(soep_2005, diag = F)"
  },
  {
    "objectID": "03-week3-workbook.html#rule-3",
    "href": "03-week3-workbook.html#rule-3",
    "title": "Week 3 Workbook",
    "section": "Rule 3",
    "text": "Rule 3\n\nDeveloping a plan for creation of analysis variable\n\ne.g., for each possible value of input variables, what should value of analysis variable be?\n\n\n\n\nI do this in my codebooks, and this topic warrants a discussion in itself. This is our focal topic for next week!\nIn this case, we want Big Five (EACNO) composites for each wave and to create composites of life events experienced across all years"
  },
  {
    "objectID": "03-week3-workbook.html#rule-4",
    "href": "03-week3-workbook.html#rule-4",
    "title": "Week 3 Workbook",
    "section": "Rule 4",
    "text": "Rule 4\n\nWrite code to create analysis variable\n\n\nFrom Rule 2, we know we need to recode missing values to NA and reverse code some items. From Rule 3, we know we need to create some composites.\nLet’s do that now!\n\nRecoding:\n\nCodesoep_long <- soep_long %>%\n  mutate_at(\n    vars(contains(\"Big5\"))\n    , ~ifelse(. < 0 | is.na(.), NA, .)\n    ) %>%\n  mutate_at(\n    vars(contains(\"LifeEvent\"))\n    , ~mapvalues(., seq(-7,1), c(rep(NA, 5), 0, NA, NA, 1), warn_missing = F)\n    )\n\n\nReverse Coding:\n\nCoderev_code <- c(\"Big5__A_coarse\", \"Big5__C_lazy\", \"Big5__E_reserved\", \"Big5__N_dealStress\")\nsoep_long <- soep_long %>%\n  mutate_at(\n    vars(all_of(rev_code))\n    , ~as.numeric(reverse.code(., keys = -1, mini = 1, maxi = 7))\n    )\n\n\nLet’s check to make sure some correlations just reversed:\n\nCodesoep_2005 <- soep_long %>% filter(year == 2005) %>% select(-year)\ncor.plot(soep_2005, diag = F)\n\n\n\n\nCreate Composites:\n(Note: I honestly wouldn’t normally do it like this, but we haven’t learned how to reshape data yet! Check the online materials for code on how to do this)\n\nCodesoep_long <- soep_long %>% \n  group_by(year, Procedural__SID) %>%\n  rowwise() %>%\n  mutate(\n    Big5__E = mean(cbind(Big5__E_reserved, Big5__E_communic, Big5__E_sociable), na.rm = T),\n    Big5__A = mean(cbind(Big5__A_coarse, Big5__A_friendly, Big5__A_forgive), na.rm = T),\n    Big5__C = mean(cbind(Big5__C_thorough, Big5__C_efficient, Big5__C_lazy), na.rm = T),\n    Big5__N = mean(cbind(Big5__N_worry, Big5__N_nervous, Big5__N_dealStress), na.rm = T),\n    Big5__O = mean(cbind(Big5__O_original, Big5__O_artistic, Big5__O_imagin), na.rm = T)) %>%\n  group_by(Procedural__SID) %>%\n  mutate_at(\n    vars(contains(\"LifeEvent\"))\n    , lst(ever = ~max(., na.rm = T))\n    ) %>%\n  ungroup() %>%\n  filter(year %in% c(2005, 2009, 2013))"
  },
  {
    "objectID": "03-week3-workbook.html#rule-5",
    "href": "03-week3-workbook.html#rule-5",
    "title": "Week 3 Workbook",
    "section": "Rule 5",
    "text": "Rule 5\n\nRun descriptive checks to verify new variables are constructed correctly\n\nCan “comment out” these checks, but don’t delete them\n\n\n\n\nCodesoep_long %>% \n  select(Big5__E:LifeEvent__SepPart_ever) %>%\n  describe()\n\n\n\n  \n\n\n\n\nUh oh, Inf values popping up what went wrong?\n\n-Inf pops up when there were no non-missing values and you use na.rm = T\n\nLet’s recode those as NA\n\n\n\nCodesoep_long <- soep_long %>%\n  mutate_all(~ifelse(is.infinite(.) | is.nan(.), NA, .))\n\n\nAnd check out the descriptives again\n\nCodesoep_long %>% \n  select(Big5__E:LifeEvent__SepPart_ever) %>%\n  describe()\n\n\n\n  \n\n\n\n\nCodesoep_long %>%\n  filter(year == 2005) %>%\n  select(Big5__E:LifeEvent__SepPart_ever) %>%\n  cor.plot(., diag = F)"
  },
  {
    "objectID": "03-week3-workbook.html#rule-6",
    "href": "03-week3-workbook.html#rule-6",
    "title": "Week 3 Workbook",
    "section": "Rule 6",
    "text": "Rule 6\n\nDocument new variables with notes and labels\n\n\nAgain, I do this in my codebooks, so more on this next week!!"
  },
  {
    "objectID": "03-week3-workbook.html#eda",
    "href": "03-week3-workbook.html#eda",
    "title": "Week 3 Workbook",
    "section": "EDA",
    "text": "EDA\n\n\nOne-way descriptive analyses (i.e,. focus on one variable)\n\nDescriptive analyses for continuous variables\nDescriptive analyses for discreet/categorical variables\n\n\n\nTwo-way descriptive analyses (relationship between two variables)\n\nCategorical by categorical\nCategorical by continuous\nContinuous by continuous\n\n\nRealistically, we’ve actually already covered all this above, so we’ll loop back to this after learning tidyr\n\n\n\n\n\n\n\nData Wrangling in tidyr\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/master/thumbs/tidyr.png\")"
  },
  {
    "objectID": "03-week3-workbook.html#pivot_longer",
    "href": "03-week3-workbook.html#pivot_longer",
    "title": "Week 3 Workbook",
    "section": "1. pivot_longer()\n",
    "text": "1. pivot_longer()\n\n\n(Formerly gather()) Makes wide data long, based on a key \n\nCore arguments:\n\n\ndata: the data, blank if piped\n\ncols: columns to be made long, selected via select() calls\n\nnames_to: name(s) of key column(s) in new long data frame (string or string vector)\n\nvalues_to: name of values in new long data frame (string)\n\nnames_sep: separator in column headers, if multiple keys\n\nvalues_drop_na: drop missing cells (similar to na.rm = T) \n\n\n\nBasic Application\nLet’s start with an easy one – one key, one value:\n\nCodebfi %>%\n  rownames_to_column(\"SID\") %>%\n  pivot_longer(\n    cols = A1:O5\n    , names_to = \"item\"\n    , values_to = \"values\"\n    , values_drop_na = T\n  ) %>%\n  print(n = 8)\n\n# A tibble: 69,492 × 6\n  SID   gender education   age item  values\n  <chr>  <int>     <int> <int> <chr>  <int>\n1 61617      1        NA    16 A1         2\n2 61617      1        NA    16 A2         4\n3 61617      1        NA    16 A3         3\n4 61617      1        NA    16 A4         4\n5 61617      1        NA    16 A5         4\n6 61617      1        NA    16 C1         2\n7 61617      1        NA    16 C2         3\n8 61617      1        NA    16 C3         3\n# ℹ 69,484 more rows\n\n\nMore Advanced Application\nNow a harder one – two keys, one value:\n\nCodebfi %>%\n  rownames_to_column(\"SID\") %>%\n  pivot_longer(\n    cols = A1:O5\n    , names_to = c(\"trait\", \"item_num\")\n    , names_sep = -1\n    , values_to = \"values\"\n    , values_drop_na = T\n  ) %>%\n  print(n = 8)\n\n# A tibble: 69,492 × 7\n  SID   gender education   age trait item_num values\n  <chr>  <int>     <int> <int> <chr> <chr>     <int>\n1 61617      1        NA    16 A     1             2\n2 61617      1        NA    16 A     2             4\n3 61617      1        NA    16 A     3             3\n4 61617      1        NA    16 A     4             4\n5 61617      1        NA    16 A     5             4\n6 61617      1        NA    16 C     1             2\n7 61617      1        NA    16 C     2             3\n8 61617      1        NA    16 C     3             3\n# ℹ 69,484 more rows"
  },
  {
    "objectID": "03-week3-workbook.html#pivot_wider",
    "href": "03-week3-workbook.html#pivot_wider",
    "title": "Week 3 Workbook",
    "section": "2. pivot_wider()\n",
    "text": "2. pivot_wider()\n\n\n(Formerly spread()) Makes wide data long, based on a key \n\nCore arguments:\n\n\ndata: the data, blank if piped\n\nnames_from: name(s) of key column(s) in new long data frame (string or string vector)\n\nnames_sep: separator in column headers, if multiple keys\n\nnames_glue: specify multiple or custom separators of multiple keys\n\nvalues_from: name of values in new long data frame (string)\n\nvalues_fn: function applied to data with duplicate labels \n\n\n\nBasic Application\n\nCodebfi_long <- bfi %>%\n  rownames_to_column(\"SID\") %>%\n  pivot_longer(\n    cols = A1:O5\n    , names_to = \"item\"\n    , values_to = \"values\"\n    , values_drop_na = T\n  )\n\n\nMore Advanced\n\nCodebfi_long <- bfi %>%\n  rownames_to_column(\"SID\") %>%\n  pivot_longer(\n    cols = A1:O5\n    , names_to = c(\"trait\", \"item_num\")\n    , names_sep = -1\n    , values_to = \"values\"\n    , values_drop_na = T\n  )\n\n\n\nCodebfi_long %>%\n  pivot_wider(\n    names_from = c(\"trait\", \"item_num\")\n    , values_from = \"values\"\n    , names_sep = \"_\"\n  )\n\n\n\n  \n\n\n\nA Little More Advanced\n\nCodebfi_long %>%\n  select(-item_num) %>%\n  pivot_wider(\n    names_from = \"trait\"\n    , values_from = \"values\"\n    , names_sep = \"_\"\n    , values_fn = mean\n  )"
  },
  {
    "objectID": "03-week3-workbook.html#the-_join-functions",
    "href": "03-week3-workbook.html#the-_join-functions",
    "title": "Week 3 Workbook",
    "section": "The _join() Functions",
    "text": "The _join() Functions\n\nOften we may need to pull different data from different sources\nThere are lots of reasons to need to do this\nWe don’t have time to get into all the use cases here, so we’ll talk about them in high level terms\n\nWe’ll focus on:\n\nfull_join()\ninner_join()\nleft_join()\nright_join()\n\n\nLet’s separate demographic and BFI data\n\n\nCodebfi_only <- bfi %>% \n  rownames_to_column(\"SID\") %>%\n  select(SID, matches(\"[0-9]\"))\nbfi_only %>% as_tibble() %>% print(n = 6)\n\n# A tibble: 2,800 × 26\n  SID      A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\n  <chr> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 61617     2     4     3     4     4     2     3     3     4     4     3     3\n2 61618     2     4     5     2     5     5     4     4     3     4     1     1\n3 61620     5     4     5     4     4     4     5     4     2     5     2     4\n4 61621     4     4     6     5     5     4     4     3     5     5     5     3\n5 61622     2     3     3     4     5     4     4     5     3     2     2     2\n6 61623     6     6     5     6     5     6     6     6     1     3     2     1\n# ℹ 2,794 more rows\n# ℹ 13 more variables: E3 <int>, E4 <int>, E5 <int>, N1 <int>, N2 <int>,\n#   N3 <int>, N4 <int>, N5 <int>, O1 <int>, O2 <int>, O3 <int>, O4 <int>,\n#   O5 <int>\n\n\n\nCodebfi_dem <- bfi %>%\n  rownames_to_column(\"SID\") %>%\n  select(SID, education, gender, age)\nbfi_dem %>% as_tibble() %>% print(n = 6)\n\n# A tibble: 2,800 × 4\n  SID   education gender   age\n  <chr>     <int>  <int> <int>\n1 61617        NA      1    16\n2 61618        NA      2    18\n3 61620        NA      2    17\n4 61621        NA      2    17\n5 61622        NA      1    17\n6 61623         3      2    21\n# ℹ 2,794 more rows\n\n\nBefore we get into it, as a reminder, this is what the data set looks like before we do any joining:\n\nCodebfi %>%\n  rownames_to_column(\"SID\") %>%\n  as_tibble() %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 29\n  SID      A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\n  <chr> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 61617     2     4     3     4     4     2     3     3     4     4     3     3\n2 61618     2     4     5     2     5     5     4     4     3     4     1     1\n3 61620     5     4     5     4     4     4     5     4     2     5     2     4\n4 61621     4     4     6     5     5     4     4     3     5     5     5     3\n5 61622     2     3     3     4     5     4     4     5     3     2     2     2\n6 61623     6     6     5     6     5     6     6     6     1     3     2     1\n# ℹ 2,794 more rows\n# ℹ 16 more variables: E3 <int>, E4 <int>, E5 <int>, N1 <int>, N2 <int>,\n#   N3 <int>, N4 <int>, N5 <int>, O1 <int>, O2 <int>, O3 <int>, O4 <int>,\n#   O5 <int>, gender <int>, education <int>, age <int>"
  },
  {
    "objectID": "03-week3-workbook.html#full_join",
    "href": "03-week3-workbook.html#full_join",
    "title": "Week 3 Workbook",
    "section": "3. full_join()\n",
    "text": "3. full_join()\n\nMost simply, we can put those back together keeping all observations.\n\nCodebfi_only %>%\n  full_join(bfi_dem) %>%\n  as_tibble() %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 29\n  SID      A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\n  <chr> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 61617     2     4     3     4     4     2     3     3     4     4     3     3\n2 61618     2     4     5     2     5     5     4     4     3     4     1     1\n3 61620     5     4     5     4     4     4     5     4     2     5     2     4\n4 61621     4     4     6     5     5     4     4     3     5     5     5     3\n5 61622     2     3     3     4     5     4     4     5     3     2     2     2\n6 61623     6     6     5     6     5     6     6     6     1     3     2     1\n# ℹ 2,794 more rows\n# ℹ 16 more variables: E3 <int>, E4 <int>, E5 <int>, N1 <int>, N2 <int>,\n#   N3 <int>, N4 <int>, N5 <int>, O1 <int>, O2 <int>, O3 <int>, O4 <int>,\n#   O5 <int>, education <int>, gender <int>, age <int>"
  },
  {
    "objectID": "03-week3-workbook.html#inner_join",
    "href": "03-week3-workbook.html#inner_join",
    "title": "Week 3 Workbook",
    "section": "4. inner_join()\n",
    "text": "4. inner_join()\n\nWe can also keep all rows present in both data frames\n\nCodebfi_dem %>%\n  filter(row_number() %in% 1:1700) %>%\n  inner_join(\n    bfi_only %>%\n      filter(row_number() %in% 1200:2800)\n  ) %>%\n  as_tibble() %>%\n  print(n = 6)\n\n# A tibble: 501 × 29\n  SID   education gender   age    A1    A2    A3    A4    A5    C1    C2    C3\n  <chr>     <int>  <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 64151         3      2    18     1     5     6     5     5     5     6     5\n2 64152         4      2    29     1     5     6     5     5     2     1     4\n3 64154         5      1    46     2     5     6     5     6     6     6     6\n4 64155         5      1    58     5     4     4     4     5     4     4     5\n5 64156         5      2    38     1     4     6     6     6     4     4     5\n6 64158         5      2    27     2     3     1     1     1     4     2     2\n# ℹ 495 more rows\n# ℹ 17 more variables: C4 <int>, C5 <int>, E1 <int>, E2 <int>, E3 <int>,\n#   E4 <int>, E5 <int>, N1 <int>, N2 <int>, N3 <int>, N4 <int>, N5 <int>,\n#   O1 <int>, O2 <int>, O3 <int>, O4 <int>, O5 <int>"
  },
  {
    "objectID": "03-week3-workbook.html#left_join",
    "href": "03-week3-workbook.html#left_join",
    "title": "Week 3 Workbook",
    "section": "5. left_join()\n",
    "text": "5. left_join()\n\nOr all rows present in the left (first) data frame, perhaps if it’s a subset of people with complete data\n\nCodebfi_dem %>%\n  drop_na() %>%\n  left_join(bfi_only) %>%\n  as_tibble() %>%\n  print(n = 6)\n\n# A tibble: 2,577 × 29\n  SID   education gender   age    A1    A2    A3    A4    A5    C1    C2    C3\n  <chr>     <int>  <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 61623         3      2    21     6     6     5     6     5     6     6     6\n2 61629         2      1    19     4     3     1     5     1     3     2     4\n3 61630         1      1    19     4     3     6     3     3     6     6     3\n4 61634         1      1    21     4     4     5     6     5     4     3     5\n5 61640         1      1    17     4     5     2     2     1     5     5     5\n6 61661         5      1    68     1     5     6     5     6     4     3     2\n# ℹ 2,571 more rows\n# ℹ 17 more variables: C4 <int>, C5 <int>, E1 <int>, E2 <int>, E3 <int>,\n#   E4 <int>, E5 <int>, N1 <int>, N2 <int>, N3 <int>, N4 <int>, N5 <int>,\n#   O1 <int>, O2 <int>, O3 <int>, O4 <int>, O5 <int>"
  },
  {
    "objectID": "03-week3-workbook.html#right_join",
    "href": "03-week3-workbook.html#right_join",
    "title": "Week 3 Workbook",
    "section": "6. right_join()\n",
    "text": "6. right_join()\n\nOr all rows present in the right (second) data frame, such as I do when I join a codebook with raw data\n\nCodebfi_dem %>%\n  drop_na() %>%\n  right_join(bfi_only) %>%\n  as_tibble() %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 29\n  SID   education gender   age    A1    A2    A3    A4    A5    C1    C2    C3\n  <chr>     <int>  <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1 61623         3      2    21     6     6     5     6     5     6     6     6\n2 61629         2      1    19     4     3     1     5     1     3     2     4\n3 61630         1      1    19     4     3     6     3     3     6     6     3\n4 61634         1      1    21     4     4     5     6     5     4     3     5\n5 61640         1      1    17     4     5     2     2     1     5     5     5\n6 61661         5      1    68     1     5     6     5     6     4     3     2\n# ℹ 2,794 more rows\n# ℹ 17 more variables: C4 <int>, C5 <int>, E1 <int>, E2 <int>, E3 <int>,\n#   E4 <int>, E5 <int>, N1 <int>, N2 <int>, N3 <int>, N4 <int>, N5 <int>,\n#   O1 <int>, O2 <int>, O3 <int>, O4 <int>, O5 <int>"
  },
  {
    "objectID": "03-week3-workbook.html#eda-1",
    "href": "03-week3-workbook.html#eda-1",
    "title": "Week 3 Workbook",
    "section": "EDA",
    "text": "EDA\n\n\nOne-way descriptive analyses (i.e,. focus on one variable)\n\nDescriptive analyses for continuous variables\nDescriptive analyses for discreet/categorical variables\n\n\n\nTwo-way descriptive analyses (relationship between two variables)\n\nCategorical by categorical\nCategorical by continuous\nContinuous by continuous"
  },
  {
    "objectID": "03-week3-workbook.html#one-way-descriptive-analyses",
    "href": "03-week3-workbook.html#one-way-descriptive-analyses",
    "title": "Week 3 Workbook",
    "section": "One-way descriptive analyses",
    "text": "One-way descriptive analyses\n\nThese are basically what they sound like – the focus is on single variables\nDescriptive analyses for continuous variables\n\nmeans, standard deviations, minima, maxima, counts\n\n\nDescriptive analyses for discreet/categorical variables\n\ncounts, percentages\n\n\n\nContinuous\n\nCodesoep_long %>% \n  select(Procedural__SID,year,Big5__E:Big5__O) %>%\n  pivot_longer(\n    cols = contains(\"Big5\")\n    , names_to = \"trait\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) %>%\n  group_by(year, trait) %>%\n  summarize_at(\n    vars(value)\n    , lst(mean, sd, min, max)\n    , na.rm = T\n  ) %>%\n  ungroup()\n\n\n\n  \n\n\n\nCategorical / Count\n\nCodesoep_long %>%\n  select(Procedural__SID, contains(\"_ever\")) %>%\n  distinct() %>%\n  pivot_longer(\n    cols = contains(\"LifeEvent\")\n    , names_to = \"event\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) %>%\n  group_by(event, value) %>%\n  tally() %>%\n  group_by(event) %>%\n  mutate(total = sum(n)\n         , perc = n/total*100)"
  },
  {
    "objectID": "03-week3-workbook.html#two-way-descriptive-analyses",
    "href": "03-week3-workbook.html#two-way-descriptive-analyses",
    "title": "Week 3 Workbook",
    "section": "Two-way descriptive analyses",
    "text": "Two-way descriptive analyses\n\nAims to capture relationships between variables\n\nCategorical by categorical\n\ncross-tabs, percentages\n\n\n\nCategorical by continuous\n\nmeans, standard deviations, etc. within categories\n\n\n\nContinuous by continuous\n\ncorrelations, covariances, etc.\n\n\n\nCategorical x Categorical\n\nCodesoep_long %>%\n  select(Procedural__SID, Demographic__Sex, contains(\"_ever\")) %>%\n  distinct() %>%\n  pivot_longer(\n    cols = contains(\"LifeEvent\")\n    , names_to = \"event\"\n    , values_to = \"occurred\"\n    , values_drop_na = T\n  ) %>%\n  mutate(Demographic__Sex = mapvalues(Demographic__Sex, c(1,2), c(\"Male\", \"Female\"))\n         , occurred = mapvalues(occurred, c(0,1), c(\"No Event\", \"Event\"))) %>%\n  group_by(event, occurred, Demographic__Sex) %>%\n  tally() %>%\n  group_by(event) %>%\n  mutate(perc = n/sum(n)*100) %>%\n  pivot_wider(\n    names_from = c(occurred)\n    , values_from = c(n, perc)\n  )\n\n\n\n  \n\n\n\nCategorical x Continuous\nSet up the data\n\nCodesoep_twoway <- soep_long %>% \n  filter(year == 2005) %>%\n  select(Procedural__SID, Big5__E:Big5__O) %>%\n  pivot_longer(\n    cols = contains(\"Big5\")\n    , names_to = \"trait\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) %>%\n  left_join(\n    soep_long %>%\n      select(Procedural__SID, contains(\"_ever\")) %>%\n      distinct() %>%\n      pivot_longer(\n        cols = contains(\"LifeEvent\")\n        , names_to = \"event\"\n        , values_to = \"occurred\"\n        , values_drop_na = T\n      ) %>%\n      mutate(occurred = mapvalues(occurred, c(0,1), c(\"No Event\", \"Event\")))\n  )\n\n\nRun the descriptives\n\nCodesoep_twoway %>%\n  group_by(trait, event, occurred) %>%\n  summarize_at(\n    vars(value)\n    , lst(mean, sd, min, max)\n    , na.rm = T\n  ) %>%\n  ungroup() %>%\n  pivot_wider(\n    names_from = trait\n    , values_from = c(mean, sd, min, max)\n  )\n\n\n\n  \n\n\n\nContinuous x continuous\nHere’s how I create more customizable heat maps in ggplot2 for those who would like a reference for themselves.\n\nCoder <- soep_long %>% \n  filter(year == 2005) %>%\n  select(Big5__E:Big5__O) %>%\n  cor(., use = \"pairwise\") \n\nr[lower.tri(r, diag = T)] <- NA\nvars <- rownames(r)\nr %>%\n  data.frame() %>%\n  rownames_to_column(\"V1\") %>%\n  pivot_longer(\n    cols = -V1\n    , names_to = \"V2\"\n    , values_to = \"r\"\n  ) %>%\n  mutate(V1 = factor(V1, levels = vars)\n         , V2 = factor(V2, levels = rev(vars))) %>%\n  ggplot(aes(x = V1, y = V2, fill = r)) + \n    geom_raster() + \n  geom_text(aes(label = round(r, 2))) + \n  scale_fill_gradient2(\n    limits = c(-1,1)\n    , breaks = c(-1, -.5, 0, .5, 1)\n    , low = \"blue\", high = \"red\"\n    , mid = \"white\", na.value = \"white\") + \n  labs(\n    x = NULL\n    , y = NULL\n    , fill = \"Zero-Order Correlation\"\n    , title = \"Zero-Order Correlations Among Variables\"\n    ) + \n  theme_classic() + \n  theme(\n    legend.position = \"bottom\"\n    , axis.text = element_text(face = \"bold\")\n    , axis.text.x = element_text(angle = 45, hjust = 1)\n    , plot.title = element_text(face = \"bold\", hjust = .5)\n    , plot.subtitle = element_text(face = \"italic\", hjust = .5)\n    , panel.background = element_rect(color = \"black\", size = 1)\n  )"
  },
  {
    "objectID": "03-week3-workbook.html#attributions",
    "href": "03-week3-workbook.html#attributions",
    "title": "Week 3 Workbook",
    "section": "Attributions",
    "text": "Attributions\nParts of Part 1 of these slides was adapted from Ozan Jaquette’s EDUC 260A at UCLA."
  },
  {
    "objectID": "ps2-week2.html",
    "href": "ps2-week2.html",
    "title": "Problem Set Week 2",
    "section": "",
    "text": "Due Date: Monday, October 16, 12:01 AM PST.\nDownload your problem set for week 2 below or on Canvas.\nAnswers can be found here."
  },
  {
    "objectID": "01-week1-workbook.html",
    "href": "01-week1-workbook.html",
    "title": "Week 1 Workbook",
    "section": "",
    "text": "You can download the code for this workbook and the rendered workbook here.\n\nCodelibrary(knitr)\nlibrary(psych)\nlibrary(plyr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()     masks psych::%+%()\n✖ ggplot2::alpha()   masks psych::alpha()\n✖ dplyr::arrange()   masks plyr::arrange()\n✖ purrr::compact()   masks plyr::compact()\n✖ dplyr::count()     masks plyr::count()\n✖ dplyr::desc()      masks plyr::desc()\n✖ dplyr::failwith()  masks plyr::failwith()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::id()        masks plyr::id()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::mutate()    masks plyr::mutate()\n✖ dplyr::rename()    masks plyr::rename()\n✖ dplyr::summarise() masks plyr::summarise()\n✖ dplyr::summarize() masks plyr::summarize()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
  },
  {
    "objectID": "01-week1-workbook.html#course-goals-learning-outcomes",
    "href": "01-week1-workbook.html#course-goals-learning-outcomes",
    "title": "Week 1 Workbook",
    "section": "Course Goals & Learning Outcomes",
    "text": "Course Goals & Learning Outcomes\nAfter successful completion of this course, you will be able to:\n\nBuild your own research workflow that can be ported to future projects.\nLearn new programming skills that will help you efficiently, accurately, and deliberately clean and manage your data.\nCreate a bank of code and tools that can be used for a variety of types of research."
  },
  {
    "objectID": "01-week1-workbook.html#course-expectations",
    "href": "01-week1-workbook.html#course-expectations",
    "title": "Week 1 Workbook",
    "section": "Course Expectations",
    "text": "Course Expectations\n\n~50% of the course will be in R\nYou will get the most from this course if you:\n\nhave your own data you can apply course content to\nknow how to clean clean, transform, and manage that data\ntoday’s workshop is a good litmus test for this"
  },
  {
    "objectID": "01-week1-workbook.html#course-materials",
    "href": "01-week1-workbook.html#course-materials",
    "title": "Week 1 Workbook",
    "section": "Course Materials",
    "text": "Course Materials\n\nAll materials (required and optional) are free and online\n\nWickham & Grolemond: R for Data Science https://r4ds.had.co.nz\n\nWickham: Advanced R http://adv-r.had.co.nz\n\n\nData Camp: All paid content unlocked"
  },
  {
    "objectID": "01-week1-workbook.html#assignments",
    "href": "01-week1-workbook.html#assignments",
    "title": "Week 1 Workbook",
    "section": "Assignments",
    "text": "Assignments\n\n\nAssignment Weights\nPercent\n\n\n\nClass Participation\n20%\n\n\nProblem Sets\n40%\n\n\nFinal Project Proposal\n10%*\n\n\nClass Presentation\n10%*\n\n\nFinal Project\n20%*\n\n\nTotal\n100%\n\n\n\nClass Participation\n\nThere are lots of ways to participate, both in and outside class meetings\nClasses will be technologically hybrid\nThe goal of this is for accessibility and to create recordings\nIf you need to miss 2+ classes (i.e. 20+% of total class time), maybe consider taking the course in a different year\nProblem Sets\n\nThe main homework in the course are weekly problem sets\nThe goal is to let you apply concepts from that week to your own data (or whatever data you’ll focus on for the class)\nProblem sets will be posted on Mondays before class\nDue 12:01 AM each Monday (starting next Monday and not including the last day of the course)\nFinal Projects\n\nFinal project replaces final exam (there are no exams)\nThis is a bring your own data class, so the goal of the course is to apply what you’re learning to your own research throughout the term\nDetails of the final project TBD, but will generally include\n\nStage 1: Proposals (due 11/13/23)\nStage 2: In-class presentations (12/04/23)\nStage 3: Final project submission (Due day and time of scheduled final; which I can’t access because ScheduleBuilder thinks I need a CRN for my own course and no one emails me back 🙃)\n\n\nExtra Credit\n\nParticipate in a https://www.tidytuesday.com.\n2 pt extra credit for each one you participate in (max 6 pt total).\nCan post on Twitter or just create a document with the code and output\n\nSubmit on Canvas\n\nIf posting, link the post in the Canvas submission\nIf not posting, attach the knitted file on Canvas"
  },
  {
    "objectID": "01-week1-workbook.html#grading-scale",
    "href": "01-week1-workbook.html#grading-scale",
    "title": "Week 1 Workbook",
    "section": "Grading Scale",
    "text": "Grading Scale\n92.5% - 100% = A; 89.5% - 92.4% = A-\n87.5% - 89.4% = B+; 82.5% - 87.4% = B; 79.5% - 82.4% = B-\n77.5% - 79.4% = C+; 72.5% - 77.4% = C; 69.5% - 72.4% = C-\n67.5% - 69.4% = D+; 62.5% - 67.4% = D; 59.5% - 62.4% = D-\n0% - 59.4% = F"
  },
  {
    "objectID": "01-week1-workbook.html#schedule",
    "href": "01-week1-workbook.html#schedule",
    "title": "Week 1 Workbook",
    "section": "Schedule",
    "text": "Schedule\n\nWeek 1: Intro & Basics\n\nWeek 2: Reproducibility & dplyr\n\nWeek 3: Data Quality & tidyr\n\nWeek 4: Codebooks & importing data\n\nWeek 5: Data structures & transformation\nWeek 6: Versioning & purrr\n\nWeek 7: Efficient R & parallelization\n\nWeek 8: TBD & tables and figures in R\n\nWeek 9: Odds and ends & help with projects\n\nWeek 10: Presentations"
  },
  {
    "objectID": "01-week1-workbook.html#why-should-i-care",
    "href": "01-week1-workbook.html#why-should-i-care",
    "title": "Week 1 Workbook",
    "section": "Why Should I Care?",
    "text": "Why Should I Care?\n\nWhether you like it or not, you have a workflow\nYou have ways you go about doing a project that you maybe haven’t thought too much about\nIssues arise when\n\nA workflow has missing steps\n\nYour workflow is inconsistent across projects\nYour workflow is inefficient, which can lead to mistakes\n\n\nA workflow is a work in progress. If it no longer serves you, let it go"
  },
  {
    "objectID": "01-week1-workbook.html#how-do-i-build-a-workflow",
    "href": "01-week1-workbook.html#how-do-i-build-a-workflow",
    "title": "Week 1 Workbook",
    "section": "How Do I Build a Workflow?",
    "text": "How Do I Build a Workflow?\n\nBuilding a good workflow is both top-down (i.e. big steps to smaller ones) and bottom-up (i.e. necessary smaller steps make certain larger ones necessary)\nWhat?\n\n\n\nExample: New Data Collection\n1. Conceptualization\n2. Funding acquisition\n3. Preregistration\n4. Project Building\n5. Data Collection\n6. Data Cleaning\n7. Data Analysis\n8. Writing (and rewriting)\n9. Submission\n10. Revision (and possibly crying)\n11. ACCEPTANCE\n\nExample: Secondary Data\n1. Conceptualization\n2. Data search\n3. Project Building\n4. Data documentation\n5. Preregistration\n6. Data Cleaning\n7. Data Analysis\n8. Writing (and rewriting)\n9. Submission\n10. Revision (and possibly crying)\n11. ACCEPTANCE\n\n\n\nWorkflows Are Hierarchical: Example – Data Cleaning Steps\n\n\n\nExperimental Data\n1. Gather all data files\n2. Quality checks for each file\n3. Load all files\n4. Merge all files\n5. Check all descriptives\n6. Scoring, coding, and data transformation\n7. Recheck all descriptives\n8. Correlations and visualization\n9. Restructure data for analyses\n\nSecondary Data\n1. Gather all data files\n2. Load each file\n3. Extract variables used\n4. Rename variables, possibly deal with time variables\n4. Merge all files\n5. Check all descriptives\n6. Scoring, coding, and data transformation\n7. Recheck all descriptives\n8. Correlations and visualization\n9. Restructure data for analyses"
  },
  {
    "objectID": "01-week1-workbook.html#workflows-overview-of-the-course",
    "href": "01-week1-workbook.html#workflows-overview-of-the-course",
    "title": "Week 1 Workbook",
    "section": "Workflows: Overview of the Course",
    "text": "Workflows: Overview of the Course\nIn this class, we will focus on building tools for:\n\nDocumenting Data (both before and after collection)\nFile management (how do I build a machine and human navigable directory)\nLoading data files\nAll steps of cleaning data\nRestructuring Data\nDESCRIPTIVES DESCRIPTIVES DESCRIPTIVES\nEfficient Programming (plz stop copy-pasting)\nThis class does not focus on modeling but rather how you get your data set up to run models (Weeks 1-5/6) AND how to extract and present data after you run them (Weeks 6/7-9)\nWe will focus on classes of models in R you will most likely encounter (lm(), glm(), lmer(), nlme(), lavaan, brms)\nIf you run other kinds of models, most tools we will use are portable to many packages and other object classes\nBy the end of this class, my goal is that you:\n\n\nHave a documented workflow for the kind of research you work on\n\nHave a set of tools and skills that apply to each piece of that workflow\n\nHave a skillset that will allow you to adapt and build new workflows for different kinds of research"
  },
  {
    "objectID": "01-week1-workbook.html#what-is-r-why-r",
    "href": "01-week1-workbook.html#what-is-r-why-r",
    "title": "Week 1 Workbook",
    "section": "What is R? Why R?",
    "text": "What is R? Why R?\n\nAn “open source” programming language and software that provide collections of interrelated “functions”\n“open source” means that R is free and created by the user community. The user community can modify basic things about R and add new capabilities to what R can do the user community can modify R and\na “function” is usually something that takes in some “input,” processes this input in some way, and creates some “output”\n\ne.g., the max() function takes as input a collection of numbers (e.g., 3,5,6) and returns as output the number with the maximum value\ne.g., the lm() function takes in as inputs a dataset and a statistical model you specify within the function, and returns as output the results of the regression model"
  },
  {
    "objectID": "01-week1-workbook.html#base-r-vs.-r-packages",
    "href": "01-week1-workbook.html#base-r-vs.-r-packages",
    "title": "Week 1 Workbook",
    "section": "Base R vs. R packages",
    "text": "Base R vs. R packages\n\n\nBase R\n\nWhen you install R, you automatically install the “Base R” set of functions\nExample of a few of the functions in in Base R:\n\n\nas.character() function\n\nprint() function\n\nsetwd() function\n\n\n\n\nR packages\n\nan R “package” (or “library”) is a collection of (related) functions developed by the R community\nExamples of R packages:\n\n\ntidyverse package for manipulating and visualizing data\n\nigraph package for network analyses\n\nleaflet package for mapping\n\nrvest package for webscraping\n\nrtweet package for streaming and downloading data from Twitter\n\n\n\nAll R packages are free!"
  },
  {
    "objectID": "01-week1-workbook.html#why-use-rstudio-pivot",
    "href": "01-week1-workbook.html#why-use-rstudio-pivot",
    "title": "Week 1 Workbook",
    "section": "Why Use RStudio (Pivot)",
    "text": "Why Use RStudio (Pivot)\n\n\n\nAlso free\nBasically a GUI for R\nOrganize files, import data, etc. with ease\nRMarkdown, Quarto, and more are powerful tools (they were used to create these slides!)\nLots of new features and support\n\n\n\nCodeknitr::include_graphics(\"https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/01-week1-intro/02-code/02-images/RStudio-Logo-Flat.png\")"
  },
  {
    "objectID": "01-week1-workbook.html#why-use-the-tidyverse",
    "href": "01-week1-workbook.html#why-use-the-tidyverse",
    "title": "Week 1 Workbook",
    "section": "Why Use the tidyverse\n",
    "text": "Why Use the tidyverse\n\n\n\n\nMaintained by RStudio (Pivot)\nNo one should have to use a for loop to change data from long to wide\nTons of integrated tools for data cleaning, manipulation, transformation, and visualization\nEven increasing support for modeling (e.g., tidymodels)\n\n\n\nCodeknitr::include_graphics(\"https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/01-week1-intro/02-code/02-images/tidyverse.png\")\n\n\n\n\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/tidyr.png\")\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/stringr.png\")\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/shiny.png\")\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/rmarkdown.png\")\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/quarto.png\")\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/knitr.png\")\n\n\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/ggplot2.png\")\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/forcats.png\")\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/dplyr.png\")\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/broom.png\")\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/tibble.png\")\n\n\n\n\n\n\n\n\nCodeknitr::include_graphics(\"https://github.com/rstudio/hex-stickers/raw/main/thumbs/purrr.png\")"
  },
  {
    "objectID": "01-week1-workbook.html#why-use-quarto",
    "href": "01-week1-workbook.html#why-use-quarto",
    "title": "Week 1 Workbook",
    "section": "Why use Quarto",
    "text": "Why use Quarto\nQuarto\n\n\n\n\n\nThese slides\n\nThe course website\n\nYour homework\n\nAll written in Quarto"
  },
  {
    "objectID": "01-week1-workbook.html#some-r-basics",
    "href": "01-week1-workbook.html#some-r-basics",
    "title": "Week 1 Workbook",
    "section": "Some R Basics",
    "text": "Some R Basics\nExecuting R commands\nThree ways to execute commands in R\n\nType/copy commands directly into the “console”\n`code chunks’ in RMarkdown (.Rmd files)\n\n\nCmd/Ctrl + Enter: execute highlighted line(s) within chunk\n\nCmd/Ctrl + Shift + k: “knit” entire document\n\n\nR scripts (.R files)\n\n\nCmd/Ctrl + Enter: execute highlighted line(s)\n\nCmd/Ctrl + Shift + Enter (without highlighting any lines): run entire script\n\n\nAssignment\nAssignment refers to creating an “object” and assigning values to it\n\nThe object may be a variable, a dataset, a bit of text that reads “la la la”\n\n<- is the assignment operator\n\nin other languages = is the assignment operator\n\n\ngeneral syntax:\n\nobject_name <- object_values\ngood practice to put a space before and after assignment operator\n\n\nObjects\nR is an “object-oriented” programming language (like Python, JavaScript). So, what is an “object”?\n\nformal computer science definitions are confusing because they require knowledge of concepts we haven’t introduced yet\nMore intuitively, I think objects as anything I assign values to\n\nFor example, below, a and b are the names of objects I assigned values to\n\n\n\n\nCodea <- 5\na\n\n[1] 5\n\nCodeb <- \"yay!\"\nb\n\n[1] \"yay!\"\n\n\n\nBen Skinner says “Objects are like boxes in which we can put things: data, functions, and even other objects.”\nMany commercial statistical software packages (e.g., SPSS, Stata) operate on datasets, which consist of rows of observations and columns of variables\nUsually, these packages can open only one dataset at a time\nBy contrast, in R everything is an object and there is no limit to the number of objects R can hold (except memory)\nVectors\nThe fundamental data structure in R is the “vector”\n\nA vector is a collection of values\nThe individual values within a vector are called “elements”\nValues in a vector can be numeric, character (e.g., “Apple”), or some other type\nBelow we use the combine function c() to create a numeric vector that contains three elements\nHelp file says that c() “combines values into a vector or list”\n\n\nCode#?c # to see help file for the c() \"combine\" function\nx <- c(4, 7, 9) # create object called x, which is a vector with three elements \n# (each an integer)\nx # print object x\n\n[1] 4 7 9\n\n\nVector where the elements are characters\n\nCodeanimals <- c(\"lions\", \"tigers\", \"bears\", \"oh my\") # create object called animals\nanimals\n\n[1] \"lions\"  \"tigers\" \"bears\"  \"oh my\""
  },
  {
    "objectID": "01-week1-workbook.html#exercise",
    "href": "01-week1-workbook.html#exercise",
    "title": "Week 1 Workbook",
    "section": "EXERCISE",
    "text": "EXERCISE\nEither in the R console or within the R markdown file, do the following:\n\nCreate a vector called v1 with three elements, where all the elements are numbers. Then print the values.\nCreate a vector called v2 with four elements, where all the elements are characters (i.e., enclosed in single ’’ or double “” quotes). Then print the values.\nCreate a vector called v3 with five elements, where some elements are numeric and some elements are characters. Then print the values."
  },
  {
    "objectID": "01-week1-workbook.html#solution-to-exercise",
    "href": "01-week1-workbook.html#solution-to-exercise",
    "title": "Week 1 Workbook",
    "section": "Solution to Exercise",
    "text": "Solution to Exercise\n\nCodev1 <- c(1, 2, 3) \n# create a vector called v1 with three elements\n# all the elements are numbers\nv1 # print value\n\n[1] 1 2 3\n\n\n\nCodev2 <- c(\"a\", \"b\", \"c\", \"d\") \n# create a vector called v2 with four elements\n# all the elements are characters\nv2 # print value\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\n\n\nCodev3 <- c(1, 2, 3, \"a\", \"b\") \n# create a vector called v3 with five element\n# some elements are numeric and some elements are characters\nv3 # print value\n\n[1] \"1\" \"2\" \"3\" \"a\" \"b\""
  },
  {
    "objectID": "01-week1-workbook.html#formal-classification-of-vectors-in-r",
    "href": "01-week1-workbook.html#formal-classification-of-vectors-in-r",
    "title": "Week 1 Workbook",
    "section": "Formal classification of vectors in R",
    "text": "Formal classification of vectors in R\n\nHere, I introduce the classification of vectors by Grolemund and Wickham\nThere are two broad types of vectors\n\n\n\nAtomic vectors. An object that contains elements. Six “types” of atomic vectors:\n\n\nlogical, integer, double, character, complex, and raw.\n\n\nInteger and double vectors are collectively known as numeric vectors.\n\n\n\n\n\nLists. Like atomic vectors, lists are objects that contain elements\n\nelements within a list may be atomic vectors\nelements within a list may also be other lists; that is lists can contain other lists\n\n\n\nOne difference between atomic vectors and lists: homogeneous vs. heterogeneous elements\n\natomic vectors are homogeneous: all elements within atomic vector must be of the same type\nlists can be heterogeneous: e.g., one element can be an integer and another element can be character\n\nProblem with this classification:\n\nNot conceptually intutive\nTechnically, lists are a type of vector, but people often think of atomic vectors and lists as fundamentally different things\n\nClassification used by Ben Skinner:\n\ndata type: logical, numeric (integer and double), character, etc.\ndata structure: vector, list, matrix, etc."
  },
  {
    "objectID": "01-week1-workbook.html#using-r-functions",
    "href": "01-week1-workbook.html#using-r-functions",
    "title": "Week 1 Workbook",
    "section": "Using R functions",
    "text": "Using R functions\nWhat are functions\n\nFunctions are pre-written bits of code that accomplish some task.\nFunctions generally follow three sequential steps:\n\n\ntake in an input object(s)\n\nprocess the input.\n\nreturn (A) a new object or (B) a visualizatoin (e.g., plot)\n\n\nFor example, sum() function calculates sum of elements in a vector\n\n\n\ninput. takes in a vector of elements (numeric or logical)\n\nprocessing. Calculates the sum of elements\n\nreturn. Returns numeric vector of length=1; value is sum of input vector\n\n\nCodesum(c(1,2,3))\n\n[1] 6\n\nCodetypeof(sum(c(1,2,3))) # type of object created by sum()\n\n[1] \"double\"\n\nCodelength(sum(c(1,2,3))) # length of object created by sum()\n\n[1] 1\n\n\nFunction syntax\nComponents of a function\n\nfunction name (e.g., sum(), length(), seq())\nfunction arguments\n\nInputs that the function takes, which determine what function does\n\ncan be vectors, data frames, logical statements, etc.\n\n\nIn “function call” you specify values to assign to these function arguments\n\ne.g., sum(c(1,2,3))\n\n\n\nSeparate arguments with a comma ,\n\ne.g., seq(10,15)\n\n\n\n\n\nExample: the sequence function, seq()\n\n\n\nCodeseq(10,15)\n\n[1] 10 11 12 13 14 15\n\n\nFunction syntax: More on function arguments\nUsually, function arguments have names\n\ne.g., the seq() function includes the arguments from, to, by\n\nwhen you call the function, you need to assign values to these arguments; but you usually don’t have to specify the name of the argument\n\n\n\nCodeseq(from=10, to=20, by=2)\n\n[1] 10 12 14 16 18 20\n\nCodeseq(10,20,2)\n\n[1] 10 12 14 16 18 20\n\n\n\nMany function arguments have “default values”, set by whoever wrote the function\n\nif you don’t specify a value for that argument, the default value is inserted\ne.g., partial list of default values for seq(): seq(from=1, to=1, by=1)\n\n\n\n\nCodeseq()\n\n[1] 1\n\nCodeseq(to=10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nCodeseq(10) # R assigned value of 10 to \"to\" rather than \"from\" or \"by\"\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nHelp files for functions\n\n\nTo see help file on a function, type ?function_name without parentheses\n\nCode?sum\n?seq\n\n\n\nContents of help files\n\n\nDescription. What the function does\n\nUsage. Syntax, including default values for arguments\n\nArguments. Description of function arguments\n\nDetails. Details and idiosyncracies of about how the function works.\n\nValue. What (object) the function “returns”\n\ne.g., sum() returns vector of length 1 whose value is sum of input vector\n\n\n\nReferences. Additional reading\n\nSee Also. Related functions\n\nExamples. Examples of function in action\nBottom of help file identifies the package the function comes from"
  },
  {
    "objectID": "01-week1-workbook.html#what-is-quarto",
    "href": "01-week1-workbook.html#what-is-quarto",
    "title": "Week 1 Workbook",
    "section": "What is Quarto",
    "text": "What is Quarto\n\nQuarto documents embed R code, output associated with R code, and text into one document\nAn Quarto document is a “‘Living’ document that updates every time you compile [”Render”] it”\nQuarto documents have the extension .qmd\n\nCan think of them as text files with the extension .qmd rather than .txt\n\n\nAt top of .qmd file you specify the “output” style, which dictates what kind of formatted document will be created\n\ne.g., html_document or pdf_document (this document was created with revealjs)\n\n\nWhen you compile [“Render”] a .qmd file, the resulting formatted document can be an HTML document, a PDF document, an MS Word document, or many other types"
  },
  {
    "objectID": "01-week1-workbook.html#creating-quarto-documents",
    "href": "01-week1-workbook.html#creating-quarto-documents",
    "title": "Week 1 Workbook",
    "section": "Creating Quarto documents",
    "text": "Creating Quarto documents\nDo this with a partner\nApproach for creating a Quarto document.\n\nPoint-and-click from within RStudio\n\nClick on File >> New File >> Quarto Document… >> choose HTML >> click OK\n\nOptional: add title (this is not the file name, just what appears at the top of document)\nOptional: add author name\n\n\nSave the .qmd file; File >> Save As\n\nAny file name\nRecommend you save it in same folder you saved this lecture\n\n\n“Render” the entire .qmd file\n\nPoint-and-click OR shortcut: Cmd/Ctrl + Shift + k"
  },
  {
    "objectID": "01-week1-workbook.html#creating-and-formatting-quarto-documents",
    "href": "01-week1-workbook.html#creating-and-formatting-quarto-documents",
    "title": "Week 1 Workbook",
    "section": "Creating and Formatting Quarto Documents",
    "text": "Creating and Formatting Quarto Documents\nTake a few minutes and have you peruse the Quarto site to build familiarity (I still access it all the time when I forget how to do specific things)\nI especially want you to take some time to peruse documents on YAML headers:\n\nHTML documents\nPDF documents\nWord documents\nWebstes\nRevealJS slides\nBeamer slides\nPowerpoint Slides"
  },
  {
    "objectID": "08-week8-workbook.html",
    "href": "08-week8-workbook.html",
    "title": "Week 8 Workbook",
    "section": "",
    "text": "Loading required package: Matrix\n\n\nThis is lavaan 0.6-15\nlavaan is FREE software! Please report any bugs.\n\n\n\nAttaching package: 'lavaan'\n\n\nThe following object is masked from 'package:psych':\n\n    cor2cov\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.3\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()           masks psych::%+%()\n✖ ggplot2::alpha()         masks psych::alpha()\n✖ dplyr::arrange()         masks plyr::arrange()\n✖ lubridate::as.difftime() masks base::as.difftime()\n✖ purrr::compact()         masks plyr::compact()\n✖ dplyr::count()           masks plyr::count()\n✖ lubridate::date()        masks base::date()\n✖ dplyr::desc()            masks plyr::desc()\n✖ tidyr::expand()          masks Matrix::expand()\n✖ dplyr::failwith()        masks plyr::failwith()\n✖ dplyr::filter()          masks stats::filter()\n✖ dplyr::group_rows()      masks kableExtra::group_rows()\n✖ dplyr::id()              masks plyr::id()\n✖ lubridate::intersect()   masks base::intersect()\n✖ dplyr::lag()             masks stats::lag()\n✖ dplyr::mutate()          masks plyr::mutate()\n✖ tidyr::pack()            masks Matrix::pack()\n✖ dplyr::rename()          masks plyr::rename()\n✖ lubridate::setdiff()     masks base::setdiff()\n✖ dplyr::summarise()       masks plyr::summarise()\n✖ dplyr::summarize()       masks plyr::summarize()\n✖ lubridate::union()       masks base::union()\n✖ tidyr::unpack()          masks Matrix::unpack()"
  },
  {
    "objectID": "08-week8-workbook.html#tables-in-r",
    "href": "08-week8-workbook.html#tables-in-r",
    "title": "Week 8 Workbook",
    "section": "Tables in R",
    "text": "Tables in R\nIn this workshop, we will directly cover 3 different use cases, while a few others will be included in supplementary materials.\nPersonally, I favor non-automated tools, so we will cover the following packages:\n- kable + kableExtra (html and LaTeX)\n- papaja\nUsing these packages will build on earlier tutorials using tidyr, dplyr, workflow, and purrr and round out our discuss on data presentation using ggplot2.\nFor less flexible but more accessible tables see:\n- apaTable\n- sjPlot\n- corrplot"
  },
  {
    "objectID": "08-week8-workbook.html#important-tools",
    "href": "08-week8-workbook.html#important-tools",
    "title": "Week 8 Workbook",
    "section": "Important Tools",
    "text": "Important Tools\nAlthough it doesn’t cover all models, the broom and broom.mixed family of packages will provide easy to work with estimates of nearly all types of models and will also provide the model terms that are ideal for most APA tables, including estimates, standard errors, and confidence intervals.\nlavaan models are slightly more complicated, but it’s actually relatively easy to deal with them (and how to extract their terms), assuming that you understand the models you are running."
  },
  {
    "objectID": "08-week8-workbook.html#data",
    "href": "08-week8-workbook.html#data",
    "title": "Week 8 Workbook",
    "section": "Data",
    "text": "Data\nThe data we’re going to use are from the teaching sample from the German Socioeconomic Panel Study. These data have been pre-cleaned (see earlier workshop on workflow and creating guidelines for tips).\nThe data we’ll use fall into three categories:\n1. Personality trait composites: Negative Affect, Positive Affect, Self-Esteem, CESD Depression, and Optimism. These were cleaned, reversed coded, and composited prior to being included in this final data set.\n2. Outcomes: Moving in with a partner, marriage, divorce, and child birth. These were cleaned, coded as 1 (occurred) or 0 (did not occur) according to whether an outcome occurred for each individual or not after each possible measured personality year. Moreover, people who experienced these outcomes prior to the target personality year are excluded.\n3. Covariates: Age, gender (0 = male, 1 = female, education (0 = high school or below, 1 = college, 2 = higher than college), gross wages, self-rated health, smoking (0 = never smoked 1 = ever smoked), exercise, BMI, religion, parental education, and parental occupational prestige (ISEI). Each of these were composited for all available data up to the measured personality years.\n\nCode(gsoep <- read_csv(\"week8-data.csv\"))"
  },
  {
    "objectID": "08-week8-workbook.html#set-up-the-data-frame",
    "href": "08-week8-workbook.html#set-up-the-data-frame",
    "title": "Week 8 Workbook",
    "section": "Set up the data frame",
    "text": "Set up the data frame\nFirst, we’ll use some of what we learned in the purrr workshop to set ourselves up to be able to create these tables easily, using group_by() and nest() to create nested data frames for our target personality + outcome combinations. To do this, we’ll also use what you learned about filter() and mutate().\n\nCodegsoep_nested1 <- gsoep %>%\n  filter(Outcome == \"chldbrth\") %>%\n  group_by(Trait, Outcome) %>%\n  nest() %>%\n  ungroup()\n\n\nNext, let’s pause and see what we have. We now have a data frame with 3 columns (Outcome, Trait, and data) and 4 rows. The data column is of class list, meaning it’s a “list column” that contains a tibble in each cell. This means that we can use purrr functions to run operations on each of these data frames individually but without having to copy and paste the same operation multiple times for each model we want to run.\n\nCodegsoep_nested1"
  },
  {
    "objectID": "08-week8-workbook.html#run-models",
    "href": "08-week8-workbook.html#run-models",
    "title": "Week 8 Workbook",
    "section": "Run Models",
    "text": "Run Models\nTo run the models, I like to write short functions that are easier to read than including a local function within a call to purrr::map(). Here, we’re just going to write a simple function to predict child birth from personality.\n\nCodemod1_fun <- function(d){\n  d$o_value <- factor(d$o_value)\n  glm(o_value ~ p_value, data = d, family = binomial(link = \"logit\"))\n}\n\ngsoep_nested1 <- gsoep_nested1 %>%\n  mutate(m = map(data, mod1_fun))\n\n\nNow, when we look at the nested frame, we see an additional column, which is also a list, but this column contains <glm> objects rather than tibbles.\n\nCodegsoep_nested1"
  },
  {
    "objectID": "08-week8-workbook.html#get-key-terms",
    "href": "08-week8-workbook.html#get-key-terms",
    "title": "Week 8 Workbook",
    "section": "Get Key Terms",
    "text": "Get Key Terms\nNow that we have the models, we want to get our key terms. I’m a big fan of using the function tidy from the broom package to do this. Bonus because it plays nicely with purrr. Double bonus because it will give us confidence intervals, which I generally prefer over p-values and standard erorrs because I find them more informative.\n\nCodegsoep_nested1 <- gsoep_nested1 %>%\n  mutate(tidy = map(m, ~tidy(., conf.int = T)))\ngsoep_nested1\n\n\n\n  \n\n\n\nNote that what I’ve used here is a local function, meaning that I’ve used the notation ~function(., arguments). The tilda tells R we want a local function, and the . tells R to use the mapped m column as the function input.\nNow we have a fifth column, which is a list column called tidy that contains a tibble, just like the data column."
  },
  {
    "objectID": "08-week8-workbook.html#creating-a-table",
    "href": "08-week8-workbook.html#creating-a-table",
    "title": "Week 8 Workbook",
    "section": "Creating a Table",
    "text": "Creating a Table\nNow we are finally ready to create a table! I’m going to use kable + kableExtra to do this in steps.\nFirst, we’ll unnest the tidy column from our data frame. Before doing so, we will drop the data and m columns because they’ve done their work for now.\n\nCodetidy1 <- gsoep_nested1 %>%\n  select(-data, -m) %>%\n  unnest(tidy)\ntidy1\n\n\n\n  \n\n\n\nAs you can see, we now have lots of information about our model terms, which are already nicely indexed by Outcome and Trait combinations.\nBut before we’re ready to create a table, we have to make a few considerations:\n\nWhat is our target term? In this case “p_value” which is the change in log odds associated with a 1 unit increase/decrease in p_value.\n\nHow will we denote significance? In this case, we’ll use confidence intervals whose signs match. We’ll then bold these terms for our table.\nWhat is the desired final structure for the table? I’d like columns for Trait, estimate (b), and confidence intervals (CI) formatted to two decimal places and bolded if significant. I’d also like a span header denoting that the outcome measure is child birth.\n\nWhat is our target term?\nIn this case “p_value” which is the change in log odds associated with a 1 unit increase/decrease in p_value.\n\nCodetidy1 <- tidy1 %>% filter(term == \"p_value\")\ntidy1\n\n\n\n  \n\n\n\nHow will we denote significance?\nIn this case, we’ll use confidence intervals whose signs match. We’ll then bold these terms for our table.\n\nCodetidy1 <- tidy1 %>% mutate(sig = ifelse(sign(conf.low) == sign(conf.high), \"sig\", \"ns\"))\ntidy1\n\n\n\n  \n\n\n\nWhat is the desired final structure for the table?\nI’d like columns for Trait, estimate (b), and confidence intervals (CI) formatted to two decimal places and bolded if significant. I’d also like a span header denoting that the outcome measure is child birth.\nBefore we do this, though, we need to convert our log odds to odds ratios, using the exp() function.\n\nCodetidy1 <- tidy1 %>%\n  mutate_at(vars(estimate, conf.low, conf.high), exp) \ntidy1\n\n\n\n  \n\n\n\nNow, we can format them.\n\nCodetidy1 <- tidy1 %>%\n  mutate_at(vars(estimate, conf.low, conf.high), ~sprintf(\"%.2f\", .)) \ntidy1\n\n\n\n  \n\n\n\nsprintf() is my favorite base R formatting function. “%.2f” means I’m asking it to take a floating point number and include 2 digits after the “.” and 0 before. We can now see that the estimate, conf.low, and conf.high columns are of class <chr> instead of <dbl>.\nBut now we need to create our confidence intervals.\n\nCodetidy1 <- tidy1 %>%\n  mutate(CI = sprintf(\"[%s, %s]\", conf.low, conf.high))\ntidy1\n\n\n\n  \n\n\n\nAnd bold the significant confidence intervals and estimates.\n\nCodetidy1 <- tidy1 %>%\n  mutate_at(vars(estimate, CI), ~ifelse(sig == \"sig\", sprintf(\"<strong>%s</strong>\", .), .))\ntidy1\n\n\n\n  \n\n\n\nThis reads as “for both the estimate and the CI columns, if the sig column is equal to”sig”, then let’s format it as bold using html. Otherwise, let’s leave it alone.” And indeed, we can see that the final result formats 3/4 rows.\nThankfully, these can be achieved without considerable reshaping of the data, which is why we’ve started here, so we’re almost done. We just need to get rid of some unnecessary columnns.\n\nCodetidy1 <- tidy1 %>%\n  select(Trait, OR = estimate, CI)\n\n\nBecause we just have one target term and one outcome, we don’t need those columns, so we’re just keeping Trait, OR, which I renamed as such within in the select call, and CI."
  },
  {
    "objectID": "08-week8-workbook.html#kabling-a-table",
    "href": "08-week8-workbook.html#kabling-a-table",
    "title": "Week 8 Workbook",
    "section": "Kabling a Table",
    "text": "Kabling a Table\nNow let’s kable. You’ve likely used the kable() function from the knitr before. It’s a very useful and simple function in most occasions.\n\nCodekable(tidy1)\n\n\n\n\n Trait \n    OR \n    CI \n  \n\n\n OP \n    <strong>1.68</strong> \n    <strong>[1.50, 1.88]</strong> \n  \n\n DEP \n    <strong>1.15</strong> \n    <strong>[1.07, 1.24]</strong> \n  \n\n NegAff \n    <strong>1.11</strong> \n    <strong>[1.01, 1.22]</strong> \n  \n\n PA \n    <strong>1.97</strong> \n    <strong>[1.77, 2.20]</strong> \n  \n\n SE \n    1.09 \n    [0.97, 1.22] \n  \n\n\n\n\nIt will automatically generate the html code needed to create a table. But if we look closely at the code, it gives us some gobbledigook where we inputted html, so we need a way around that. I’m also going to throw in kable_styling(full_width = F) from the kableExtra package to help out here. It’s not doing much, but it will make the formatted table print in your Viewer.\n\nCodekable(tidy1, escape = F) %>%\n  kable_classic(full_width = F, html_font = \"Times\")\n\n\n\n\n Trait \n    OR \n    CI \n  \n\n\n OP \n    1.68 \n    [1.50, 1.88] \n  \n\n DEP \n    1.15 \n    [1.07, 1.24] \n  \n\n NegAff \n    1.11 \n    [1.01, 1.22] \n  \n\n PA \n    1.97 \n    [1.77, 2.20] \n  \n\n SE \n    1.09 \n    [0.97, 1.22] \n  \n\n\n\n\nMuch better. But this still doesn’t look like an APA table, so let’s keep going.\n\nAPA tables usually write out long names for our predictors, so let’s change those first. I’m going to create a reference tibble and use mapvalues() from the plyr function for this.\n\n\n\nCodep_names <- tibble(\n  old = c(\"NegAff\", \"PA\", \"SE\", \"OP\", \"DEP\"),\n  new = c(\"Negative Affect\", \"Positive Affect\", \"Self-Esteem\", \"Optimism\", \"Depression\")\n)\n\ntidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(., escape = F) %>%\n  kable_classic(full_width = F, html_font = \"Times\")\n\n\n\n\n Trait \n    OR \n    CI \n  \n\n\n Negative Affect \n    1.11 \n    [1.01, 1.22] \n  \n\n Positive Affect \n    1.97 \n    [1.77, 2.20] \n  \n\n Self-Esteem \n    1.09 \n    [0.97, 1.22] \n  \n\n Optimism \n    1.68 \n    [1.50, 1.88] \n  \n\n Depression \n    1.15 \n    [1.07, 1.24] \n  \n\n\n\n\n\nThe alignment of the columns isn’t quite right. Let’s fix that. We’ll change the trait to right justified and b and CI to centered.\n\n\n\nCodetidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(.\n        , escape = F\n        , align = c(\"r\", \"c\", \"c\")\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\")\n\n\n\n\n Trait \n    OR \n    CI \n  \n\n\n Negative Affect \n    1.11 \n    [1.01, 1.22] \n  \n\n Positive Affect \n    1.97 \n    [1.77, 2.20] \n  \n\n Self-Esteem \n    1.09 \n    [0.97, 1.22] \n  \n\n Optimism \n    1.68 \n    [1.50, 1.88] \n  \n\n Depression \n    1.15 \n    [1.07, 1.24] \n  \n\n\n\n\n\nBut we’re still missing our span header. There’s a great function in the kableExtra package for this add_header_above. This function takes a named vector as argument, where the elements of the vector refer to the number of columns the named element should span.\n\n\nCodetidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(.\n        , escape = F\n        , align = c(\"r\", \"c\", \"c\")\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\") %>%\n  add_header_above(c(\" \" = 1, \"Birth of a Child\" = 2))\n\n\n\n\n\n\nBirth of a Child\n\n\n Trait \n    OR \n    CI \n  \n\n\n\n Negative Affect \n    1.11 \n    [1.01, 1.22] \n  \n\n Positive Affect \n    1.97 \n    [1.77, 2.20] \n  \n\n Self-Esteem \n    1.09 \n    [0.97, 1.22] \n  \n\n Optimism \n    1.68 \n    [1.50, 1.88] \n  \n\n Depression \n    1.15 \n    [1.07, 1.24] \n  \n\n\n\n\nNote that what the \" \" = 1 does is skip the Trait column. This is very useful because it let’s us not have a span header over every column.\n\nAPA style requires we note how we denote significance and have a title, so let’s add a title and a note.\n\n\n\nCodetidy1 %>%\n  mutate(Trait = mapvalues(Trait, from = p_names$old, to = p_names$new),\n         Trait = factor(Trait, levels = p_names$new)) %>%\n  arrange(Trait) %>%\n  kable(.\n        , escape = F\n        , align = c(\"r\", \"c\", \"c\")\n        , caption = \"<strong>Table 1</strong><br><em>Estimated Personality-Outcome Associations</em>\"\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\")%>%\n  add_header_above(c(\" \" = 1, \"Birth of a Child\" = 2)) %>%\n  add_footnote(label = \"Bold values indicate terms whose confidence intervals did not overlap with 0\", notation = \"none\")\n\n\n\n\nTable 1Estimated Personality-Outcome Associations\n\n \n\n\nBirth of a Child\n\n\n Trait \n    OR \n    CI \n  \n\n\n\n Negative Affect \n    1.11 \n    [1.01, 1.22] \n  \n\n Positive Affect \n    1.97 \n    [1.77, 2.20] \n  \n\n Self-Esteem \n    1.09 \n    [0.97, 1.22] \n  \n\n Optimism \n    1.68 \n    [1.50, 1.88] \n  \n\n Depression \n    1.15 \n    [1.07, 1.24] \n  \n\n\n\n Bold values indicate terms whose confidence intervals did not overlap with 0\n\n\n\n\nWe did it!"
  },
  {
    "objectID": "08-week8-workbook.html#a-quick-note-html-v.-latex",
    "href": "08-week8-workbook.html#a-quick-note-html-v.-latex",
    "title": "Week 8 Workbook",
    "section": "A Quick Note: HTML v. LaTeX",
    "text": "A Quick Note: HTML v. LaTeX\nWhen creating tables, I prefer using HTML when I need the resulting tables to be in HTML and LaTeX when I can place the tables in a PDF. The syntax using kable and kableExtra is the same with the following exceptions:\n\nThe format argument in kable() would need to be set as format = \"latex\".\n\nThe chunk option for a table to render would need to be set as {r, results = 'asis'}.\n\nBolding would need to be done as \\\\textbf{}, rather than the html <strong></strong> tag.\n\nWhen using collapse_rows(), which we’ll get to later, you’d want to set the latex_hline argument to latex_hline = \"none\"."
  },
  {
    "objectID": "08-week8-workbook.html#data-1",
    "href": "08-week8-workbook.html#data-1",
    "title": "Week 8 Workbook",
    "section": "Data",
    "text": "Data\nFor this example, we’ll use a different data set from the SOEP that have the item level personality data. The code below reads it in, recodes, and reverse scores it.\n\nCoderev_code <- c(\"Big5__A_coarse\", \"Big5__C_lazy\", \"Big5__E_reserved\", \"Big5__N_dealStress\")\n(gsoep2 <- read_csv(\"week8-data-2.csv\") %>%\n   mutate_at(vars(matches(\"Big5\")), function(x) {x[x < 0] <- NA; x}) %>%\n   mutate_at(vars(matches(\"LifeEvent\")), ~mapvalues(., seq(-7,1), c(rep(NA, 5), 0, NA, NA, 1), warn_missing = F)) %>%\n    mutate_at(\n    vars(all_of(rev_code))\n    , ~as.numeric(reverse.code(., keys = -1, mini = 1, maxi = 7))\n    ))\n\n\n\n  \n\n\n\nNow we’ll change it to long format for easier cleaning and reshaping\n\nCodegsoep2_long <- gsoep2 %>%\n  pivot_longer(\n    cols = c(starts_with(\"Big5\"), starts_with(\"Life\"))\n    , names_to = c(\"category\", \"item\")\n    , names_sep = \"__\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  )"
  },
  {
    "objectID": "08-week8-workbook.html#data-cleaning",
    "href": "08-week8-workbook.html#data-cleaning",
    "title": "Week 8 Workbook",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nNow, we need to reshape the Big Five data for lavaan\nIf you learn nothing else from this class, I want you to learn this trick I use to run the same model multiple times on different variables\nWe are going to run a second-order latent growth model of the BFI-S, which has three items per Big Five trait\nThe item numbers are arbitrary but consistent across traits (3) as are the years (2005, 2009, 2013)\nSo if we separate the trait information from the item and year, then we can run identical models across traits\n\n\nCodegsoep2_lavaan <- gsoep2_long %>% \n  filter(category == \"Big5\") %>%\n  separate(item, c(\"trait\", \"item\"), sep = \"_\") %>%\n  group_by(Procedural__SID, year, trait) %>%\n  mutate(item = mapvalues(item, unique(item), 1:n())) %>%\n  ungroup() %>%\n  pivot_wider(\n    names_from = c(\"item\", \"year\")\n    , names_prefix = \"I\"\n    , values_from = \"value\"\n  )\ngsoep2_lavaan"
  },
  {
    "objectID": "08-week8-workbook.html#second-order-latent-growth-model",
    "href": "08-week8-workbook.html#second-order-latent-growth-model",
    "title": "Week 8 Workbook",
    "section": "Second-Order Latent Growth Model",
    "text": "Second-Order Latent Growth Model\n\nBecause of how we set up the data, we only have to write this out once. This is extra helpful for SEM because the model syntax can get really long and it would be time consuming to have to write this out separately for each of the Big Five\n\n\nCodemod <- '\n  W1 =~ NA*I1_2005 + lambda1*I1_2005 + lambda2*I2_2005 + lambda3*I3_2005\n  W2 =~ NA*I1_2009 + lambda1*I1_2009 + lambda2*I2_2009 + lambda3*I3_2009\n  W3 =~ NA*I1_2013 + lambda1*I1_2013 + lambda2*I2_2013 + lambda3*I3_2013\n  \n  i =~ 1*W1 + 1*W2 + 1*W3\n  s =~ -1*W1 + 0*W2 + 1*W3\n  \n  ## intercepts\n  I1_2005 ~ t1*1\n  I1_2009 ~ t2*1\n  I1_2013 ~ t3*1\n  \n  I2_2005 ~ t1*1\n  I2_2009 ~ t2*1\n  I2_2013 ~ t3*1\n  \n  I3_2005 ~ t1*1\n  I3_2009 ~ t2*1\n  I3_2013 ~ t3*1\n  \n  ## correlated residuals across time\n  I1_2005 ~~ I1_2009 + I1_2013\n  I1_2009 ~~ I1_2013\n  I2_2005 ~~ I2_2009 + I2_2013\n  I2_2009 ~~ I2_2013\n  I3_2005 ~~ I3_2009 + I3_2013\n  I3_2009 ~~ I3_2013\n  \n  ## latent variable intercepts\n  W1 ~ 0*1\n  W2 ~ 0*1\n  W3 ~ 0*1\n  \n  #model constraints for effect coding\n  ## loadings must average to 1\n  lambda1 == 3 - lambda2 - lambda3\n  ## means must average to 0\n  t1 == 0 - t2 - t3\n  '\n\n\nNow, we’ll write a little function that will run the model syntax across cross-sections of our data for each Big Five trait\n\nCodelavaan_fun <- function(d){\n  m <- growth(\n    mod\n    , data = d\n    , missing = \"fiml\"\n  )\n  return(m)\n}\n\n\nNow, we can create those cross-sections using tidyr::nest() and then run the model using purrr::map(). Having the trait information as rows in our data frame and using list columns let’s us separate out the trait information from the item/year information we need to run the models.\n\nCodegsoep_nested2 <- gsoep2_lavaan %>%\n  group_by(category, trait) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(m = map(data, lavaan_fun))\ngsoep_nested2"
  },
  {
    "objectID": "08-week8-workbook.html#extracting-results",
    "href": "08-week8-workbook.html#extracting-results",
    "title": "Week 8 Workbook",
    "section": "Extracting Results",
    "text": "Extracting Results\n\nSummaries of SEM models are going to spit out a MUCH longer summary of results than the typical models we run and extract information from using broom/broom.mixed.\nWhen I do SEM, I do make a table that includes all of these parameters for each model and put it in the supplement\nBut for the paper itself, we only want a subset of key model terms across all traits\n\n\nCodesummary(gsoep_nested2$m[[1]])\n\nlavaan 0.6.15 ended normally after 81 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        44\n  Number of equality constraints                    14\n\n  Number of observations                         16716\n  Number of missing patterns                        38\n\nModel Test User Model:\n                                                      \n  Test statistic                               284.272\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  W1 =~                                               \n    I1_2005 (lmb1)    1.053    0.001 1312.911    0.000\n    I2_2005 (lmb2)    0.960    0.001  810.689    0.000\n    I3_2005 (lmb3)    0.987    0.001 1142.417    0.000\n  W2 =~                                               \n    I1_2009 (lmb1)    1.053    0.001 1312.911    0.000\n    I2_2009 (lmb2)    0.960    0.001  810.689    0.000\n    I3_2009 (lmb3)    0.987    0.001 1142.417    0.000\n  W3 =~                                               \n    I1_2013 (lmb1)    1.053    0.001 1312.911    0.000\n    I2_2013 (lmb2)    0.960    0.001  810.689    0.000\n    I3_2013 (lmb3)    0.987    0.001 1142.417    0.000\n  i =~                                                \n    W1                1.000                           \n    W2                1.000                           \n    W3                1.000                           \n  s =~                                                \n    W1               -1.000                           \n    W2                0.000                           \n    W3                1.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n .I1_2005 ~~                                          \n   .I1_2009           0.029    0.010    3.073    0.002\n   .I1_2013           0.046    0.011    4.130    0.000\n .I1_2009 ~~                                          \n   .I1_2013           0.052    0.010    5.131    0.000\n .I2_2005 ~~                                          \n   .I2_2009           0.775    0.025   31.118    0.000\n   .I2_2013           0.685    0.029   23.661    0.000\n .I2_2009 ~~                                          \n   .I2_2013           0.924    0.028   32.973    0.000\n .I3_2005 ~~                                          \n   .I3_2009           0.148    0.011   13.042    0.000\n   .I3_2013           0.139    0.013   11.011    0.000\n .I3_2009 ~~                                          \n   .I3_2013           0.179    0.012   15.087    0.000\n  i ~~                                                \n    s                -0.004    0.005   -0.811    0.417\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .I1_2005   (t1)    0.443    0.089    4.952    0.000\n   .I1_2009   (t2)   -0.027    0.005   -4.944    0.000\n   .I1_2013   (t3)   -0.416    0.089   -4.650    0.000\n   .I2_2005   (t1)    0.443    0.089    4.952    0.000\n   .I2_2009   (t2)   -0.027    0.005   -4.944    0.000\n   .I2_2013   (t3)   -0.416    0.089   -4.650    0.000\n   .I3_2005   (t1)    0.443    0.089    4.952    0.000\n   .I3_2009   (t2)   -0.027    0.005   -4.944    0.000\n   .I3_2013   (t3)   -0.416    0.089   -4.650    0.000\n   .W1                0.000                           \n   .W2                0.000                           \n   .W3                0.000                           \n    i                 5.841    0.007  867.282    0.000\n    s                 0.405    0.088    4.607    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .I1_2005           0.429    0.011   39.517    0.000\n   .I2_2005           1.849    0.028   65.804    0.000\n   .I3_2005           0.719    0.013   55.538    0.000\n   .I1_2009           0.478    0.012   40.829    0.000\n   .I2_2009           2.021    0.031   66.239    0.000\n   .I3_2009           0.754    0.014   55.362    0.000\n   .I1_2013           0.433    0.011   39.096    0.000\n   .I2_2013           2.094    0.032   64.482    0.000\n   .I3_2013           0.684    0.013   52.839    0.000\n   .W1                0.144    0.018    7.990    0.000\n   .W2                0.205    0.009   22.139    0.000\n   .W3                0.084    0.018    4.696    0.000\n    i                 0.380    0.009   44.413    0.000\n    s                 0.057    0.010    5.808    0.000\n\nConstraints:\n                                               |Slack|\n    lambda1 - (3-lambda2-lambda3)                0.000\n    t1 - (0-t2-t3)                               0.000\n\n\nSo what I do is to write a little function. I try to write it so that it can be used across multiple qualitatively different models if I’m running different (e.g., see second example later). To do that, I have (1) the function and (2) a tibble called terms that contains the paths I want to extract.\n\nCodeterms <- tribble(\n  ~path,     ~new,\n  \"i~1\", \"Intercept\",\n  \"s~1\", \"Slope\",\n  \"i~~i\", \"Intercept Variance\",\n  \"s~~s\", \"Slope Variance\",\n  \"i~~s\", \"Intercept-Slope Covariance\"\n)\n\nextract_fun <- function(m){\n  parameterEstimates(m) %>%\n    data.frame() %>%\n    unite(path, lhs, op, rhs, sep = \"\") %>%\n    filter(path %in% terms$path) %>%\n    left_join(terms) %>%\n    select(term = new, est, se, ci.lower, ci.upper, pvalue)\n}\n\n\nNow we can run that function to extract the results we care about\n\nCodegsoep_nested2 <- gsoep_nested2 %>%\n  mutate(summary = map(m, extract_fun))\ngsoep_nested2"
  },
  {
    "objectID": "08-week8-workbook.html#formatting-results",
    "href": "08-week8-workbook.html#formatting-results",
    "title": "Week 8 Workbook",
    "section": "Formatting Results",
    "text": "Formatting Results\nThe way that we need to format the data is pretty standard, so below are three functions I use when I need to format results from lavaan for putting into a table\n\nCoderound_fun <- function(x) if(!is.na(x)) if(abs(x) > .01) sprintf(\"%.2f\", x) else sprintf(\"%.3f\", x) else \"\"\npround_fun <- function(x) if(!is.na(x)) if(abs(x) > .01) sprintf(\"%.2f\", x) else if(x >= .001) sprintf(\"%.3f\", x) else \"&lt; .001\" else \"\"\n\nformat_fun <- function(d){\n  d %>%\n    mutate(sig = ifelse(pvalue < .05, \"sig\", \"ns\")) %>%\n    rowwise() %>%\n    mutate_at(vars(est, ci.lower, ci.upper), round_fun) %>%\n    mutate_at(vars(pvalue), pround_fun) %>%\n    ungroup() %>%\n    mutate(CI = sprintf(\"[%s,%s]\", ci.lower, ci.upper)) %>%\n    mutate_at(vars(est, CI, pvalue), ~ifelse(sig == \"sig\" & !is.na(sig), sprintf(\"<strong>%s</strong>\", .), .)) \n}\n\n\nNow let’s run that and see what it looks like\n\nCodegsoep_tab <- gsoep_nested2 %>%\n  select(-data, -m) %>%\n  unnest(summary) %>%\n  format_fun() \ngsoep_tab\n\n\n\n  \n\n\n\nWe’re really close to being ready to make the table using kable, but we need to do a little reshaping and to remove some columns first.\n\nCodegsoep_tab <- gsoep_tab %>%\n  select(-ci.lower, -ci.upper, -sig, -category, -pvalue) %>%\n  pivot_wider(\n    names_from = \"trait\"\n    , names_glue = \"{trait}_{.value}\"\n    , values_from = c(\"est\", \"CI\")\n  ) \ngsoep_tab\n\n\n\n  \n\n\n\nUnfortunately, pivot_wider() doesn’t really let us have too much control over the order of the columns, so we need to move them around so the estimates and CI’s for each trait are next to each other. There are many ways to do this, but I’m showing you two below:\n\nCodeord <- paste(rep(c(\"E\", \"A\", \"C\", \"N\", \"O\"), each = 2), rep(c(\"est\", \"CI\"), times = 5), sep = \"_\")\ngsoep_tab <- gsoep_tab %>%\n  select(term, all_of(ord)) %>%\n  # select(starts_with(\"E\"), starts_with(\"A\"), starts_with(\"C\"), starts_with(\"N\"), starts_with(\"O\"))\n  mutate(term = factor(term, terms$new)) %>%\n  arrange(term)\ngsoep_tab"
  },
  {
    "objectID": "08-week8-workbook.html#kabling-the-table",
    "href": "08-week8-workbook.html#kabling-the-table",
    "title": "Week 8 Workbook",
    "section": "Kabling the Table",
    "text": "Kabling the Table\nNow, let’s go ahead and create the table using kable. Like before, we’re going to use add_header_above(), but I’m going to show you a trick that I use for it to make specifying it a little easier. - add_header_above() takes in a named vector as input where the values have to sum to the number of columns in the data frame (11)\n\nCodehdr <- c(1, rep(2,5))\nnames(hdr) <- c(\" \", \"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Neuroticism\", \"Openness\")\ngsoep_tab %>%\n  kable(.\n        , escape = F\n        , align = c(\"r\", rep(\"c\", 10))\n        , col.names = c(\"Term\", rep(c(\"<em>est.</em>\", \"CI\"), times = 5))\n        , caption = \"<strong>Table 2</strong><br><em>Big Five Personality Trait Trajectories from Latent Growth Models</em>\"\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\") %>%\n  add_header_above(hdr) %>%\n  add_footnote(label = \"Bold values indicate terms p < .05\", notation = \"none\")\n\n\n\n\nTable 2Big Five Personality Trait Trajectories from Latent Growth Models\n\n \n\n\nExtraversion\nAgreeableness\nConscientiousness\nNeuroticism\nOpenness\n\n\n Term \n    est. \n    CI \n    est. \n    CI \n    est. \n    CI \n    est. \n    CI \n    est. \n    CI \n  \n\n\n\n Intercept \n     \n     \n     \n     \n    5.84 \n    [5.83,5.85] \n     \n     \n     \n     \n  \n\n Intercept \n    4.83 \n    [4.81,4.84] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n    5.41 \n    [5.39,5.42] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n     \n     \n    4.51 \n    [4.49,4.53] \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n    3.84 \n    [3.82,3.86] \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n    0.40 \n    [0.23,0.58] \n     \n     \n     \n     \n  \n\n Slope \n    0.10 \n    [0.04,0.16] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n    0.49 \n    [0.34,0.65] \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.40 \n    [-0.57,-0.24] \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n    -0.95 \n    [-1.05,-0.86] \n     \n     \n  \n\n Intercept Variance \n     \n     \n     \n     \n    0.38 \n    [0.36,0.40] \n     \n     \n     \n     \n  \n\n Intercept Variance \n    0.69 \n    [0.66,0.71] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept Variance \n     \n     \n    0.35 \n    [0.33,0.37] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept Variance \n     \n     \n     \n     \n     \n     \n     \n     \n    0.66 \n    [0.63,0.69] \n  \n\n Intercept Variance \n     \n     \n     \n     \n     \n     \n    0.69 \n    [0.66,0.72] \n     \n     \n  \n\n Slope Variance \n     \n     \n     \n     \n    0.06 \n    [0.04,0.08] \n     \n     \n     \n     \n  \n\n Slope Variance \n    0.07 \n    [0.05,0.10] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope Variance \n     \n     \n    0.03 \n    [0.009,0.05] \n     \n     \n     \n     \n     \n     \n  \n\n Slope Variance \n     \n     \n     \n     \n     \n     \n     \n     \n    0.07 \n    [0.04,0.10] \n  \n\n Slope Variance \n     \n     \n     \n     \n     \n     \n    0.07 \n    [0.04,0.10] \n     \n     \n  \n\n Intercept-Slope Covariance \n     \n     \n     \n     \n    -0.004 \n    [-0.02,0.006] \n     \n     \n     \n     \n  \n\n Intercept-Slope Covariance \n    0.004 \n    [-0.010,0.02] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept-Slope Covariance \n     \n     \n    0.009 \n    [-0.003,0.02] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept-Slope Covariance \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.007 \n    [-0.02,0.009] \n  \n\n Intercept-Slope Covariance \n     \n     \n     \n     \n     \n     \n    0.02 \n    [0.006,0.04] \n     \n     \n  \n\n\n\n Bold values indicate terms p < .05"
  },
  {
    "objectID": "08-week8-workbook.html#second-order-latent-growth-model-slopes-as-predictors",
    "href": "08-week8-workbook.html#second-order-latent-growth-model-slopes-as-predictors",
    "title": "Week 8 Workbook",
    "section": "Second-Order Latent Growth Model: Slopes as Predictors",
    "text": "Second-Order Latent Growth Model: Slopes as Predictors\nNow, let’s do a second example that is more complex and uses the intercepts and slopes we estimated to predict life outcomes. To do so, we first need to clean and prep the life event data and merge it back with our Big Five item-level data.\nNote that in this case, we’re using long data two ways: for the Big Five traits and for the life events. In other words, we’re getting every combination of the two and are using inner_join() to make that happen!\n\nCodegsoep2_lavaan <- gsoep2_long %>%\n  filter(category == \"LifeEvent\") %>%\n  group_by(Procedural__SID, Demographic__DOB, item) %>%\n  summarize(le_value = max(value)) %>%\n  ungroup() %>%\n  mutate(age = 2005 - Demographic__DOB - 45) %>%\n  rename(le = item) %>%\n  inner_join(gsoep2_lavaan)\ngsoep2_lavaan"
  },
  {
    "objectID": "08-week8-workbook.html#data-cleaning-1",
    "href": "08-week8-workbook.html#data-cleaning-1",
    "title": "Week 8 Workbook",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nNow let’s create the nested data frame.\n\nCodegsoep_nested3 <- gsoep2_lavaan %>%\n  drop_na(trait, le) %>%\n  group_by(trait, le) %>%\n  nest() %>%\n  ungroup()\ngsoep_nested3"
  },
  {
    "objectID": "08-week8-workbook.html#model-setup",
    "href": "08-week8-workbook.html#model-setup",
    "title": "Week 8 Workbook",
    "section": "Model Setup",
    "text": "Model Setup\nWe’re really just building on the second-order latent growth models we already ran. We already ran those and know that we have specified the models correctly / have enough variance to use the slopes and intercept to predict other things. So now we just need to add the new regression paths to that syntax to be able to run the models.\nAs with the model we specified before, by having the life event data long, we are able to do this just one time and have that work for all the life events. So using this trick, we can run 50 unique models with one set of model syntax!\nNote that we have to modify the function that calls growth() because we have a categorical outcome that needs a different estimator.\n\nCodemod2 <- '\nle_value ~ i + age\nle_value ~ s\n'\n\nmod2 <- paste(mod, mod2, collapse = \"\\n\")\n\nlavaan_fun <- function(d){\n  m <- growth(\n    mod2\n    , data = d\n    , ordered = \"le_value\"\n    , estimator = 'WLSMV'\n    , missing = \"pairwise\"\n    , parallel = \"multicore\"\n  )\n  return(m)\n}\n\ngsoep_nested3 <- gsoep_nested3 %>%\n  mutate(m = map(data, lavaan_fun))\n# saveRDS(gsoep_nested3, \"gsoep_nested3.RDS\")"
  },
  {
    "objectID": "08-week8-workbook.html#extract-results",
    "href": "08-week8-workbook.html#extract-results",
    "title": "Week 8 Workbook",
    "section": "Extract Results",
    "text": "Extract Results\nRemember how I said that the reason we created the terms tibble was because it would make extract_fun() more flexible / portable? Let’s see that in action. We’ll create a new one that lists only the key additional terms in this model and use that to extract those estimates.\n\nCodegsoep_nested3 <- readRDS(\"gsoep_nested3.RDS\")\nterms <- tribble(\n  ~path,     ~new,\n  \"le_value~i\", \"Intercept\",\n  \"le_value~s\", \"Slope\",\n  \"le_value~age\", \"Age\"\n)\n\ngsoep_nested3 <- gsoep_nested3 %>%\n  mutate(summary = map(m, extract_fun))"
  },
  {
    "objectID": "08-week8-workbook.html#formatting-results-1",
    "href": "08-week8-workbook.html#formatting-results-1",
    "title": "Week 8 Workbook",
    "section": "Formatting Results",
    "text": "Formatting Results\nNow we’re ready to format the results using the same steps as before. Note that I’m combining all them here because we’ve already stepped through them slowly: 1. Remove other list columns and unnest() 2. Format the results using format_fun() 3. Remove unnecessary columns and pivot_wider() 4. Change the order of the columns 5. Factor the terms to help us arrange the rows 6. Sort the rows by Life Event and then term\n\nCodeord <- paste(rep(c(\"E\", \"A\", \"C\", \"N\", \"O\"), each = 2), rep(c(\"est\", \"CI\"), times = 5), sep = \"_\")\ngsoep_tab2 <- gsoep_nested3 %>%\n  select(-data, -m) %>%\n  unnest(summary) %>%\n  format_fun() %>%\n  select(-ci.lower, -ci.upper, -sig, -pvalue) %>%\n  pivot_wider(\n    names_from = \"trait\"\n    , names_glue = \"{trait}_{.value}\"\n    , values_from = c(\"est\", \"CI\")\n  ) %>%\n  select(le, term, all_of(ord)) %>%\n  # select(starts_with(\"E\"), starts_with(\"A\"), starts_with(\"C\"), starts_with(\"N\"), starts_with(\"O\"))\n  mutate(term = factor(term, terms$new)) %>%\n  arrange(le, term)\ngsoep_tab2"
  },
  {
    "objectID": "08-week8-workbook.html#kabling-the-table-1",
    "href": "08-week8-workbook.html#kabling-the-table-1",
    "title": "Week 8 Workbook",
    "section": "Kabling the Table",
    "text": "Kabling the Table\nNow we’re ready to kable the table! This looks identical to before, but I’m going to show you one more trick using kableExtra::group_rows. Rather than having to count the rows and adding a bunch of manual calls, I always just use the data frame to count for me to avoid errors. Then, I can create the table and use a for loop to tack on all the grouped rows using that reference data frame. Much more flexible and less error prone!\n\nCodehdr <- c(1, rep(2,5))\nnames(hdr) <- c(\" \", \"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Neuroticism\", \"Openness\")\nrs <- gsoep_tab2 %>% group_by(le) %>% tally() %>% \n    mutate(end = cumsum(n), start = lag(end) + 1, start = ifelse(is.na(start), 1, start))\ngsoep_tab2_kable <- gsoep_tab2 %>%\n  select(-le) %>%\n  kable(.\n        , escape = F\n        , align = c(\"r\", rep(\"c\", 10))\n        , col.names = c(\"Term\", rep(c(\"<em>est.</em>\", \"CI\"), times = 5))\n        , caption = \"<strong>Table 3</strong><br><em>Big Five Personality Trait Trajectory Associations with Life Events from Latent Growth Models</em>\"\n        ) %>%\n  kable_classic(full_width = F, html_font = \"Times\") %>%\n  add_header_above(hdr) %>%\n  add_footnote(label = \"Bold values indicate terms p < .05. \", notation = \"none\")\n\nfor(i in 1:nrow(rs)){\n    gsoep_tab2_kable <- gsoep_tab2_kable %>% kableExtra::group_rows(rs$le[i], rs$start[i], rs$end[i])\n}\ngsoep_tab2_kable\n\n\n\n\nTable 3Big Five Personality Trait Trajectory Associations with Life Events from Latent Growth Models\n\n \n\n\nExtraversion\nAgreeableness\nConscientiousness\nNeuroticism\nOpenness\n\n\n Term \n    est. \n    CI \n    est. \n    CI \n    est. \n    CI \n    est. \n    CI \n    est. \n    CI \n  \n\n\nChldBrth\n\n Intercept \n     \n     \n     \n     \n    0.12 \n    [0.05,0.18] \n     \n     \n     \n     \n  \n\n Intercept \n    0.005 \n    [-0.04,0.05] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n    0.09 \n    [-0.02,0.20] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.09 \n    [-0.14,-0.04] \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n    -0.04 \n    [-0.09,0.02] \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n    0.21 \n    [-0.31,0.72] \n     \n     \n     \n     \n  \n\n Slope \n    -0.05 \n    [-0.44,0.35] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n    -1.19 \n    [-3.39,1.00] \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n     \n     \n    0.006 \n    [-0.41,0.42] \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n    0.07 \n    [-0.38,0.53] \n     \n     \n  \n\n Age \n     \n     \n     \n     \n    -0.03 \n    [-0.04,-0.03] \n     \n     \n     \n     \n  \n\n Age \n    -0.03 \n    [-0.04,-0.03] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n    -0.03 \n    [-0.04,-0.03] \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.03 \n    [-0.04,-0.03] \n  \n\n Age \n     \n     \n     \n     \n     \n     \n    -0.03 \n    [-0.04,-0.03] \n     \n     \n  \nChldMvOut\n\n Intercept \n     \n     \n     \n     \n    0.15 \n    [0.09,0.21] \n     \n     \n     \n     \n  \n\n Intercept \n    0.02 \n    [-0.02,0.06] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n    0.05 \n    [-0.06,0.16] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n     \n     \n    0.009 \n    [-0.04,0.05] \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n    0.01 \n    [-0.03,0.06] \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n    -0.87 \n    [-1.37,-0.37] \n     \n     \n     \n     \n  \n\n Slope \n    -0.51 \n    [-0.87,-0.15] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n    -1.62 \n    [-4.11,0.86] \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.65 \n    [-1.07,-0.23] \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n    0.09 \n    [-0.26,0.44] \n     \n     \n  \n\n Age \n     \n     \n     \n     \n    0.005 \n    [0.003,0.008] \n     \n     \n     \n     \n  \n\n Age \n    0.005 \n    [0.003,0.008] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n    0.005 \n    [0.003,0.008] \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n     \n     \n     \n     \n     \n     \n    0.005 \n    [0.003,0.008] \n  \n\n Age \n     \n     \n     \n     \n     \n     \n    0.005 \n    [0.003,0.008] \n     \n     \n  \nDadDied\n\n Intercept \n     \n     \n     \n     \n    0.005 \n    [-0.06,0.07] \n     \n     \n     \n     \n  \n\n Intercept \n    -0.02 \n    [-0.07,0.03] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n    -0.07 \n    [-0.16,0.03] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.04 \n    [-0.09,0.02] \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n    0.04 \n    [-0.02,0.10] \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n    -0.43 \n    [-0.90,0.04] \n     \n     \n     \n     \n  \n\n Slope \n    -0.57 \n    [-1.00,-0.15] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n    -0.92 \n    [-2.73,0.88] \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.66 \n    [-1.13,-0.20] \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n    0.01 \n    [-0.40,0.43] \n     \n     \n  \n\n Age \n     \n     \n     \n     \n    -0.004 \n    [-0.007,-0.001] \n     \n     \n     \n     \n  \n\n Age \n    -0.004 \n    [-0.007,-0.001] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n    -0.004 \n    [-0.007,-0.001] \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.004 \n    [-0.007,-0.001] \n  \n\n Age \n     \n     \n     \n     \n     \n     \n    -0.004 \n    [-0.007,-0.001] \n     \n     \n  \nDivorce\n\n Intercept \n     \n     \n     \n     \n    0.12 \n    [0.03,0.21] \n     \n     \n     \n     \n  \n\n Intercept \n    0.05 \n    [-0.01,0.12] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n    -0.07 \n    [-0.20,0.05] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n     \n     \n    0.03 \n    [-0.03,0.10] \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n    0.10 \n    [0.02,0.19] \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n    0.35 \n    [-0.35,1.04] \n     \n     \n     \n     \n  \n\n Slope \n    -0.35 \n    [-0.84,0.15] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n    1.01 \n    [-1.14,3.17] \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.15 \n    [-0.66,0.36] \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n    -0.67 \n    [-1.38,0.04] \n     \n     \n  \n\n Age \n     \n     \n     \n     \n    -0.007 \n    [-0.01,-0.003] \n     \n     \n     \n     \n  \n\n Age \n    -0.007 \n    [-0.01,-0.003] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n    -0.007 \n    [-0.01,-0.003] \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.007 \n    [-0.01,-0.003] \n  \n\n Age \n     \n     \n     \n     \n     \n     \n    -0.007 \n    [-0.01,-0.003] \n     \n     \n  \nMarried\n\n Intercept \n     \n     \n     \n     \n    0.10 \n    [0.04,0.17] \n     \n     \n     \n     \n  \n\n Intercept \n    0.05 \n    [0.000,0.09] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n    0.03 \n    [-0.08,0.14] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n     \n     \n    0.04 \n    [-0.01,0.08] \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n    -0.002 \n    [-0.06,0.06] \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n    0.21 \n    [-0.30,0.72] \n     \n     \n     \n     \n  \n\n Slope \n    -0.06 \n    [-0.46,0.34] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n    -1.26 \n    [-3.47,0.95] \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.003 \n    [-0.40,0.40] \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n    0.05 \n    [-0.39,0.49] \n     \n     \n  \n\n Age \n     \n     \n     \n     \n    -0.03 \n    [-0.03,-0.03] \n     \n     \n     \n     \n  \n\n Age \n    -0.03 \n    [-0.03,-0.03] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n    -0.03 \n    [-0.03,-0.03] \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.03 \n    [-0.03,-0.03] \n  \n\n Age \n     \n     \n     \n     \n     \n     \n    -0.03 \n    [-0.03,-0.03] \n     \n     \n  \nMomDied\n\n Intercept \n     \n     \n     \n     \n    0.08 \n    [0.01,0.15] \n     \n     \n     \n     \n  \n\n Intercept \n    -0.02 \n    [-0.07,0.02] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n    0.05 \n    [-0.05,0.16] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n     \n     \n    0.05 \n    [-0.005,0.09] \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n    0.03 \n    [-0.02,0.09] \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n    -0.82 \n    [-1.37,-0.26] \n     \n     \n     \n     \n  \n\n Slope \n    -0.35 \n    [-0.73,0.03] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n    -1.07 \n    [-3.04,0.89] \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.02 \n    [-0.38,0.34] \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n    -0.06 \n    [-0.47,0.35] \n     \n     \n  \n\n Age \n     \n     \n     \n     \n    0.007 \n    [0.005,0.010] \n     \n     \n     \n     \n  \n\n Age \n    0.007 \n    [0.005,0.010] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n    0.007 \n    [0.005,0.010] \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n     \n     \n     \n     \n     \n     \n    0.007 \n    [0.005,0.010] \n  \n\n Age \n     \n     \n     \n     \n     \n     \n    0.007 \n    [0.005,0.010] \n     \n     \n  \nMoveIn\n\n Intercept \n     \n     \n     \n     \n    0.06 \n    [-0.01,0.13] \n     \n     \n     \n     \n  \n\n Intercept \n    0.07 \n    [0.02,0.12] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n    0.02 \n    [-0.10,0.13] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n     \n     \n    0.05 \n    [-0.003,0.09] \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n    0.08 \n    [0.02,0.15] \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n    0.33 \n    [-0.22,0.88] \n     \n     \n     \n     \n  \n\n Slope \n    -0.61 \n    [-1.12,-0.09] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n    -1.29 \n    [-3.65,1.07] \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.22 \n    [-0.67,0.22] \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n    -0.28 \n    [-0.79,0.23] \n     \n     \n  \n\n Age \n     \n     \n     \n     \n    -0.03 \n    [-0.03,-0.03] \n     \n     \n     \n     \n  \n\n Age \n    -0.03 \n    [-0.03,-0.03] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n    -0.03 \n    [-0.03,-0.03] \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.03 \n    [-0.03,-0.03] \n  \n\n Age \n     \n     \n     \n     \n     \n     \n    -0.03 \n    [-0.03,-0.03] \n     \n     \n  \nNewPart\n\n Intercept \n     \n     \n     \n     \n    -0.13 \n    [-0.22,-0.04] \n     \n     \n     \n     \n  \n\n Intercept \n    0.17 \n    [0.10,0.24] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n    -0.09 \n    [-0.22,0.03] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n     \n     \n    0.13 \n    [0.06,0.20] \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n    -0.03 \n    [-0.11,0.05] \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n    0.71 \n    [0.24,1.18] \n     \n     \n     \n     \n  \n\n Slope \n    0.29 \n    [-0.22,0.79] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n    1.21 \n    [-0.23,2.65] \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n     \n     \n    0.10 \n    [-0.47,0.67] \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n    -0.14 \n    [-0.71,0.44] \n     \n     \n  \n\n Age \n     \n     \n     \n     \n    -0.04 \n    [-0.04,-0.03] \n     \n     \n     \n     \n  \n\n Age \n    -0.04 \n    [-0.04,-0.03] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n    -0.04 \n    [-0.04,-0.03] \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.04 \n    [-0.04,-0.03] \n  \n\n Age \n     \n     \n     \n     \n     \n     \n    -0.04 \n    [-0.04,-0.03] \n     \n     \n  \nPartDied\n\n Intercept \n     \n     \n     \n     \n    0.02 \n    [-0.07,0.10] \n     \n     \n     \n     \n  \n\n Intercept \n    0.04 \n    [-0.02,0.10] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n    0.10 \n    [-0.11,0.30] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.09 \n    [-0.15,-0.02] \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n    0.10 \n    [0.02,0.18] \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n    -0.29 \n    [-0.98,0.40] \n     \n     \n     \n     \n  \n\n Slope \n    0.48 \n    [-0.04,0.99] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n    3.32 \n    [-1.64,8.27] \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.11 \n    [-0.61,0.38] \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n    -0.23 \n    [-0.79,0.33] \n     \n     \n  \n\n Age \n     \n     \n     \n     \n    0.03 \n    [0.03,0.03] \n     \n     \n     \n     \n  \n\n Age \n    0.03 \n    [0.03,0.03] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n    0.03 \n    [0.03,0.03] \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n     \n     \n     \n     \n     \n     \n    0.03 \n    [0.03,0.03] \n  \n\n Age \n     \n     \n     \n     \n     \n     \n    0.03 \n    [0.03,0.03] \n     \n     \n  \nSepPart\n\n Intercept \n     \n     \n     \n     \n    0.04 \n    [-0.02,0.11] \n     \n     \n     \n     \n  \n\n Intercept \n    0.09 \n    [0.04,0.14] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n    -0.11 \n    [-0.24,0.03] \n     \n     \n     \n     \n     \n     \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n     \n     \n    0.07 \n    [0.02,0.12] \n  \n\n Intercept \n     \n     \n     \n     \n     \n     \n    0.09 \n    [0.02,0.16] \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n    0.64 \n    [0.07,1.21] \n     \n     \n     \n     \n  \n\n Slope \n    -0.14 \n    [-0.52,0.24] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n    1.92 \n    [-1.05,4.90] \n     \n     \n     \n     \n     \n     \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n     \n     \n    0.010 \n    [-0.38,0.40] \n  \n\n Slope \n     \n     \n     \n     \n     \n     \n    -0.77 \n    [-1.38,-0.15] \n     \n     \n  \n\n Age \n     \n     \n     \n     \n    -0.02 \n    [-0.02,-0.02] \n     \n     \n     \n     \n  \n\n Age \n    -0.02 \n    [-0.02,-0.02] \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n    -0.02 \n    [-0.02,-0.02] \n     \n     \n     \n     \n     \n     \n  \n\n Age \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.02 \n    [-0.02,-0.02] \n  \n\n Age \n     \n     \n     \n     \n     \n     \n    -0.02 \n    [-0.02,-0.02] \n     \n     \n  \n\n\n\n Bold values indicate terms p < .05."
  },
  {
    "objectID": "08-week8-workbook.html#kabling-lots-of-tables",
    "href": "08-week8-workbook.html#kabling-lots-of-tables",
    "title": "Week 8 Workbook",
    "section": "Kabling lots of tables",
    "text": "Kabling lots of tables\nThe code below is a pretty chunk of code I use to generate tables for each model I run with all the model parameter estimates.\n\nCodeall_term_tab_fun <- function(m, trait){\n  long_trait <- mapvalues(trait, p_names$old, p_names$new, warn_missing = F)\n  cap <- sprintf(\"<strong>Table SX</strong><br><em>Second Order Latent Growth Models of %s</em>\", long_trait)\n  note <- \"Bold values indicate estimates were significant at p < .05. est. = unstandardized estimate. \"\n  tab <- parameterEstimates(m, standardized = T) %>%\n    data.frame() %>%\n    format_fun() %>%\n    select(lhs:est, CI, pvalue) %>%\n    kable(.\n          , format = \"html\"\n          , escape = F\n          , align = c(\"r\", \"c\", \"l\", \"l\", rep(\"c\", 3))\n          , col.names = c(\"LHS\", \"op\", \"RHS\", \"label\", \"est.\", \"CI\", \"<em>p</em>\")\n          , caption = cap\n          ) %>%\n    kable_classic(full_width = F, html_font = \"Times\") %>%\n    footnote(note)\n  # save_kable(tab, file = sprintf(\"results/tables/all-terms/%s.html\", trait))\n  return(tab)\n}\n\ngsoep_nested2 <- gsoep_nested2 %>%\n  mutate(all_term_tab = map2(m, trait, all_term_tab_fun))"
  },
  {
    "objectID": "08-week8-workbook.html#set-up-the-data-frame-1",
    "href": "08-week8-workbook.html#set-up-the-data-frame-1",
    "title": "Week 8 Workbook",
    "section": "Set up the data frame",
    "text": "Set up the data frame\nFirst, we’ll use some of what we learned in the purrr workshop to set ourselves up to be able to create these tables easily, using group_by() and nest() to create nested data frames for our target personality + outcome combinations. To do this, we’ll also use what you learned about filter() and mutate().\n\nCodeoutcomes <- tribble(\n  ~old, ~new,\n  \"chldbrth\", \"Child Birth\"\n  , \"divorced\", \"Divorced\"\n  , \"married\", \"Married\"\n  , \"mvInPrtner\", \"Move in with Partner\"\n)\n\ngsoep_nested4 <- gsoep %>%\n  mutate(age = age - 45) %>%\n  group_by(Trait, Outcome) %>%\n  nest() %>%\n  ungroup()\ngsoep_nested4"
  },
  {
    "objectID": "08-week8-workbook.html#run-models-1",
    "href": "08-week8-workbook.html#run-models-1",
    "title": "Week 8 Workbook",
    "section": "Run Models",
    "text": "Run Models\nNow, we’ll run the models predicting outcomes from personality x age interactions (and their lower order terms)\n\nCodemod2_fun <- function(d){\n  d$o_value <- factor(d$o_value)\n  glm(o_value ~ p_value*age, data = d, family = binomial(link = \"logit\"))\n}\n\ngsoep_nested4 <- gsoep_nested4 %>%\n  mutate(m = map(data, mod2_fun))"
  },
  {
    "objectID": "08-week8-workbook.html#generating-model-predictions",
    "href": "08-week8-workbook.html#generating-model-predictions",
    "title": "Week 8 Workbook",
    "section": "Generating Model Predictions",
    "text": "Generating Model Predictions\nThe next step is to get predictions from the model. It is also good practice to always get standard errors and/or confidence intervals of the estimates. Thankfully, with lm() and glm(), this is easy. Lavaan will be covered briefly and more complex model forms are covered in my data visualization class.\n\nCodeglm_pred_fun <- function(m){\n  rng_p <- range(m$model$p_value, na.rm = T)\n  frame <- crossing(\n    age = c(-15, 0, 15)\n    , p_value = seq(rng_p[1], rng_p[2], length.out = 30)\n  )\n  \n  pred <- predict(m, newdata = frame, se.fit = T)[1:2]\n  frame <- frame %>% \n    mutate(fit = pred$fit\n           , se = pred$se.fit\n           , lower = fit - 1.96*se\n           , upper = fit + 1.96*se\n           ) %>%\n    mutate_at(vars(fit, lower, upper), exp)\n}\n\n\nNow we run the predictions\n\nCodegsoep_nested4 <- gsoep_nested4 %>%\n  mutate(pred = map(m, glm_pred_fun))\ngsoep_nested4"
  },
  {
    "objectID": "08-week8-workbook.html#plot-the-predictions",
    "href": "08-week8-workbook.html#plot-the-predictions",
    "title": "Week 8 Workbook",
    "section": "Plot the Predictions",
    "text": "Plot the Predictions\nOne of the greatest strengths of list-columns and using purrr is what I think of as the data accordion. With list-columns, I can unnest() whatever level the models were estimated at and then reaggregate using whatever combinations / groups make the most sense.\nSo here, for example, we ran models for all Big Five trait (5) x life outcome (10) combinations. But having 50 separate plots would be a little silly, so we’d much rather create one for each outcome. But I’m not going to write that code 10 times because that’d be a waste. So instead if I reaggregate the data, I can then use a function + purrr to generate each of the plots.\n\nCodegsoep_plot4 <- gsoep_nested4 %>%\n  select(-data, -m) %>%\n  unnest(pred) %>%\n  group_by(Outcome) %>%\n  nest() %>%\n  ungroup()\ngsoep_plot4\n\n\n\n  \n\n\n\nWhen we write code for plots in ggplot, we have a lot of things that we end up writing over and over, especially for theme elements. To get around this, I use this little function to modify all of my theme elements in a single line.\n\nCodemy_theme <- function(){\n  theme_classic() + \n  theme(\n    legend.position = \"bottom\"\n    , legend.title = element_text(face = \"bold\", size = rel(1))\n    , legend.text = element_text(face = \"italic\", size = rel(1))\n    , axis.text = element_text(face = \"bold\", size = rel(1.1), color = \"black\")\n    , axis.title = element_text(face = \"bold\", size = rel(1.2))\n    , panel.grid.major = element_line(color = \"grey90\", linewidth = .2)\n    , plot.title = element_text(face = \"bold\", size = rel(1.2), hjust = .5)\n    , plot.subtitle = element_text(face = \"italic\", size = rel(1.2), hjust = .5)\n    , strip.text = element_text(face = \"bold\", size = rel(1.1), color = \"white\")\n    , strip.background = element_rect(fill = \"black\")\n    )\n}\n\n\nLike I mentioned, once we reaggregate the data, we can then write a function to generate the plot for each outcome separately. Let’s start by building the basic code for the plot. We need it to: 1. Have x = personality, y = predicted values, separate lines for moderator levels, and separate panels for different traits 2. A line indicating personality-outcome associations (again separately across age groups) and a ribbon with the confidence interval around the prediction\n\nCodeplot_fun <- function(d){\n  d %>%\n    mutate(age_fac = factor(age, c(-15, 0, 15), c(\"30\", \"45\", \"60\"))) %>%\n    ggplot(aes(x = p_value, y = fit, color = age_fac)) +\n    geom_ribbon(\n      aes(ymin = lower, ymax = upper, fill = age_fac)\n      , alpha = .4\n    ) + \n    geom_line() + \n    facet_wrap(~Trait, scales = \"free_x\") + \n    my_theme()\n}\n\n\nWith our reaggregated data, we can easily just run the plots and view the results. This is getting there, but let’s make some additional modifications to get them publication ready.\n\nCodegsoep_plot4 <- gsoep_plot4 %>%\n  mutate(p = map(data, plot_fun))\ngsoep_plot4\n\n\n\n  \n\n\nCodegsoep_plot4$p[[1]]\n\n\n\n\nLet’s add: 1. full personality trait names 2. the outcome in the title 3. better axis and other labels\n\nCodeplot_fun <- function(d, outcome){\n  out <- mapvalues(outcome, outcomes$old, outcomes$new, warn_missing = F)\n  d %>%\n    mutate(age_fac = factor(age, c(-15, 0, 15), c(\"30\", \"45\", \"60\"))\n           , Trait = factor(Trait, p_names$old, p_names$new)) %>%\n    ggplot(aes(x = p_value, y = fit, color = age_fac)) +\n    geom_ribbon(\n      aes(ymin = lower, ymax = upper, fill = age_fac)\n      , alpha = .4\n    ) + \n    geom_line() + \n    labs(\n      x = \"Personality Trait Level\"\n      , y = \"Predicted Value (CI)\"\n      , color = \"Age\"\n      , fill = \"Age\"\n      , title = sprintf(\"%s: Personality x Age Interaction\", out)\n    ) + \n    facet_wrap(~Trait, scales = \"free_x\") + \n    my_theme() + \n    theme(\n      legend.position = c(.8, .25)\n      \n    )\n  # ggsave(filename = sprintf(\"results/figures/%s.png\", outcome), width = 6, height = 6)\n  # ggsave(filename = sprintf(\"results/figures/%s.pdf\", outcome), width = 6, height = 6)\n}\n\n\nNow, we can run these and get the results.\n\nCodegsoep_plot4 <- gsoep_plot4 %>%\n  mutate(p = map2(data, Outcome, plot_fun))\ngsoep_plot4\n\n\n\n  \n\n\nCodegsoep_plot4$p[[1]]"
  },
  {
    "objectID": "08-week8-workbook.html#lavaan-trajectories",
    "href": "08-week8-workbook.html#lavaan-trajectories",
    "title": "Week 8 Workbook",
    "section": "Lavaan Trajectories",
    "text": "Lavaan Trajectories\nLavaan is trickier. Because of the way residual variance, etc. is calculted in SEM, it’s much less straightforward to estimate standard errors and confidence intervals of predictions (not to mention that lavaan just doesn’t make predictions easy). So instead, I just use some basic matrix algebra and bootstrapping to get these. If you aren’t comfortable with basic matrix algebra, you can just write a little function that uses terms individually.\n\nCodelavaan_pred_fun <- function(m){\n  coef <- coef(gsoep_nested3$m[[1]])[c(\"i~1\", \"s~1\")]\n  b <- bootstrapLavaan(m, parallel = \"multicore\", R = 100)\n  frame <- tibble(intercept = 1, wave = seq(-1,1, length.out = 50))\n  pred_boot <- function(x,y) bind_cols(frame, pred = as.vector(as.matrix(frame) %*% c(x,y)))\n  frame <- frame %>%\n    mutate(pred = as.vector(as.matrix(frame) %*% coef)) %>%\n    left_join(\n      tibble(sample = 1:100, pred = map2(b[,\"i~1\"], b[,\"s~1\"], pred_boot)) %>%\n        unnest(pred) %>%\n        group_by(wave) %>%\n        summarize(lower = quantile(pred, probs = .025)\n                  , upper = quantile(pred, probs = .975))\n    )\n}\n\ngsoep_nested2 <- gsoep_nested2 %>%\n  mutate(pred = map(m, lavaan_pred_fun))\n# saveRDS(gsoep_nested2, file = \"gsoep_nested2.RDS\")\n\n\n\n\n\nOnce we’ve estimated the predictions, we can graph them just like we did for the previous example. (Again, a full discussion of plotting is beyond the scope of this course and is covered for a whole quarter in my data visualization class instead.)\n\nCodegsoep_nested2 %>%\n  select(-data, -m, -summary) %>%\n  unnest(pred) %>%\n  ggplot(aes(x = wave + 1, y = pred)) + \n    geom_ribbon(\n      aes(ymin = lower, ymax = upper, fill = trait)\n      , alpha = .4\n    ) + \n    geom_line() + \n    scale_y_continuous(limits = c(1,7), breaks = seq(1,7,1)) + \n    scale_x_continuous(limits = c(-.25,2.25), breaks = 0:2, labels = c(2005, 2009, 2013)) + \n    labs(\n        x = \"Personality Trait Level\"\n        , y = \"Predicted Value (CI)\"\n        , title = sprintf(\"Personality Trajectories\")\n      ) + \n    facet_wrap(~trait, scales = \"free_x\") + \n    my_theme() + \n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "05-week5-slides.html",
    "href": "05-week5-slides.html",
    "title": "Week 5 Slides",
    "section": "",
    "text": "Welcome to Week 5! This week, we’ll talk about data documentation, readable codebooks, and data cleaning workflows."
  },
  {
    "objectID": "ps1-week1.html",
    "href": "ps1-week1.html",
    "title": "Problem Set Week 1",
    "section": "",
    "text": "Due Date: Monday, October 9, 12:01 AM PST.\nDownload your problem set for week 1 below or on Canvas.\nAnswers can be found here."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr. Emorie D. Beck (she/her/hers)\nE-mail: edbeck@ucdavis.edu\nOffice: 268J\nOffice Hours:\n- Drop-in hour: Tuesdays 11a-12p\n- Or by appointment: edbeck@ucdavis.edu\n\n\n\n\n\n\nDr. Beck is an Assistant Professor in the Psychology Department specializing in personality psychology. She received her PhD in Social and Personality Psychology from Washington University in St. Louis in 2020 and her BA (with honors) from Brown University in 2016. \nDr. Beck’s research focuses around the question of what personality is. Definitions have big consequences for how we measure personality, what those measures predict both short- and long-term, how personality is thought to change, and more. One way of doing this is to focus on different levels of aggregation. Thus, she studies how to understand the personality of an individual relative to only themself, relative to some others, and relative to all others. To do so, she uses a mix of methods, including experience sampling methods, passive sensing, survey data, panel data, cognitive tests, and more measured across time intervals from moments to years along with an array of statistical approaches, including time series analysis, multilevel / hierarchical modeling, machine learning, network psychometrics, structural equation modeling, and more. For example, Dr. Beck has been working to build personalized machine learning prediction of behaviors, experiences, and more, finding that we can predict behaviors and experiences better when we don’t assume that people have the same antecedents of the behaviors and experiences.  Instead, people have unique antecedents, which could have consequences for how to change or intervene upon behaviors and experiences. In other work, Dr. Beck uses longitudinal panel data across multiple continents to answer questions about what personality traits predict over time. For example, she recently examined personality trait and well-being predictors of later dementia diagnoses and neuropathology measures after death, finding that personality traits are strong predictors of dementia diagnosis but have a much more complex relationship with neuropathology measures."
  },
  {
    "objectID": "ps7-week7.html",
    "href": "ps7-week7.html",
    "title": "Problem Set Week 7",
    "section": "",
    "text": "Due Date: Monday, November 20, 12:01 AM PST.\nDownload your problem set for week 7 below or on Canvas.\nAnswers will be posted after the due date."
  },
  {
    "objectID": "final-project.html",
    "href": "final-project.html",
    "title": "Final Projects",
    "section": "",
    "text": "Due at 11:59 PM PST on November 19, 2022 on Canvas\n1-2 page (single spaced) proposal\n\nShort background (What are you hacking?)\nApproach (How you do plan to hack this? What “product” will you producing?)\nMotivation (Why are you hacking this particular problem / issue / question / procedure?)\nChallenges and barriers (what do you struggle with with bringing your project to life; are there specific barriers?)\n\n\nYou can also download this document here\n\n\nThe goal of this course is to build a set of tools for cleaning and managing your data. As discussed in class, a big part of that is to create a workflow for your research that is efficient, clear, and minimizes errors. The final project in the course aims to provide an opportunity for you to flesh out what that means for you and research. There’s no right or wrong way to build a sustainable workflow for yourself.\n\n\n\nThe project is open-ended, but some examples are:\n\nA preregistration template for your kind of data / research\nA quarto / Rmarkdown / R script / R Project template for the typical structure of research projects\nA data cleaning pipeline / template for your kind of data / research\nA set of functions meant to help you improve your research workflow\nWriting clear documentation for a new or previous data collection\nA data cleaning or analysis script that draws on different course concepts (e.g., directory structures, codebooks, functions, iteration / purrr, etc.)\nA “checks” / procedure checklist that documents robustness tests and aims to improve the accuracy and efficiency of your workflow\nAnything else that draws on things you learned in class and focuses on data cleaning, R, data management, procedures, GitHub, etc.\n\n\n\n\nFor the final project, you will be creating something to help improve your workflow. You can think of it sort of like a chance to “hack” your own work. I recommend choosing something you’ve been wanting to do but haven’t had time to prioritize and using this as an excuse to do so.\nAs mentioned above, this could be anything from finishing up cleaning some gnarly data you’ve had for awhile, redoing your documentation for a study that’s already been collected (or you plan to collect), building templates or procedures, etc. What I want from you for this project is something that is: * Useful to you, both short- and long-term * Challenging but doable (you’re better at this than you think!) * Something you feel proud of"
  },
  {
    "objectID": "04-week4-slides.html",
    "href": "04-week4-slides.html",
    "title": "Week 4 Slides",
    "section": "",
    "text": "Welcome to Week 4! This week, we’ll talk about data documentation, readable codebooks, and data cleaning workflows."
  },
  {
    "objectID": "02-week2-workbook.html",
    "href": "02-week2-workbook.html",
    "title": "Week 2 Workbook",
    "section": "",
    "text": "Codelibrary(knitr)\nlibrary(psych)\nlibrary(emo)\nlibrary(plyr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "02-week2-workbook.html#why-reproducibility-and-values",
    "href": "02-week2-workbook.html#why-reproducibility-and-values",
    "title": "Week 2 Workbook",
    "section": "Why reproducibility AND values?",
    "text": "Why reproducibility AND values?\n\n\nThe definition of reproducibility is somewhat debated\n\n“‘Reproducibility’ refers to instances in which the original researcher’s data and computer codes are used to regenerate the results”\n\n“‘Reproducibility’ refers to independent researchers arriving at the same results using their own data and methods”\n\n\n\nBut regardless of what definition you choose, reproducibility starts with a commitment in research to be\nclear\ntransparent\nhonest\nthorough"
  },
  {
    "objectID": "02-week2-workbook.html#why-reproducibility-and-values-1",
    "href": "02-week2-workbook.html#why-reproducibility-and-values-1",
    "title": "Week 2 Workbook",
    "section": "Why reproducibility AND values?",
    "text": "Why reproducibility AND values?\n\nReproducibility is ethical.\nWhen I post a project, I pour over my code for hours, adding comments, rendering to multiple formats, trying to flag locations in online materials in the mansucript, etc.\nI am trying to prevent errors, but I am also trying to make sure that other people know what I did, especially if I did make errors\nReproducible research is also equitable.\nA reproducible research workflow can be downloaded by another person as a starting point, providing tools to other researchers who may not have the same access to education and resources as you"
  },
  {
    "objectID": "02-week2-workbook.html#where-should-we-reproducible",
    "href": "02-week2-workbook.html#where-should-we-reproducible",
    "title": "Week 2 Workbook",
    "section": "Where should we reproducible?",
    "text": "Where should we reproducible?\n\nPlanning\n\nStudy planning and design\n\nLab Protocols\n\nCodebooks\n\netc.\n\n\n\nAnalyses\n\nScripting\n\nCommunication\n\netc."
  },
  {
    "objectID": "02-week2-workbook.html#aspects-of-reproducibility",
    "href": "02-week2-workbook.html#aspects-of-reproducibility",
    "title": "Week 2 Workbook",
    "section": "Aspects of Reproducibility",
    "text": "Aspects of Reproducibility\n\nData within files should be ‘tidy’ (next week – tidyr)\nProject based approach (today)\nConsistency: naming, space, style (today)\nDocumentation: commenting and README (today)\nLiterate programming e.g. Rmarkdown (every day!)"
  },
  {
    "objectID": "02-week2-workbook.html#reproducible-workflow",
    "href": "02-week2-workbook.html#reproducible-workflow",
    "title": "Week 2 Workbook",
    "section": "Reproducible Workflow",
    "text": "Reproducible Workflow\nA reproducible workflow is organized. What does it mean to be be organized? At least:\n\nUse a project based approach, e.g., RStudio project or similar\n\nHave a hierarchical folder structure\n\nHave a consistent and informative naming system that ‘plays nice’\n\nDocument code with comments and analyses with README\n\n\nMore advanced (later in the class)\n\n\nGeneralize with functions and packages\nversion control"
  },
  {
    "objectID": "02-week2-workbook.html#what-is-a-project",
    "href": "02-week2-workbook.html#what-is-a-project",
    "title": "Week 2 Workbook",
    "section": "What is a project?",
    "text": "What is a project?\n\nA project is a discrete piece of work which has a number of files associated with it such as the data and scripts for an analysis and the production reports.\nUsing a project-oriented workflow means to have a hierarchical folder structure with everything needed to reproduce an analysis.\n\nOne research project might have several organizational projects associated with it, for example:\n\ndata files and metadata (which may be made into a package)\npreregistration\nanalysis and reporting\na package developed for the analysis\nan app for allowing data to be explored by others"
  },
  {
    "objectID": "02-week2-workbook.html#example",
    "href": "02-week2-workbook.html#example",
    "title": "Week 2 Workbook",
    "section": "Example",
    "text": "Example\nGood Workflows are:\n\nstructured\n\nsystematic\n\nrepeatable\n\nNaming\n\nhuman and machine readable\n\nno spaces\n\nuse snake/kebab case\n\nordering: numbers (zero left padded), dates\n\nfile extensions\n\n\n\n-- ipcs_data_2019\n   |__ipcs_data_2019.Rproj\n   |__data\n      |__raw_data\n         |__2019-03-21_ema_raw.csv\n         |__2019-03-21_baseline_raw.csv\n      |__clean_data\n         |__2019-06-21_ema_long.csv\n         |__2019-06-21_ema_long.RData\n         |__2019-06-21_baseline_wide.csv\n         |__2019-06-21_baseline_wide.RData\n   |__results\n      |__01_models\n         |__E_mortality.RData\n         |__A_mortality.RData\n      |__02_summaries\n         |__E_mortality.RData\n         |__A_mortality.RData\n      |__03_figures\n         |__mortality.png\n         |__mortality.pdf\n      |__04_tables\n         |__zero_order_cors.RData\n         |__descriptives.RData\n         |__key_terms.RData\n         |__all_model_terms.RData\n   |__README.md\n   |__refs\n      |__r_refs.bib\n      |__proj_refs.bib\n   |__analyses\n      |__01_background.Rmd\n      |__02_data_cleaning.Rmd\n      |__03_models.Rmd\n      |__04_summary.Rmd"
  },
  {
    "objectID": "02-week2-workbook.html#what-is-a-path",
    "href": "02-week2-workbook.html#what-is-a-path",
    "title": "Week 2 Workbook",
    "section": "What is a path?",
    "text": "What is a path?\nA path gives the address - or location - of a filesystem object, such as a file or directory.\n\nPaths appear in the address bar of your browser or file explorer.\nWe need to know a file path whenever we want to read, write or refer to a file using code rather than interactively pointing and clicking to navigate.\nA path can be absolute or relative\n\nabsolute = whole path from root\nrelative = path from current directory\n\n\n\nAbsolute paths\n\nAn Absolute path is given from the “root directory” of the object.\nThe root directory of a file system is the first or top directory in the hierarchy.\nFor example, C:\\ or M:\\ on windows or / on a Mac which is displayed as Macintosh HD in Finder.\n\nThe absolute path for a file, pigeon.txt could be:\n\nwindows: C:/Users/edbeck/Desktop/pigeons/data-raw/pigeon.txt\n\nMac/unix systems: /Users/edbeck/Desktop/pigeons/data-raw/pigeon.txt\n\nweb: http://github.com/emoriebeck/pigeons/data/pigeon.txt\n\nWhat is a directory?\n\nDirectory is the old word for what many now call a folder 📂.\nCommands that act on directories in most programming languages and environments reflect this.\nFor example, in R this means “tell me my working directory”:\ngetwd() get working directory in R\nWhat is a working directory?\n\nThe working directory is the default location a program is using. It is where the program will read and write files by default. You have only one working directory at a time.\nThe terms ‘working directory’, ‘current working directory’ and ‘current directory’ all mean the same thing.\n\nFind your current working directory with:\n\nCodegetwd()\n\n[1] \"/Users/emoriebeck/Documents/teaching/PSC290-cleaning-fall-2023/psc290-data-FQ23/psc290-data-FQ23\"\n\n\nRelative paths\nA relative path gives the location of a filesystem object relative to the working directory, (i.e., that returned by getwd()).\n\nWhen pigeon.txt is in the working directory the relative path is just the file * name: pigeon.txt\nIf there is a folder in the working directory called data-raw and pigeon.txt is in there then the relative path is data-raw/pigeon.txt\nPaths: moving up the hierarchy\n\n../ allows you to look in the directory above the working directory\nWhen pigeon.txt is in folder above the working the relative path is ../pigeon.txt\nAnd if it is in a folder called data-raw which is in the directory above the working directory then the relative path is ../data-raw/pigeon.txt\nWhat’s in my directory?\nYou can list the contents of a directory using the dir() command\n\n\ndir() list the contents of the working directory\n\ndir(\"..\") list the contents of the directory above the working directory\n\ndir(\"../..\") list the contents of the directory two directories above the working directory\n\ndir(\"data-raw\") list the contents of a folder call data-raw which is in the working directory.\nRelative or absolute\n\nMost of the time you should use relative paths because that makes your work portable (i.e. to a different machine / user / etc.).\n🎉 The tab key is your friend!\nYou only need to use absolute paths when you are referring to filesystem outside the one you are using.\n\nI often store the beginning of that path as object.\n\nweb_wd <- “https://github.com/emoriebeck/pigeons/”\nThen I can use sprintf() or paste() to add different endings\n\n\n\n\nCodeweb_wd <- \"https://github.com/emoriebeck/pigeons/\"\nsprintf(\"%s/data-raw/pigeon.txt\", web_wd)\n\n[1] \"https://github.com/emoriebeck/pigeons//data-raw/pigeon.txt\""
  },
  {
    "objectID": "02-week2-workbook.html#example-1",
    "href": "02-week2-workbook.html#example-1",
    "title": "Week 2 Workbook",
    "section": "Example",
    "text": "Example\nDownload and unzip pigeons.zip which has the following structure:\n-- pigeons\n   |__data-processed\n      |__pigeon_long.txt\n   |__data-raw\n      |__pigeon.txt\n   |__figures\n      |__fig1.tiff\n   |__scripts\n      |__analysis.R\n      |__import_reshape.R\n   |__pigeons.Rproj"
  },
  {
    "objectID": "02-week2-workbook.html#rstudio-projects-1",
    "href": "02-week2-workbook.html#rstudio-projects-1",
    "title": "Week 2 Workbook",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\nProject is obviously a commonly used word. When I am referring to an RStudio Project I will use the capitalised words ‘RStudio Project’ or ‘Project’.\nIn other cases, I will use ‘project’.\nAn RStudio Project is a directory with an .Rproj file in it.\nThe name of the RStudio Project is the same as the name of the top level directory which is referred to as the Project directory.\n\nFor example, if you create an RStudio Project ipcs_data_2019 your folder structure would look something like this:\n-- ipcs_data_2019\n   |__ipcs_data_2019.Rproj\n   |__data\n      |__raw_data\n         |__2019-03-21_ema_raw.csv\n         |__2019-03-21_baseline_raw.csv\n      |__clean_data\n         |__2019-06-21_ema_long.csv\n         |__2019-06-21_ema_long.RData\n         |__2019-06-21_baseline_wide.csv\n         |__2019-06-21_baseline_wide.RData\n   |__results\n      |__01_models\n      |__02_summaries\n      |__03_figures\n      |__04_tables\n   |__README.md\n   |__refs\n      |__r_refs.bib\n      |__proj_refs.bib\n   |__analyses\n      |__01_background.Rmd\n      |__02_data_cleaning.Rmd\n      |__03_models.Rmd\n      |__04_summary.Rmd\n\nthe .RProj file which is the defining feature of an RStudio Project\nWhen you open an RStudio Project, the working directory is set to the Project directory (i.e., the location of the .Rproj file).\nThis makes your work portable. You can zip up the project folder and send it to any person, including future you, or any computer.\nThey will be able to unzip, open the project and have all the code just work.\n(This is great for sending code and/or results to your advisors)"
  },
  {
    "objectID": "02-week2-workbook.html#directory-structure",
    "href": "02-week2-workbook.html#directory-structure",
    "title": "Week 2 Workbook",
    "section": "Directory structure",
    "text": "Directory structure\nYou are aiming for structured, systematic and repeatable. For example, the Project directory might contain:\n\n.RProj file\n\nREADME - tell people what the project is and how to use it\n\nLicense - tell people what they are allowed to do with your project\nDirectories\ndata/\n\nprereg/\n\nscripts/\nresults/\n\nmanuscript/"
  },
  {
    "objectID": "02-week2-workbook.html#readme",
    "href": "02-week2-workbook.html#readme",
    "title": "Week 2 Workbook",
    "section": "README",
    "text": "README\n\nREADMEs are a form of documentation which have been widely used for a long time. They contain all the information about the other files in a directory. They can be extensive.\nWikipedia README page\nGitHub Doc’s About READMEs\nOSF\n\nA minimal README might give:\n\nTitle\nDescription, 50 words or so on what the project is\nTechnical Description of the project\n\nWhat software and packages are needed including versions\nAny instructions needed to run the analysis/use the software\nAny issues that a user might face in running the analysis/using the software\n\n\nInstructions on how to use the work\nLinks to where other files, materials, etc. are stored\n\nE.g., an OSF readme may point to GitHub, PsyArxiv, etc."
  },
  {
    "objectID": "02-week2-workbook.html#license",
    "href": "02-week2-workbook.html#license",
    "title": "Week 2 Workbook",
    "section": "License",
    "text": "License\nA license tells others what they can and can’t do with your work.\nchoosealicense.com is a useful explainer.\nI typically use:\n\n\nMIT License for software\n\nCC-BY-SA-4.0 for other work"
  },
  {
    "objectID": "02-week2-workbook.html#rstudio-project-infrastructure",
    "href": "02-week2-workbook.html#rstudio-project-infrastructure",
    "title": "Week 2 Workbook",
    "section": "RStudio Project infrastructure",
    "text": "RStudio Project infrastructure\n🎬 create a new Project called iris by:\n\nclicking File->New Project…\nclicking on the little icon (second from the left) at the top\nChoose New Project, then New Directory, then New Project. Name the RStudio Project iris.\nCreate folders in iris called data-raw, data-processed and figures.\nStart new scripts called 01-import.R, 02-tidy.R, and 03-figures.R"
  },
  {
    "objectID": "02-week2-workbook.html#save-and-import",
    "href": "02-week2-workbook.html#save-and-import",
    "title": "Week 2 Workbook",
    "section": "Save and Import",
    "text": "Save and Import\n\nSave a copy of iris.csv to your data-raw folder. These data give the information about different species of irises.\nIn your 01-import.R script, load the tidyverse set of packages.\n\n\nCodelibrary(tidyverse)\nwrite_csv(iris, file = \"data-raw/iris.csv\")\n\n\n\nAdd the command to import the data:\n\n\nCodeiris <- read_csv(\"data-raw/iris.csv\")\n\n\n\n\n\n\nThe relative path is data-raw/iris.csv because your working directory is the Project directory, iris."
  },
  {
    "objectID": "02-week2-workbook.html#reformat-the-data",
    "href": "02-week2-workbook.html#reformat-the-data",
    "title": "Week 2 Workbook",
    "section": "Reformat the data",
    "text": "Reformat the data\nThis dataset has three observations in a row - it is not ‘tidy’.\n\nOpen your 02-tidy.R script, and reshape the data using:\n\n\nCodeiris <- pivot_longer(data = iris, \n                     cols = -Species, \n                     names_to = \"attribute\", \n                     values_to = \"value\")\n\n\n\nThis reformats the dataframe in R but does not overwrite the text file of the data.\nDon’t worry too much about this right now. We’ll spend a lot of time talking about reshaping data next week!"
  },
  {
    "objectID": "02-week2-workbook.html#writing-files",
    "href": "02-week2-workbook.html#writing-files",
    "title": "Week 2 Workbook",
    "section": "Writing files",
    "text": "Writing files\nOften we want to write to files.\n\nMy main reasons for doing so are to save copies of data that have been processed and to save manuscripts and graphics.\nAlso, as someone who collects a lot of data, the de-identified, fully anonymized data files I can share and the identifiable data I collect require multiple versions (and encryption, keys, etc.)\nWrite your dataframe iris to a csv file named iris-long.csv in your data-processed folder:\n\n\nCodefile <- \"data-processed/iris-long.csv\"\nwrite_csv(iris, file)\n\n\n\n\n\n\nPutting file paths into variables often makes your code easier to read especially when file paths are long or used multiple times."
  },
  {
    "objectID": "02-week2-workbook.html#create-a-plot",
    "href": "02-week2-workbook.html#create-a-plot",
    "title": "Week 2 Workbook",
    "section": "Create a plot",
    "text": "Create a plot\nOpen your 03-figures.R script and create a simple plot of this data with:\n\nCodefig1 <- ggplot(\n  data = iris\n  , aes(y = Species, x = value, fill = Species)\n  ) + \n  geom_boxplot() +                       \n  facet_grid(attribute~.) + \n  scale_x_continuous(name = \"Attribute\") +\n  scale_y_discrete(name = \"Species\") +\n  theme_classic() + \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "02-week2-workbook.html#view-plot",
    "href": "02-week2-workbook.html#view-plot",
    "title": "Week 2 Workbook",
    "section": "View plot",
    "text": "View plot\nView plot with:\n\nCodefig1"
  },
  {
    "objectID": "02-week2-workbook.html#write-ggplot-figure-to-file",
    "href": "02-week2-workbook.html#write-ggplot-figure-to-file",
    "title": "Week 2 Workbook",
    "section": "Write ggplot figure to file",
    "text": "Write ggplot figure to file\n\nA useful function for saving ggplot figures is ggsave().\nIt has arguments for the size, resolution and device for the image. See the ggsave() reference page.\nSince I often make more than one figure, I might set these arguments first.\n\n\n\n\nAssign ggsave argument values to variables:\n\n\nCode# figure saving settings\nunits <- \"in\"  \nfig_w <- 3.2\nfig_h <- fig_w\ndpi <- 600\ndevice <- \"tiff\" \n\n\n\n\nSave the figure to your figures directory:\n\n\nCodeggsave(\"figures/fig1.tiff\",\n       plot = fig1,\n       device = device,\n       width = fig_w,\n       height = fig_h,\n       units = units,\n       dpi = dpi)\n\n\n\n\n\n\nCheck it is there!\n\n\n\n\n\n\n\nData Manipulation in dplyr"
  },
  {
    "objectID": "02-week2-workbook.html#core-functions",
    "href": "02-week2-workbook.html#core-functions",
    "title": "Week 2 Workbook",
    "section": "Core Functions",
    "text": "Core Functions\n\n\n\n\n%>%\nfilter()\nselect()\narrange()\ngroup_by()\nmutate()\nsummarize()\n\n\n\nAlthough each of these functions are powerful alone, they are incredibly powerful in conjunction with one another. So below, I’ll briefly introduce each function, then link them all together using an example of basic data cleaning and summary."
  },
  {
    "objectID": "02-week2-workbook.html#section",
    "href": "02-week2-workbook.html#section",
    "title": "Week 2 Workbook",
    "section": "1. %>%\n",
    "text": "1. %>%\n\n\nThe pipe %>% is wonderful. It makes coding intuitive. Often in coding, you need to use so-called nested functions. For example, you might want to round a number after taking the square of 43.\n\n\nCodesqrt(43)\n\n[1] 6.557439\n\nCoderound(sqrt(43), 2)\n\n[1] 6.56\n\n\nThe issue with this comes whenever we need to do a series of operations on a data set or other type of object. In such cases, if we run it in a single call, then we have to start in the middle and read our way out.\n\nCoderound(sqrt(43/2), 2)\n\n[1] 4.64\n\n\nThe pipe solves this by allowing you to read from left to right (or top to bottom). The easiest way to think of it is that each call of %>% reads and operates as “and then.” So with the rounded square root of 43, for example:\n\nCodesqrt(43) %>%\n  round(2)\n\n[1] 6.56"
  },
  {
    "objectID": "02-week2-workbook.html#filter",
    "href": "02-week2-workbook.html#filter",
    "title": "Week 2 Workbook",
    "section": "2. filter()\n",
    "text": "2. filter()\n\nOften times, when conducting research (experiments or otherwise), there are observations (people, specific trials, etc.) that you don’t want to include.\n\n\nCodedata(bfi) # grab the bfi data from the psych package\nbfi <- bfi %>% as_tibble()\nhead(bfi)\n\n\n\n  \n\n\n\nOften times, when conducting research (experiments or otherwise), there are observations (people, specific trials, etc.) that you don’t want to include.\n\nCodesummary(bfi$age) # get age descriptives\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   3.00   20.00   26.00   28.78   35.00   86.00 \n\n\nOften times, when conducting research (experiments or otherwise), there are observations (people, specific trials, etc.) that you don’t want to include.\n\nCodebfi2 <- bfi %>% # see a pipe!\n  filter(age <= 18) # filter to age up to 18\n\nsummary(bfi2$age) # summary of the new data \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    3.0    16.0    17.0    16.3    18.0    18.0 \n\n\nBut this isn’t quite right. We still have folks below 12. But, the beauty of filter() is that you can do sequence of OR and AND statements when there is more than one condition, such as up to 18 AND at least 12.\n\nCodebfi2 <- bfi %>%\n  filter(age <= 18 & age >= 12) # filter to age up to 18 and at least 12\n\nsummary(bfi2$age) # summary of the new data \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    16.0    17.0    16.4    18.0    18.0 \n\n\nGot it!\n\nBut filter works for more use cases than just conditional\n\n\n<, >, <=, and >=\n\n\n\nIt can also be used for cases where we want a single values to match cases with text.\nTo do that, let’s convert one of the variables in the bfi data frame to a string.\nSo let’s change gender (1 = male, 2 = female) to text (we’ll get into factors later).\n\n\nCodebfi$education <- plyr::mapvalues(bfi$education, 1:5, c(\"Below HS\", \"HS\", \"Some College\", \"College\", \"Higher Degree\"))\n\n\nNow let’s try a few things:\n1. Create a data set with only individuals with some college (==).\n\nCodebfi2 <- bfi %>% \n  filter(education == \"Some College\")\nunique(bfi2$education)\n\n[1] \"Some College\"\n\n\n2. Create a data set with only people age 18 (==).\n\nCodebfi2 <- bfi %>%\n  filter(age == 18)\nsummary(bfi2$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     18      18      18      18      18      18 \n\n\n3. Create a data set with individuals with some college or above (%in%).\n\nCodebfi2 <- bfi %>%\n  filter(education %in% c(\"Some College\", \"College\", \"Higher Degree\"))\nunique(bfi2$education)\n\n[1] \"Some College\"  \"Higher Degree\" \"College\"      \n\n\n%in% is great. It compares a column to a vector rather than just a single value, you can compare it to several\n\nCodebfi2 <- bfi %>%\n  filter(age %in% 12:18)\nsummary(bfi2$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    16.0    17.0    16.4    18.0    18.0"
  },
  {
    "objectID": "02-week2-workbook.html#select",
    "href": "02-week2-workbook.html#select",
    "title": "Week 2 Workbook",
    "section": "3. select()\n",
    "text": "3. select()\n\n\nIf filter() is for pulling certain observations (rows), then select() is for pulling certain variables (columns).\nit’s good practice to remove these columns to stop your environment from becoming cluttered and eating up your RAM.\nIn our bfi data, most of these have been pre-removed, so instead, we’ll imagine we don’t want to use any indicators of Agreeableness (A1-A5) and that we aren’t interested in gender.\nWith select(), there are few ways choose variables. We can bare quote name the ones we want to keep, bare quote names we want to remove, or use any of a number of select() helper functions.\n\nA. Bare quote columns we want to keep:\n\n\n\nCodebfi %>%\n  select(C1, C2, C3, C4, C5) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 5\n     C1    C2    C3    C4    C5\n  <int> <int> <int> <int> <int>\n1     2     3     3     4     4\n2     5     4     4     3     4\n3     4     5     4     2     5\n4     4     4     3     5     5\n5     4     4     5     3     2\n6     6     6     6     1     3\n# ℹ 2,794 more rows\n\n\n\n\nCodebfi %>%\n  select(C1:C5) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 5\n     C1    C2    C3    C4    C5\n  <int> <int> <int> <int> <int>\n1     2     3     3     4     4\n2     5     4     4     3     4\n3     4     5     4     2     5\n4     4     4     3     5     5\n5     4     4     5     3     2\n6     6     6     6     1     3\n# ℹ 2,794 more rows\n\n\n\n\n\nB. Bare quote columns we don’t want to keep:\n\nCodebfi %>% \n  select(-(A1:A5), -gender) %>% # Note the `()` around the columns\n  print(n = 6)\n\n# A tibble: 2,800 × 22\n     C1    C2    C3    C4    C5    E1    E2    E3    E4    E5    N1    N2    N3\n  <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n1     2     3     3     4     4     3     3     3     4     4     3     4     2\n2     5     4     4     3     4     1     1     6     4     3     3     3     3\n3     4     5     4     2     5     2     4     4     4     5     4     5     4\n4     4     4     3     5     5     5     3     4     4     4     2     5     2\n5     4     4     5     3     2     2     2     5     4     5     2     3     4\n6     6     6     6     1     3     2     1     6     5     6     3     5     2\n# ℹ 2,794 more rows\n# ℹ 9 more variables: N4 <int>, N5 <int>, O1 <int>, O2 <int>, O3 <int>,\n#   O4 <int>, O5 <int>, education <chr>, age <int>\n\n\nC. Add or remove using select() helper functions.\n\n\n\n\nstarts_with()\n\nends_with()\ncontains()\nmatches()\nnum_range()\none_of()\nall_of()\n\n\n\n\nCodebfi %>%\n  select(starts_with(\"C\"))"
  },
  {
    "objectID": "02-week2-workbook.html#arrange",
    "href": "02-week2-workbook.html#arrange",
    "title": "Week 2 Workbook",
    "section": "4. arrange()\n",
    "text": "4. arrange()\n\n\nSometimes, either in order to get a better sense of our data or in order to well, order our data, we want to sort it\nAlthough there is a base R sort() function, the arrange() function is tidyverse version that plays nicely with other tidyverse functions.\n\n\nSo in our previous examples, we could also arrange() our data by age or education, rather than simply filtering. (Or as we’ll see later, we can do both!)\n\n\nCode# sort by age\nbfi %>% \n  select(gender:age) %>%\n  arrange(age) %>% \n  print(n = 6)\n\n# A tibble: 2,800 × 3\n  gender education       age\n   <int> <chr>         <int>\n1      1 Higher Degree     3\n2      2 <NA>              9\n3      2 Some College     11\n4      2 <NA>             11\n5      2 <NA>             11\n6      2 <NA>             12\n# ℹ 2,794 more rows\n\n\n\n\nCode# sort by education\nbfi %>%\n  select(gender:age) %>%\n  arrange(education) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 3\n  gender education   age\n   <int> <chr>     <int>\n1      1 Below HS     19\n2      1 Below HS     21\n3      1 Below HS     17\n4      1 Below HS     18\n5      1 Below HS     18\n6      2 Below HS     18\n# ℹ 2,794 more rows\n\n\n\n\nWe can also arrange by multiple columns, like if we wanted to sort by gender then education:\n\nCodebfi %>%\n  select(gender:age) %>%\n  arrange(gender, education) %>% \n  print(n = 6)\n\n# A tibble: 2,800 × 3\n  gender education   age\n   <int> <chr>     <int>\n1      1 Below HS     19\n2      1 Below HS     21\n3      1 Below HS     17\n4      1 Below HS     18\n5      1 Below HS     18\n6      1 Below HS     32\n# ℹ 2,794 more rows"
  },
  {
    "objectID": "02-week2-workbook.html#group_by",
    "href": "02-week2-workbook.html#group_by",
    "title": "Week 2 Workbook",
    "section": "5. group_by()\n",
    "text": "5. group_by()\n\n\nThe group_by() function is the “split” of the method\nIt basically implicitly breaks the data set into chunks by whatever bare quoted column(s)/variable(s) are supplied as arguments.\n\nSo imagine that we wanted to group_by() education levels to get average ages at each level\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  group_by(education) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 8\n# Groups:   education [6]\n     C1    C2    C3    C4    C5   age gender education   \n  <int> <int> <int> <int> <int> <int>  <int> <chr>       \n1     2     3     3     4     4    16      1 <NA>        \n2     5     4     4     3     4    18      2 <NA>        \n3     4     5     4     2     5    17      2 <NA>        \n4     4     4     3     5     5    17      2 <NA>        \n5     4     4     5     3     2    17      1 <NA>        \n6     6     6     6     1     3    21      2 Some College\n# ℹ 2,794 more rows\n\n\n\nHadley’s first law of data cleaning: “What is split, must be combined”\nThis is super easy with the ungroup() function:\n\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  group_by(education) %>%\n  ungroup() %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 8\n     C1    C2    C3    C4    C5   age gender education   \n  <int> <int> <int> <int> <int> <int>  <int> <chr>       \n1     2     3     3     4     4    16      1 <NA>        \n2     5     4     4     3     4    18      2 <NA>        \n3     4     5     4     2     5    17      2 <NA>        \n4     4     4     3     5     5    17      2 <NA>        \n5     4     4     5     3     2    17      1 <NA>        \n6     6     6     6     1     3    21      2 Some College\n# ℹ 2,794 more rows\n\n\nMultiple group_by() calls overwrites previous calls:\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  group_by(education) %>%\n  group_by(gender, age) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 8\n# Groups:   gender, age [115]\n     C1    C2    C3    C4    C5   age gender education   \n  <int> <int> <int> <int> <int> <int>  <int> <chr>       \n1     2     3     3     4     4    16      1 <NA>        \n2     5     4     4     3     4    18      2 <NA>        \n3     4     5     4     2     5    17      2 <NA>        \n4     4     4     3     5     5    17      2 <NA>        \n5     4     4     5     3     2    17      1 <NA>        \n6     6     6     6     1     3    21      2 Some College\n# ℹ 2,794 more rows"
  },
  {
    "objectID": "02-week2-workbook.html#mutate",
    "href": "02-week2-workbook.html#mutate",
    "title": "Week 2 Workbook",
    "section": "6. mutate()\n",
    "text": "6. mutate()\n\n\n\nmutate() is one of your “apply” functions\nWhen you use mutate(), the resulting data frame will have the same number of rows you started with\nYou are directly mutating the existing data frame, either modifying existing columns or creating new ones\n\nTo demonstrate, let’s add a column that indicated average age levels within each age group\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  arrange(education) %>%\n  group_by(education) %>% \n  mutate(age_by_edu = mean(age, na.rm = T)) %>%\n  print(n = 6)\n\n# A tibble: 2,800 × 9\n# Groups:   education [6]\n     C1    C2    C3    C4    C5   age gender education age_by_edu\n  <int> <int> <int> <int> <int> <int>  <int> <chr>          <dbl>\n1     6     6     3     4     5    19      1 Below HS        25.1\n2     4     3     5     3     2    21      1 Below HS        25.1\n3     5     5     5     2     2    17      1 Below HS        25.1\n4     5     5     4     1     1    18      1 Below HS        25.1\n5     4     5     4     3     3    18      1 Below HS        25.1\n6     3     2     3     4     6    18      2 Below HS        25.1\n# ℹ 2,794 more rows\n\n\nmutate() is also super useful even when you aren’t grouping\nWe can create a new category\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  mutate(gender_cat = plyr::mapvalues(gender, c(1,2), c(\"Male\", \"Female\")))\n\n\n\n  \n\n\n\nWe could also just overwrite it:\n\nCodebfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  mutate(gender = plyr::mapvalues(gender, c(1,2), c(\"Male\", \"Female\")))"
  },
  {
    "objectID": "02-week2-workbook.html#summarize-summarise",
    "href": "02-week2-workbook.html#summarize-summarise",
    "title": "Week 2 Workbook",
    "section": "7. summarize() / summarise()\n",
    "text": "7. summarize() / summarise()\n\n\n\nsummarize() is one of your “apply” functions\nThe resulting data frame will have the same number of rows as your grouping variable\nYou number of groups is 1 for ungrouped data frames\n\n\nCode# group_by() education\nbfi %>%\n  select(starts_with(\"C\"), age, gender, education) %>%\n  arrange(education) %>%\n  group_by(education) %>% \n  summarize(age_by_edu = mean(age, na.rm = T))  \n\n\n\n  \n\n\n\n\nCode# no groups  \nbfi %>% \n  select(starts_with(\"C\"), age, gender, education) %>%\n  arrange(education) %>%\n  summarize(age_by_edu = mean(age, na.rm = T))"
  },
  {
    "objectID": "07-week7-slides.html",
    "href": "07-week7-slides.html",
    "title": "Week 7 Slides",
    "section": "",
    "text": "Welcome to Week 7! This week, we’ll talk about tricky classes like dates and strings along with regex (regular expressions)."
  },
  {
    "objectID": "09-week9-workbook.html",
    "href": "09-week9-workbook.html",
    "title": "Week 9 Workbook",
    "section": "",
    "text": "Codepkg <- c(\"knitr\", \"psych\", \"palmerpenguins\", \"lavaan\", \"future\", \"plyr\", \"tidyverse\", \"furrr\")\npkg <- pkg[!pkg %in% rownames(installed.packages())]\nif(length(pkg) > 0) map(pkg, install.packages)\n\nlibrary(knitr)\nlibrary(psych)\nlibrary(lavaan)\n\nThis is lavaan 0.6-15\nlavaan is FREE software! Please report any bugs.\n\n\n\nAttaching package: 'lavaan'\n\n\nThe following object is masked from 'package:psych':\n\n    cor2cov\n\nCodelibrary(future)\nlibrary(plyr)\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.4.2     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.3\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()     masks psych::%+%()\n✖ ggplot2::alpha()   masks psych::alpha()\n✖ dplyr::arrange()   masks plyr::arrange()\n✖ purrr::compact()   masks plyr::compact()\n✖ dplyr::count()     masks plyr::count()\n✖ dplyr::desc()      masks plyr::desc()\n✖ dplyr::failwith()  masks plyr::failwith()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::id()        masks plyr::id()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::mutate()    masks plyr::mutate()\n✖ dplyr::rename()    masks plyr::rename()\n✖ dplyr::summarise() masks plyr::summarise()\n✖ dplyr::summarize() masks plyr::summarize()\n\nCodelibrary(furrr) # note loading this last ONLY because it depends on tidyverse and will not mask it"
  },
  {
    "objectID": "09-week9-workbook.html#what-and-why-use-git-and-github",
    "href": "09-week9-workbook.html#what-and-why-use-git-and-github",
    "title": "Week 9 Workbook",
    "section": "What and why use Git and GitHub?",
    "text": "What and why use Git and GitHub?\nVideo from Will Doyle, Professor at Vanderbilt University"
  },
  {
    "objectID": "09-week9-workbook.html#what-is-version-control",
    "href": "09-week9-workbook.html#what-is-version-control",
    "title": "Week 9 Workbook",
    "section": "What is version control?",
    "text": "What is version control?\n\n\nVersion control is a “system that records changes to a file or set of files over time so that you can recall specific versions later”\nKeeps records of changes, who made changes, and when those changes were made\nYou or collaborators take “snapshots” of a document at a particular point in time. Later on, you can recover any previous snapshot of the document."
  },
  {
    "objectID": "09-week9-workbook.html#how-version-control-works",
    "href": "09-week9-workbook.html#how-version-control-works",
    "title": "Week 9 Workbook",
    "section": "How version control works:",
    "text": "How version control works:\n\nImagine you write a simple text file document that gives a recipe for yummy chocolate chip cookies and you save it as cookies.txt\n\nLater on, you make changes to cookies.txt (e.g., add alternative baking time for people who like “soft and chewy” cookies)\nWhen using version control to make these changes, you don’t save entirely new version of cookies.txt; rather, you save the changes made relative to the previous version of cookies.txt"
  },
  {
    "objectID": "09-week9-workbook.html#why-use-git-and-github",
    "href": "09-week9-workbook.html#why-use-git-and-github",
    "title": "Week 9 Workbook",
    "section": "Why use Git and GitHub?",
    "text": "Why use Git and GitHub?\nWhy use version control when you can just save new version of document?\n\nSaving entirely new document each time a change is made is very inefficient from a memory/storage perspective\n\nWhen you save a new version of a document, much of the contents are the same as the previous version\nInefficient to devote space to saving multiple copies of the same content\n\n\nWhen document undergoes lots of changes – especially a document that multiple people are collaborating on – it’s hard to keep track of so many different documents. Easy to end up with a situation like this:"
  },
  {
    "objectID": "09-week9-workbook.html#why-use-git-and-github-1",
    "href": "09-week9-workbook.html#why-use-git-and-github-1",
    "title": "Week 9 Workbook",
    "section": "Why use Git and GitHub?",
    "text": "Why use Git and GitHub?\n\n\n\n\nCredit: Jorge Chan (and also, lifted this example from Benjamin Skinner’s intro to Git/GitHub lecture)"
  },
  {
    "objectID": "09-week9-workbook.html#what-is-git-from-git-website",
    "href": "09-week9-workbook.html#what-is-git-from-git-website",
    "title": "Week 9 Workbook",
    "section": "What is Git? (from git website)",
    "text": "What is Git? (from git website)\n\n“Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency”\n\n\nGit is a particular version control software created by The Git Project\n\n\nGit is free and open source software, meaning that anyone can use, share, and modify the software\nAlthough Microsoft owns Github (described) below, it thankfully does not own Git!\n\n\nGit can be used by:\n\nAn individual/standalone developer\nFor collaborative projects, where multiple people collaborate on each file"
  },
  {
    "objectID": "09-week9-workbook.html#what-is-a-git-repository",
    "href": "09-week9-workbook.html#what-is-a-git-repository",
    "title": "Week 9 Workbook",
    "section": "What is a Git repository?",
    "text": "What is a Git repository?\n\nA Git repository is any project managed in Git\nFrom Git Handbook by github.com:\n\nA repository “encompasses the entire collection of files and folders associated with a project, along with each file’s revision history”\nBecause git is a distributed version control system, “repositories are self-contained units and anyone who owns a copy of the repository can access the entire codebase and its history”\n\n\nThis course is a Git repository (PSC290 FQ23 repository)"
  },
  {
    "objectID": "09-week9-workbook.html#what-is-a-git-repository-1",
    "href": "09-week9-workbook.html#what-is-a-git-repository-1",
    "title": "Week 9 Workbook",
    "section": "What is a Git repository?",
    "text": "What is a Git repository?\n\nLocal vs. remote git repository:\n\n\nLocal git repository: git repository for a project stored on your machine\n\nRemote git repository: git repository for a project stored on the internet\n\n\nTypically, a local git repository is connected to a remote git repository\n\nYou can make changes to local repository on your machine and then push those changes to the remote repository\nOther collaborators can also make changes to their local repository, push them to the remote repository, and then you can pull these changes into your local repository"
  },
  {
    "objectID": "09-week9-workbook.html#private-vs.-public-repositories",
    "href": "09-week9-workbook.html#private-vs.-public-repositories",
    "title": "Week 9 Workbook",
    "section": "Private vs. public repositories",
    "text": "Private vs. public repositories\n\nPublic repositories: anyone can access the repository\n\ne.g., PSC290 FQ23, the git repository we created to develop this course is a public repository because we want the public to benefit from this course\n\n\nPrivate repositories: only those who have been granted access by a repository “administrator” can access the repository"
  },
  {
    "objectID": "09-week9-workbook.html#what-is-github",
    "href": "09-week9-workbook.html#what-is-github",
    "title": "Week 9 Workbook",
    "section": "What is GitHub?",
    "text": "What is GitHub?\n\n\nGitHub is the industry standard hosting site/service for Git repositories\n\nHosting services allow people/organizations to store files on the internet and make those files available to others\n\n\nMicrosoft acquired Github in 2018 for $7.5 billion\nGithub is where remote git repositories live"
  },
  {
    "objectID": "09-week9-workbook.html#git-workflow",
    "href": "09-week9-workbook.html#git-workflow",
    "title": "Week 9 Workbook",
    "section": "Git Workflow",
    "text": "Git Workflow\nVersion control systems that save differences: - Prior to Git, “centralized version control systems” were the industry standard version control systems (From Getting Started - About Version Control) - In these systems, a central server stored all the versions of a file and “clients” (e.g., a programmer working on a project on their local computer) could “check out” files from the central server - These centralized version control systems stored multiple versions of a file as “differences” - The below figure portrays version control systems that store data as changes relative to the base version of each file:"
  },
  {
    "objectID": "09-week9-workbook.html#git-workflow-1",
    "href": "09-week9-workbook.html#git-workflow-1",
    "title": "Week 9 Workbook",
    "section": "Git Workflow",
    "text": "Git Workflow\n\nCredit: Getting Started - What is Git"
  },
  {
    "objectID": "09-week9-workbook.html#git-workflow-2",
    "href": "09-week9-workbook.html#git-workflow-2",
    "title": "Week 9 Workbook",
    "section": "Git Workflow",
    "text": "Git Workflow\nGit stores data as snapshots rather than differences:\n\nGit doesn’t think of data as differences relative to the base version of each file\nRather, Git thinks of data as “a series of snapshots of a miniature filesystem” or, said differently, a series of snapshots of all files in the repository\nFor files that have changed:\n\nthe “commit” will save lines that you have changed or added [like “differences”]\nlines that have not changed will not be re-saved; because these lines have been saved in previous commit(s) that are linked to the current commit"
  },
  {
    "objectID": "09-week9-workbook.html#git-workflow-3",
    "href": "09-week9-workbook.html#git-workflow-3",
    "title": "Week 9 Workbook",
    "section": "Git Workflow",
    "text": "Git Workflow\n\nThe below figure portrays storing data as a stream of snapshots over time:\n\n\nCredit: Getting Started - What is Git"
  },
  {
    "objectID": "09-week9-workbook.html#what-is-a-commit",
    "href": "09-week9-workbook.html#what-is-a-commit",
    "title": "Week 9 Workbook",
    "section": "What is a commit?",
    "text": "What is a commit?\n\nA commit is a snapshot of all files in the repository at a particular time\nExample: Imagine you are working on a project (repository) that contains a dozen files\n\nYou change two files and make a commit\nGit takes a snapshot of the full repository (all files)\nContent that remains unchanged relative to the previous commit is stored vis-a-vis a link to the previous commit"
  },
  {
    "objectID": "09-week9-workbook.html#three-components-of-a-git-project",
    "href": "09-week9-workbook.html#three-components-of-a-git-project",
    "title": "Week 9 Workbook",
    "section": "Three components of a Git project",
    "text": "Three components of a Git project\n\n\n\nCredit: Lucas Maurer, medium.com"
  },
  {
    "objectID": "09-week9-workbook.html#three-components-of-a-git-project-1",
    "href": "09-week9-workbook.html#three-components-of-a-git-project-1",
    "title": "Week 9 Workbook",
    "section": "Three components of a Git project",
    "text": "Three components of a Git project\n\n\nLocal working directory (also called “working tree”)\n\nThis is the area where all your work happens! You are writing Rmd files, debugging R scripts, adding and deleting files\nThese changes are made on your local machine!\n\n\n\nGit index/staging area (git add <filename(s)> command)\n\nThe staging area is the area between your local working directory and the repository, where you list changes you have made in the local working directory that you would like to commit to the repository\n\n\n\nRepository (git commit command)\n\nThis is the actual repository where Git permanently stores the changes you’ve made in the local working directory and added to the staging area"
  },
  {
    "objectID": "09-week9-workbook.html#git-workflow-4",
    "href": "09-week9-workbook.html#git-workflow-4",
    "title": "Week 9 Workbook",
    "section": "Git Workflow",
    "text": "Git Workflow\nHypothetical work flow to cookies.txt:\n\n\nAdd changes from local working directory to staging area\n\n\nCommit changes from staging area to repository\n\nEach commit to the repository is a different version of the file that represents a snapshot of the file at a particular time\nCommits are made to branches in the repo\n\nBy default, a git repository comes with one main branch (typically called main)\nBut we can also create other branches (discussed more later)"
  },
  {
    "objectID": "09-week9-workbook.html#git-workflow-5",
    "href": "09-week9-workbook.html#git-workflow-5",
    "title": "Week 9 Workbook",
    "section": "Git Workflow",
    "text": "Git Workflow\n\n\nLocal vs. remote repository\n\nWhen you add a change to the staging area and then commit the change to your repository, this changes your local repository (i.e., on your computer) rather than your remote repository (i.e., on GitHub)\n\n\nIf you want to change the remote repository (typically named origin), you must push the change from your local repository to your remote repository\n\nAs seen below, each circle represents a commit. After you make commits on a branch in your local repository (i.e., main), you need to push them in order for the corresponding branch on the remote repository (i.e., origin/main) to be up-to-date with your changes."
  },
  {
    "objectID": "09-week9-workbook.html#github-desktop",
    "href": "09-week9-workbook.html#github-desktop",
    "title": "Week 9 Workbook",
    "section": "GitHub Desktop",
    "text": "GitHub Desktop\n\nPractically, in this class, I’m going to show you how to use GitHub Desktop, which is a GUI (graphical user interface) for managing git repositories and commits.\nRelative to the command line, using a GUI means you’re ready to be “up and running with git immediately and don’t have to learn bash syntax"
  },
  {
    "objectID": "09-week9-workbook.html#exercise-setting-up-github-desktop",
    "href": "09-week9-workbook.html#exercise-setting-up-github-desktop",
    "title": "Week 9 Workbook",
    "section": "Exercise: Setting Up GitHub Desktop",
    "text": "Exercise: Setting Up GitHub Desktop\n\nRather than stepping through in the slides, I’m going to have each of you navigate to this link: https://docs.github.com/en/desktop/overview/getting-started-with-github-desktop.\nFollow it to the end of Part 1 (Installing and authenticating) and then pause\nRaise your hand if you need help\nIf you don’t already have a GitHub account and don’t want to set one up, work with someone around you"
  },
  {
    "objectID": "09-week9-workbook.html#the-basic-workflow",
    "href": "09-week9-workbook.html#the-basic-workflow",
    "title": "Week 9 Workbook",
    "section": "The basic workflow",
    "text": "The basic workflow\nFirst time\n\n\nClone the repository that you want to work on from GitHub onto your local machine\nWork on the files/scripts, e.g., penguins.R\n\nNext, you will commit your changes and include an informative message, e.g. “Plot distribution of flipper length”\nThen, you will push your changes to the remote repository\nSubsequent times\n\n\nPull any changes from the remote repository that your collaborators might have made\nRepeat steps 2-4 above"
  },
  {
    "objectID": "09-week9-workbook.html#cloning",
    "href": "09-week9-workbook.html#cloning",
    "title": "Week 9 Workbook",
    "section": "Cloning",
    "text": "Cloning"
  },
  {
    "objectID": "09-week9-workbook.html#exercise",
    "href": "09-week9-workbook.html#exercise",
    "title": "Week 9 Workbook",
    "section": "Exercise",
    "text": "Exercise\nThe repo walice/git-tutorial contains the penguins.R script, which works with data from the palmerpenguins library.\nCredit: 5 Minute Git"
  },
  {
    "objectID": "09-week9-workbook.html#work-on-the-files",
    "href": "09-week9-workbook.html#work-on-the-files",
    "title": "Week 9 Workbook",
    "section": "Work on the files",
    "text": "Work on the files\n\nCodeggplot(penguins, \n       aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + \n  geom_point() +\n  labs(title = \"Penguin bills\") + \n  theme_classic()"
  },
  {
    "objectID": "09-week9-workbook.html#staging-your-files",
    "href": "09-week9-workbook.html#staging-your-files",
    "title": "Week 9 Workbook",
    "section": "Staging your files",
    "text": "Staging your files\n\n\nStage your file"
  },
  {
    "objectID": "09-week9-workbook.html#commit-your-changes",
    "href": "09-week9-workbook.html#commit-your-changes",
    "title": "Week 9 Workbook",
    "section": "Commit your changes",
    "text": "Commit your changes"
  },
  {
    "objectID": "09-week9-workbook.html#commit-your-changes-1",
    "href": "09-week9-workbook.html#commit-your-changes-1",
    "title": "Week 9 Workbook",
    "section": "Commit your changes",
    "text": "Commit your changes\nUse an informative commit message\n\n(Not great) “Analyze data” 😞\n(Better) “Estimate logistic regression” 🎉\nHave a consistent style\n\nStart with an action verb\nCapitalize message\nCommits are cheap, use them often!"
  },
  {
    "objectID": "09-week9-workbook.html#push-your-changes",
    "href": "09-week9-workbook.html#push-your-changes",
    "title": "Week 9 Workbook",
    "section": "Push your changes",
    "text": "Push your changes"
  },
  {
    "objectID": "09-week9-workbook.html#why-use-github",
    "href": "09-week9-workbook.html#why-use-github",
    "title": "Week 9 Workbook",
    "section": "Why Use Git/Hub",
    "text": "Why Use Git/Hub\n\nDirect link to the Open Science Framework via “Add-ons,” so you don’t have to maintain / copy files multiple places\nEasy collaboration with better tools for dealing with conflicts due to remotes\nEasy to restore back to an earlier “snapshot”\nCan directly link to files, data, etc.\n\nby default, the links will be to the “main”\nswap this out for “raw” and it will link to the raw file, which will open a raw text page for text files (including .csv) or download (including .docx, .pdf, etc.)"
  },
  {
    "objectID": "09-week9-workbook.html#guarding-your-resources",
    "href": "09-week9-workbook.html#guarding-your-resources",
    "title": "Week 9 Workbook",
    "section": "Guarding your resources",
    "text": "Guarding your resources\n\nComputers have finite resources\nA common source of issues in R is a cluttered environment\n\nObjects you aren’t using\nObjects unrelated to whatever you’re working on, etc."
  },
  {
    "objectID": "09-week9-workbook.html#cleaning-up-your-environment",
    "href": "09-week9-workbook.html#cleaning-up-your-environment",
    "title": "Week 9 Workbook",
    "section": "Cleaning up your environment",
    "text": "Cleaning up your environment\n\nThe best ways to avoid issues due to a cluttered environment are:\n\nAlways start from a blank environment\nWrite scripts systematically. You shouldn’t be skipping around and should be careful about overwriting objects\nSave object(s), not your whole workspace to allow you to bring back in things you were working on previously\nUse object names with clear patterns to allow you clean up your environment\n\nrm(list=ls()[grepl(\"RQ1\", ls()])\nrm(list=ls()[!ls() %in% c(\"df1\", \"df2\")])\n\n\nOccasionally call gc() after cleaning up your environment"
  },
  {
    "objectID": "09-week9-workbook.html#guarding-your-resources-1",
    "href": "09-week9-workbook.html#guarding-your-resources-1",
    "title": "Week 9 Workbook",
    "section": "Guarding your resources",
    "text": "Guarding your resources\n\nAll of the above guard your RAM, but another cause of issues with resources is that you take full advantage of your computing power\nMost modern computers have 8+ physical and virtual cores and powerful graphics cards\nTogether, these allow us to parallelize processes, which just means to do multiple different processes in parallel at the same time"
  },
  {
    "objectID": "09-week9-workbook.html#parallel-processing",
    "href": "09-week9-workbook.html#parallel-processing",
    "title": "Week 9 Workbook",
    "section": "Parallel Processing",
    "text": "Parallel Processing\n\nYou’ve likely used parallel processing in R before\nMany packages, like lavaan, lme4, and any bootstrapping have an argument called parallel, with values like TRUE/FALSE or “fork”/“multisession”/“multicore”\nThis just means that they are using one of many available packages / tools to speed up the estmation of whatever your function is doing"
  },
  {
    "objectID": "09-week9-workbook.html#why-use-future-for-parallelization",
    "href": "09-week9-workbook.html#why-use-future-for-parallelization",
    "title": "Week 9 Workbook",
    "section": "Why use future for parallelization?",
    "text": "Why use future for parallelization?\n\nA Unifying Parallelization Framework in R for Everyone\nRequire only minimal changes to parallelize existing R code\n“Write once, Parallelize anywhere\nSame code regardless of operating system and parallel backend\nLower the bar to get started with parallelization\nFewer decisions for the developer to make\nStay with your favorite coding style\nWorry-free: globals, packages, output, warnings, errors just work\nStatistically sound: Built-in parallel random number generation (RNG)\nCorrectness and reproducibility of highest priority"
  },
  {
    "objectID": "09-week9-workbook.html#three-atomic-building-blocks",
    "href": "09-week9-workbook.html#three-atomic-building-blocks",
    "title": "Week 9 Workbook",
    "section": "Three atomic building blocks",
    "text": "Three atomic building blocks\nThere are three atomic building blocks that do everything we need:\n\n\nf  <- future(expr) : evaluates an expression via a future (non-blocking, if possible)\n\nrs <- resolved(f) : TRUE if future is resolved, otherwise FALSE (non-blocking)\n\nv  <- value(f) : the value of the future expression expr (blocking until resolved)"
  },
  {
    "objectID": "09-week9-workbook.html#example",
    "href": "09-week9-workbook.html#example",
    "title": "Week 9 Workbook",
    "section": "Example",
    "text": "Example\nTo break down what’s happening, let’s use a bad function, called slow_sum()\n\nCodeslow_sum <- function(x) {\n  sum <- 0\n  for (kk in seq_along(x)) {\n    sum <- sum + x[kk]\n    Sys.sleep(0.1)  # emulate 0.1 second cost per addition\n  }\n  sum\n}\n\n\nCredit: future tutorial useR 2022"
  },
  {
    "objectID": "09-week9-workbook.html#example-1",
    "href": "09-week9-workbook.html#example-1",
    "title": "Week 9 Workbook",
    "section": "Example",
    "text": "Example\nIf we then call, the following, it takes about 10 seconds\n\nCodex <- 1:100\nv <- slow_sum(x)\nv\n\n[1] 5050\n\nCode#> [1] 5050\n\n\nBut we could do the same in future and see that time to evaluate has been cut in half:\n\nCodelibrary(future)\nplan(multisession) # evaluate futures in parallel\n\nx <- 1:100\nf <- future(slow_sum(x))\nv <- value(f)\n#> [1] 5050"
  },
  {
    "objectID": "09-week9-workbook.html#anatomy-of-future",
    "href": "09-week9-workbook.html#anatomy-of-future",
    "title": "Week 9 Workbook",
    "section": "Anatomy of future()\n",
    "text": "Anatomy of future()\n\nWhen we call:\n\nCodef <- future(slow_sum(x))\n\n\nthen:\n\na future is created, comprising:\n\nthe R expression slow_sum(x),\nfunction slow_sum(), and\ninteger vector x\n\n\nThese future components are sent to a parallel worker, which starts evaluating the R expression\nThe future() function returns immediately a reference f to the future, and before the future evaluation is completed"
  },
  {
    "objectID": "09-week9-workbook.html#anatomy-of-value",
    "href": "09-week9-workbook.html#anatomy-of-value",
    "title": "Week 9 Workbook",
    "section": "Anatomy of value()\n",
    "text": "Anatomy of value()\n\nWhen we call:\n\nCodev <- value(f)\n\n\nthen:\n\nthe future asks the worker if it’s ready or not (using resolved() internally)\nif it is not ready, then it waits until it’s ready (blocking)\nwhen ready, the results are collected from the worker\nthe value of the expression is returned"
  },
  {
    "objectID": "09-week9-workbook.html#benefits-of-future",
    "href": "09-week9-workbook.html#benefits-of-future",
    "title": "Week 9 Workbook",
    "section": "Benefits of future:",
    "text": "Benefits of future:\n\nYou can keep doing other things while your code runs in the background and then eventually check whether it’s done using resolved() or value()\n\nYou can do multiple different futures at the same time"
  },
  {
    "objectID": "09-week9-workbook.html#benefits-of-future-1",
    "href": "09-week9-workbook.html#benefits-of-future-1",
    "title": "Week 9 Workbook",
    "section": "Benefits of future:",
    "text": "Benefits of future:\nWhen we run code normally, we experience blocking, which means that the next line can’t run until the previous one is done.\n\nCodex_head <- head(x, 50)\nx_tail <- tail(x, 50)\n\nv1 <- slow_sum(x_head)         ## ~5 secs (blocking)\nv2 <- slow_sum(x_tail)         ## ~5 secs (blocking)\nv <- v1 + v2\n\n\nBut with future, we can parallelize and continue to run other things:\n\nCodef1 <- future(slow_sum(x_head)) ## ~5 secs (in parallel)\nf2 <- future(slow_sum(x_tail)) ## ~5 secs (in parallel)\n\n## Do other things\nz <- sd(x)\n\nv <- value(f1) + value(f2)     ## ready after ~5 secs"
  },
  {
    "objectID": "09-week9-workbook.html#setting-up-your-future-backend",
    "href": "09-week9-workbook.html#setting-up-your-future-backend",
    "title": "Week 9 Workbook",
    "section": "Setting up your future backend",
    "text": "Setting up your future backend\nTo fully make use of future, you need to understand:\n\nparallel backends\n“workers”\nglobals\n\npackages"
  },
  {
    "objectID": "09-week9-workbook.html#parallel-backends",
    "href": "09-week9-workbook.html#parallel-backends",
    "title": "Week 9 Workbook",
    "section": "Parallel backends",
    "text": "Parallel backends\n\nThere are multiple ways that you can run parallel processes in R that depend on:\n\nyour OS\nwhether you are running local or remote sessions\n\n\n\n\n\nplan(sequential): default, will block\n\nplan(multisession): parallel, no blocking\n(plan(future.batchtools::batchtools_slurm))\n(plan(future.callr::callr, workers = 4))\nMore to come"
  },
  {
    "objectID": "09-week9-workbook.html#workers",
    "href": "09-week9-workbook.html#workers",
    "title": "Week 9 Workbook",
    "section": "Workers",
    "text": "Workers\n\nRemember, R resources are finite\nAs a rule of thumb, you don’t want to call more resources than you have\n\n\nCodeparallel::detectCores()\n\n[1] 16"
  },
  {
    "objectID": "09-week9-workbook.html#workers-1",
    "href": "09-week9-workbook.html#workers-1",
    "title": "Week 9 Workbook",
    "section": "Workers",
    "text": "Workers\nin future, workers are basically the number of cores you want to use for parallel processing\n\nCodeplan(multisession, workers = 8)\nnbrOfWorkers()\n#> [1] 8\n\nplan(multisession, workers = 2)\nnbrOfWorkers()\n#> [1] 2"
  },
  {
    "objectID": "09-week9-workbook.html#workers-2",
    "href": "09-week9-workbook.html#workers-2",
    "title": "Week 9 Workbook",
    "section": "Workers",
    "text": "Workers\nin future, workers are basically the number of cores you want to use for parallel processing\n\nCodeplan(multisession, workers = 2)\nnbrOfWorkers()\n#> [1] 2\n\nf1 <- future(slow_sum(x_head))\nf2 <- future(slow_sum(x_tail))\nf3 <- future(slow_sum(1:200))   ## <= blocks here\n\nresolved(f1)\n#> [1] TRUE\nresolved(f2)\n#> [1] TRUE\nresolved(f3)\n#> [1] FALSE"
  },
  {
    "objectID": "09-week9-workbook.html#globals",
    "href": "09-week9-workbook.html#globals",
    "title": "Week 9 Workbook",
    "section": "Globals",
    "text": "Globals\n-globals is an argument for future(). By default, it is set to TRUE - Alternatively, it could be a character vector including only those globals you want the future() call to have access to\n\nCodex_head <- head(x, 50)\nx_tail <- tail(x, 50)\n\nplan(multisession, workers = 2)\nf1 <- future(slow_sum(x_head), globals = c(\"slow_sum\", \"x_head\"))\nf2 <- future(slow_sum(x_tail), globals = c(\"slow_sum\", \"x_head\")) ## doesn't work"
  },
  {
    "objectID": "09-week9-workbook.html#packages",
    "href": "09-week9-workbook.html#packages",
    "title": "Week 9 Workbook",
    "section": "Packages",
    "text": "Packages\n\n\npackages is an argument for future(). By default, it is set to NULL\n\nAlternatively, it could be a character vector including only those packages you want to import into the parallel calls\n\nUseful when working with package conflicts, remote clusters"
  },
  {
    "objectID": "09-week9-workbook.html#furrr",
    "href": "09-week9-workbook.html#furrr",
    "title": "Week 9 Workbook",
    "section": "furrr",
    "text": "furrr\n\nI personally use furrr frequently when I need to a bunch of stuff in parallel but not so much I find myself needing to reach for an HPC\nWhy? It works just like purrr functions but allows me to run them in parallel!\n\n\nCodeplan(multisession, workers = 2)\n\n1:10 %>%\n  future_map(rnorm, n = 10, .options = furrr_options(seed = 123)) %>%\n  future_map_dbl(mean)"
  },
  {
    "objectID": "09-week9-workbook.html#furrr-1",
    "href": "09-week9-workbook.html#furrr-1",
    "title": "Week 9 Workbook",
    "section": "furrr",
    "text": "furrr\n\nI also use furrr because it works just like future:\n\nfuture_map(.x, .f, .options = furrr_options())\n\n\nfurrr_options() basically takes all the same arguments a typical future() call would take, including globals and packages\n\n\nSo with very little experience, you can shift an existing purrr workflow (or pieces of it) to parallel"
  },
  {
    "objectID": "09-week9-workbook.html#exercise-1",
    "href": "09-week9-workbook.html#exercise-1",
    "title": "Week 9 Workbook",
    "section": "Exercise",
    "text": "Exercise\nAs a brief exercise, let’s revisit some of the latent growth models we ran last week:\n\nCodegsoep2_lavaan <- readRDS(\"week9-data.RDS\")"
  },
  {
    "objectID": "09-week9-workbook.html#exercise-2",
    "href": "09-week9-workbook.html#exercise-2",
    "title": "Week 9 Workbook",
    "section": "Exercise",
    "text": "Exercise\nBelow is the lavaan syntax we ran:\n\nCodemod <- '\n  W1 =~ NA*I1_2005 + lambda1*I1_2005 + lambda2*I2_2005 + lambda3*I3_2005\n  W2 =~ NA*I1_2009 + lambda1*I1_2009 + lambda2*I2_2009 + lambda3*I3_2009\n  W3 =~ NA*I1_2013 + lambda1*I1_2013 + lambda2*I2_2013 + lambda3*I3_2013\n  \n  i =~ 1*W1 + 1*W2 + 1*W3\n  s =~ -1*W1 + 0*W2 + 1*W3\n  \n  ## intercepts\n  I1_2005 ~ t1*1\n  I1_2009 ~ t2*1\n  I1_2013 ~ t3*1\n  \n  I2_2005 ~ t1*1\n  I2_2009 ~ t2*1\n  I2_2013 ~ t3*1\n  \n  I3_2005 ~ t1*1\n  I3_2009 ~ t2*1\n  I3_2013 ~ t3*1\n  \n  ## correlated residuals across time\n  I1_2005 ~~ I1_2009 + I1_2013\n  I1_2009 ~~ I1_2013\n  I2_2005 ~~ I2_2009 + I2_2013\n  I2_2009 ~~ I2_2013\n  I3_2005 ~~ I3_2009 + I3_2013\n  I3_2009 ~~ I3_2013\n  \n  ## latent variable intercepts\n  W1 ~ 0*1\n  W2 ~ 0*1\n  W3 ~ 0*1\n  \n  #model constraints for effect coding\n  ## loadings must average to 1\n  lambda1 == 3 - lambda2 - lambda3\n  ## means must average to 0\n  t1 == 0 - t2 - t3\n  '"
  },
  {
    "objectID": "09-week9-workbook.html#exercise-3",
    "href": "09-week9-workbook.html#exercise-3",
    "title": "Week 9 Workbook",
    "section": "Exercise",
    "text": "Exercise\nAnd the function we ran:\n\nCodelavaan_fun <- function(d, trait){\n  m <- growth(\n    mod\n    , data = d\n    , missing = \"fiml\"\n  )\n  # saveRDS(m, file = sprintf(\"results/models/%s.RDS\", trait))\n  return(m)\n}"
  },
  {
    "objectID": "09-week9-workbook.html#exercise-4",
    "href": "09-week9-workbook.html#exercise-4",
    "title": "Week 9 Workbook",
    "section": "Exercise",
    "text": "Exercise\nBut instead of running the model using map2(), let’s run it using future_map2()\n\nCodestart <- Sys.time()\ngsoep_nested2 <- gsoep2_lavaan %>%\n  group_by(category, trait) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(m = map2(data, trait, lavaan_fun))\nend <- Sys.time()\nprint(end - start)\n\nTime difference of 6.229419 secs\n\nCode# > Time difference of 4.413002 secs"
  },
  {
    "objectID": "09-week9-workbook.html#exercise-5",
    "href": "09-week9-workbook.html#exercise-5",
    "title": "Week 9 Workbook",
    "section": "Exercise",
    "text": "Exercise\nIn this case, we’re only saving a few seconds, but in the case having many more models or models that run longer, this can HUGELY add up\n\nCodestart <- Sys.time()\nplan(multisession, workers = 5L)\ngsoep_nested2 <- gsoep2_lavaan %>%\n  group_by(category, trait) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(m = future_map2(data, trait, lavaan_fun))\nend <- Sys.time()\nprint(end - start)\n\nTime difference of 3.919519 secs\n\nCode# Time difference of 2.993128 secs"
  },
  {
    "objectID": "09-week9-workbook.html#caution-data-transfer",
    "href": "09-week9-workbook.html#caution-data-transfer",
    "title": "Week 9 Workbook",
    "section": "Caution: Data transfer",
    "text": "Caution: Data transfer\n\nThe goal of furrr is to combine purrr’s family of mapping functions with future’s parallel processing capabilities. The result is near drop in replacements for purrr functions such as map() and map2_dbl(), which can be replaced with their furrr equivalents of future_map() and future_map2_dbl() to map in parallel.\n\n\nBetter alternative: don’t export large objects using furrr but save the output to a local environment that can be loaded in later"
  },
  {
    "objectID": "09-week9-workbook.html#next-week-review",
    "href": "09-week9-workbook.html#next-week-review",
    "title": "Week 9 Workbook",
    "section": "Next Week: Review",
    "text": "Next Week: Review\n\nNext week, we’ll wrap up with a one hour “overview” highlighting big takeaways, reminders, etc. for the course\nThen, everyone will have the chance to (optionally) share their favorite R hacks (hint: this is a good excuse / nudge to remember the bonus points you can get by submitting tidy tuesday style code)\nFinally, we’ll have time to work on final projects"
  },
  {
    "objectID": "ps9-week9.html",
    "href": "ps9-week9.html",
    "title": "Problem Set Week 8",
    "section": "",
    "text": "Due Date: Friday, December 15, 12:01 AM PST.\nDownload your problem set for week 9 below or on Canvas.\nAnswers will be posted after the due date."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "After successful completion of this course, you will be able to:\n1.    Build your own research workflow that can be ported to future projects. \n2.    Learn new programming skills that will help you efficiently, accurately, and deliberately clean and manage your data.  \n3.    Create a bank of code and tools that can be used for a variety of types of research."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\nThere is no official textbook for this course (but if there was, it’d be Wickham, Cetinkaya-Rundel, & Grolemund’s R for Data Science [2nd edition]). However, many of you are coming in with different levels of knowledge and different types of questions, so I am providing some suggested readings below. \nI have arranged for students in this course to receive free access to Data Camp, a library of R (other programming languages) tutorials. Sign up using your UC Davis email here. \nWe will pull from the following two (freely available) books:\nHadley Wickham & Garret Grolemund: R for Data Science\nHadley Wickham: Advanced R \nAll course materials comply with copyright/fair use policies."
  },
  {
    "objectID": "syllabus.html#technology-requirements",
    "href": "syllabus.html#technology-requirements",
    "title": "Syllabus",
    "section": "Technology Requirements",
    "text": "Technology Requirements\nThe lecture presentations, links to articles, assignments, and rubrics are located on this Canvas site for the course and on the Quarto site. To participate in learning activities and complete assignments, you will need:\n\nAccess to a working computer that has a current operating system with updates installed;\nReliable Internet access and a UCD email account;\nA current Internet browser that is compatible with Canvas;\nR and R Studio (see below)\nReliable data storage for your work, such as Box, Office 365, or a USB drive.\n\nWe will do all of our data cleaning work in this class using the R programming language. We will use RStudio to interface with R console for a more user-friendly experience.\nPlease install both R and RStudio before the first day of class. Here’s how:\n\nGet the most recent version of R (free). Download the version of R compatible with your operating system (Mac, Linux, or Windows). If you are running Windows or MacOS, you should choose one of the precompiled binary distributions (i.e., ready-to-run applications; .exe for windows or .pkg for Mac) linked at the top of the R Project’s webpage.\nOnce R is installed, download and install R Studio (soon to be Pivot). R Studio is an “Integrated Development Environment”, or IDE. This means it is a front-end for R that makes it much easier to work with. R Studio is also free, and available for Windows, Mac, and Linux platforms.\nInstall the tidyverse library and several other add-on packages for R. These are sets of tolls or functions that will aid us in cleaning and wrangling data, and more. This is a non-exhaustive list that will get us started.\n\n\nmy_packages <- c(\n  \"plyr\", \"tidyverse\", \"furrr\", \"broom\",\n  \"MASS\", \"quantreg\", \"rlang\", \"scales\",\n  \"survey\", \"srvyr\", \"devtools\", \"future\"\n)\n\ninstall.packages(my_packages, repos = \"http://cran.rstudio.com\")"
  },
  {
    "objectID": "syllabus.html#minimum-technical-skills-needed",
    "href": "syllabus.html#minimum-technical-skills-needed",
    "title": "Syllabus",
    "section": "Minimum Technical Skills Needed",
    "text": "Minimum Technical Skills Needed\nMinimum technical skills are needed in this course. All work in this course must be completed and submitted online through Canvas and all assignments will be completed in R / Rmarkdown / Quarto. Therefore, you must have consistent and reliable access to a computer and the Internet.\nThe basic technical skills you have include the ability to:\n\nOrganize and save electronic files;\nUse UCD email and attached files;\nCheck email and Canvas a few times / week;\nDownload and upload documents;\nLocate information with a browser; and\nUse Canvas.\n\nHowever, you will spend about 50% of this course using R. Therefore, to get the most out of this class, I highly recommend having a better-than-beginner understanding or and experience with the R programming language. R is a skill, just like understanding the components of quality data and workflows, and for the purposes of this course, both are equally necessary and important. If you have any concerns about whether your R skills are strong enough for the course, please talk to the instructor or consider taking the course in a future year."
  },
  {
    "objectID": "syllabus.html#course-assignments-and-grading",
    "href": "syllabus.html#course-assignments-and-grading",
    "title": "Syllabus",
    "section": "Course Assignments and Grading",
    "text": "Course Assignments and Grading\n\nGeneral Assignment Information\n\nAll coursework (assignments) is secured in Canvas with a username and password.\nAll assignments are due on the day indicated on the course schedule.\nComplete rubrics (final project presentations and paper only) will be provided in Canvas.\n\n\n\nWeekly Assignments\nThe goal of this course is not simply to teach you how to clean hypothetical or convenient data. Rather, the goal is to teach you principles of good, accurate, reproducible, and efficient data cleaning and management, how to identify features of high quality data, and how to produce results of analyses efficiently.\nWeekly homework (40%) in this class will focus on programming concepts from that week. Each week, you will complete one problem set, applying the skills you learned that week to your own data. Submit each of these via Canvas by midnight the Sunday before class.\nThese will be graded for completion (you turned it in), relevance (it should be clear that you actually tried to do what you asked), and effort (please show your work). You will not receive feedback on them unless there is an ongoing problem (e.g., lack of depth or effort).\nThis is good opportunity to:\n\nBetter understand challenges with your own data (relative to others)\nReflect on features of your current workflow you like or dislike\nCritique your own work and note ideas to improve (I will probably do this a lot in class!).\nCreate a repository of ideas and code for future research.\n\n\n\nFinal Exam\nThe final exam for this course is instead a final project, due at the day and time of the scheduled final exam. The last day of the course will (likely) be used for presentations on the final project in order to receive feedback from the class and instructor.\nAdditional information on the project will be provided as a separate document on Canvas, announced in week 4 or 5. The project will not be long and the goal will be for you create a document outlining your workflow.\nTo ensure that your workflows are as effective as possible, this will proceed in three parts:\n\nInitial proposal of an idea submitted via Canvas.\nUpdated proposal submitted via Canvas.\n5-10 minute presentation to the class on the last day of the course (10% of your grade).\n\nFinal Project (20%).\n\n\nExtra Credit\n\nParticipate in a https://www.tidytuesday.com.\n2 pt extra credit for each one you participate in (max 6 pt total).\nCan post on Twitter or just create a document with the code and output\nSubmit on Canvas\n\nIf posting, link the post in in your submission\nIf not posting, attach the knitted file\n\n\n\n\nEvaluation and Grading Scale\nAll grades will be posted on Canvas. You are strongly encouraged to check your scores in Canvas regularly. A final letter grade will be assigned based on percentages.\n\n\n\nAssignment Weights\nPercent\n\n\nClass Participation\n20%\n\n\nProblem Sets\n40%\n\n\nFinal Project Proposal\n10%*\n\n\nClass Presentation\n10%*\n\n\nFinal Project\n20%*\n\n\nTotal\n100%\n\n\n\n* If presentations are omitted, proposals will be worth 15% and Final Projects 25%.\n\nGrading Scale\n92.5% - 100% = A; 89.5% - 92.4% = A-\n87.5% - 89.4% = B+; 82.5% - 87.4% = B; 79.5% - 82.4% = B-\n77.5% - 79.4% = C+; 72.5% - 77.4% = C; 69.5% - 72.4% = C-\n67.5% - 69.4% = D+; 62.5% - 67.4% = D; 59.5% - 62.4% = D-\n0% - 59.4% = F"
  },
  {
    "objectID": "syllabus.html#course-policies-and-procedures",
    "href": "syllabus.html#course-policies-and-procedures",
    "title": "Syllabus",
    "section": "Course Policies and Procedures",
    "text": "Course Policies and Procedures\nMany of the below are also outlined in the UC Davis Code of Academic Conduct.\n\nAttendance Policy\nWhen you miss class, you miss important information, not all of which will be available in the zoom recordings. This course is only 10 class meetings, so each meeting comprises 10% of your in-class time. If you need to miss more than one class, I suggest considering whether taking this course in a future term. I will teach this course either annually or biennially, so there will be future opportunities to take this course in many cases (e.g., for example, if you are a second year student who will miss two meetings, taking the course in your fourth year may be more effective).\n\n\nLate Work/Make-up Policy\nLate work will be allowed per instructor discretion. Please try to proactively communicate these needs. Assignments due at midnight will have a 9 hour “grace period” with no penalty. Each day late is subject to a 20% drop in course grade (e.g., a 10-point response is worth 8 points on day 1 late, 6 points on day 2 late, etc.).\n\n\nAcademic Integrity\nYou are expected to practice the highest possible standards of academic integrity. Any deviation from this expectation will result in a minimum academic penalty of your failing the assignment, and will result in additional disciplinary measures. This includes improper citation of sources, using another student’s work, and any other form of academic misrepresentation.\n\nPlagiarism\nUsing the words or ideas of another as if they were one’s own is a serious form of academic dishonesty. If another person’s complete sentence, syntax, key words, or the specific or unique ideas and information are used, one must give that person credit through proper citation.\n\n\n\nIncomplete Grades\nYou may assigned an ‘I’ (Incomplete) grade if you are unable to complete some portion of the assigned course work because of an unanticipated illness, accident, work-related responsibility, family hardship, or verified learning disability. An Incomplete grade is not intended to give you additional time to complete course assignments or extra credit unless there is indication that the specified circumstances prevented you from completing course assignments on time.\n\n\nInstructional Methods\nThe course will be taught using multiple instructional methods. I will typically briefly (45-50 minutes) lecture at the beginning of the class on conceptual topics related to data cleaning and management. We will then have a 75 minute workshop, which will be a mix of going through code and examples together and working in small groups (if preferred) on short exercises. The remainder of the class will be available to receive support on Problem Sets for that week and other general questions (optional). The proportion of these will vary by week and portions of the course will be shortened or dropped as needed.\n\n\nDiversity and Inclusion\nThe university is committed to a campus environment that is inclusive, safe, and respectful for all persons. To that end, all course activities will be conducted in an atmosphere of friendly participation and interaction among colleagues, recognizing and appreciating the unique experiences, background, and point of view each student brings. You are expected at all times to apply the highest academic standards to this course and to treat others with dignity and respect.\n\nAccessibility, Disability, and Triggers [credit to Dr. David Moscowitz]\nI am committed to ensuring course accessibility for all students. If you have a documented disability and expect reasonable accommodation to complete course requirements, please notify me at least one week before accommodation is needed. Please also provide SDRC (https://sc.edu/about/offices_and_divisions/student_disability_resource_center/) documentation to me before requesting accommodation. Likewise, if you are aware of cognitive or emotional triggers that could disrupt your intellectual or mental health, please let me know so that I can be aware in terms of course content. \nAbsences for Personal or Religious Holidays\nI am committed to allowing students to exercise their rights to religious freedom. Accommodations on assignment due dates and absences will be allowed for students observing religious holidays that fall on course days. Please email me to let me know ahead of time to allow for accommodations to be made.\n\n\nTitle IX and Gendered Pronouns [credit to Dr. David Moscowitz]\nThis course affirms equality and respect for all gendered identities and expressions. Please don’t hesitate to correct me regarding your preferred gender pronoun and/or name if different from what is indicated on the official class roster. Likewise, I am committed to nurturing an environment free from discrimination and harassment. Consistent with Title IX policy, please be aware that I as a responsible employee am obligated to report information that you provide to me about a situation involving sexual harassment or assault. \n\n\nValues [credit to Dr. David Moscowitz]\nTwo core values, inquiry and civility, govern our class. Inquiry demands that we all cultivate an open forum for exchange and substantiation of ideas. Strive to be creative, to take risks, and to challenge our conventional wisdom when you see the opportunity. Civility supports our inquiry by demanding ultimate respect for the voice, rights, and safety of others. Threatening or disruptive conduct may result in course and/or university dismissal. Civility also presumes basic courtesy: please be well rested, on time, and prepared for class (class time also includes a break to use the restroom, etc.), which includes silencing all personal devices. \nMy perspective is that we never cease being students of this world, so I believe that attentive, reflective people always have something to learn from others. Good discussions can be energetic and passionate but are neither abusive nor offensive. Vibrant, vigorous inquiry derives from discussions that:\n\nchallenge, defend, and apply different ideas, theories, perspectives, and skills,\nextend a body of knowledge into different arenas and applications, and\nresult in a synergy that compels us to seek resolution to these discussions.\n\n\n\n\nCopyright/Fair Use\nI will cite and/or reference any materials that I use in this course that I do not create.  You, as students, are expected to not distribute any of these materials, resources, homework assignments, etc. (whether graded or ungraded) without permission from the instructor."
  },
  {
    "objectID": "10-week10-slides.html",
    "href": "10-week10-slides.html",
    "title": "Week 9 Slides",
    "section": "",
    "text": "Welcome to Week 9! This week, we’ll talk about building Git, GitHub, parallelization, future, and furrr."
  },
  {
    "objectID": "04-week4-workbook.html",
    "href": "04-week4-workbook.html",
    "title": "Week 4 Workbook",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()     masks psych::%+%()\n✖ ggplot2::alpha()   masks psych::alpha()\n✖ dplyr::arrange()   masks plyr::arrange()\n✖ purrr::compact()   masks plyr::compact()\n✖ dplyr::count()     masks plyr::count()\n✖ dplyr::desc()      masks plyr::desc()\n✖ dplyr::failwith()  masks plyr::failwith()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::id()        masks plyr::id()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::mutate()    masks plyr::mutate()\n✖ dplyr::rename()    masks plyr::rename()\n✖ dplyr::summarise() masks plyr::summarise()\n✖ dplyr::summarize() masks plyr::summarize()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
  },
  {
    "objectID": "04-week4-workbook.html#preregistration",
    "href": "04-week4-workbook.html#preregistration",
    "title": "Week 4 Workbook",
    "section": "Preregistration",
    "text": "Preregistration\n\n\n\nPreregistration:\n\nSpecifying your study design, research questions, hypotheses, data cleaning, analytic plan, inference criteria, and robustness checks in advance\n\n\nWhy should you preregister?\n\nBadges are fun\nPreregistrations are not rigid but a chance to think through the questions you want to ask and answer and the challenges that might arise in doing so\nBuilds trust in the scientific process\n\n\n\n\n\n\n\n\nPreregistration is hard\n\nSpecifying your plan in advance takes considerable effort and time, which can feel like very slow science\n\n\nPreregistration is worthwhile\n\nBut preregistering plans, code, etc. can speed up the analytic portion of your research workflow, which builds great momentum for writing and submitting projects\n\n\n\nWhat should I preregister?\n\nDepends on the project, some examples include study design, individual research projects, etc.\n\nStudy design: A large survey is collected or a multi-part experiment is conducted. Measures, design, some research questions and hypotheses are specified a priori\nIndividual paper / project: A single-part survey or experiment is conducted or a specific piece of a multi-part study is investigated. If part of a multi-part study/experiment, should be linked to the parent preregistration\n\n\n\n\n\n\n\n\n\n\n\nLearning More:"
  },
  {
    "objectID": "04-week4-workbook.html#protocol-and-design-flow",
    "href": "04-week4-workbook.html#protocol-and-design-flow",
    "title": "Week 4 Workbook",
    "section": "Protocol and Design Flow",
    "text": "Protocol and Design Flow\n\nProcedure sections in scientific papers are meant to map out, as concisely and simply as possible, how data were obtained (adhering to human subjects ethical codes, etc.)\n\nBut such sections are not sufficient to replicate or reproduce research because study designs are much more intricate and include many more details than what fits in a method section\n\ne.g. measures not used because they weren’t focal, the code tha tunderlies how data are collected, preprocessing, etc.\n\n\nAs researchers, it’s our job to make sure that the work we do is documented so well that someone could replicate our studies.\nThink of it sort of like doing your taxes. You want to keep enough information that if you were audited, you would be able to quickly and easily provide all the relevant information.\nWhat you need to document will depend on the kind of work you do.\n\nAs an example, in my ecological momentary assessment work, I do the following:\n\nPreregister the design\nWrite a methods section that includes text for every measure included in any part of the study as well as an extended and detailed procedure description. This also includes information on how data will be cleaned and composited\nDetailed codebook including all measures that were collected, regardless of whether I have research questions or hypotheses for them. This is shareable for anyone who wants to use the data\nMake technical workflow. This documents how all documents, scripts, etc. work together to produce the final result, including what is automated, what requires researcher action, etc.\nComment all code and documents extensively\nDeviations document, where I document every deviation from my initial plans after the design is complete and data begin to be collected (or analyses start)\n\n\nExtensive documentation is also an investment in future you! My measures and procedures section basically write themselves, and my analytic plan is written in the preregistration\nThis both means that I’m faster and more efficient at writing these and that I feel more confident about the design choices I made, which is a win-win"
  },
  {
    "objectID": "04-week4-workbook.html#codebooks",
    "href": "04-week4-workbook.html#codebooks",
    "title": "Week 4 Workbook",
    "section": "Codebooks",
    "text": "Codebooks\n\nFor me, codebooks are the most essential and important part of any research project\nCodebooks allow me to:\n\nparse through documentation and find all the variables I want\ndocument detailed information about each of those variables\nmake cleaning and compositing choices for each (e.g., renaming, recoding, removing missings, etc.)\ndifferentiate among the kind of variables I have (e.g., predictors, outcomes, covariates, manipulations, and other categories)\nPass all this information into R to aid in data cleaning\n\n\n\nExample Codebook\nIn this case, we are going to using some data from the German Socioeconomic Panel Study (GSOEP), which is an ongoing Panel Study in Germany. Note that these data are for teaching purposes only, shared under the license for the Comprehensive SOEP teaching dataset, which I, as a contracted SOEP user, can use for teaching purposes. These data represent select cases from the full data set and should not be used for the purpose of publication. The full data are available for free at https://www.diw.de/en/diw_02.c.222829.en/access_and_ordering.html.\nFor this tutorial, I created the codebook for you: Download, and included what I believe are the core columns you may need. Some of these columns may not be particularly helpful for every dataset.\nMy Core Codebook Columns\nHere are my core columns that are based on the original data:\n\n\n\ndataset name (dataset)\n\nhow I categorize the variables (category)\n\nhow I rename each item (item)\n\nhow I composite the variables (name)\n\noriginal variable name (old_name)\n\noriginal item text (item_text)\n\noriginal item values (scale)\n\n\n\n\nhow I will recode each item (in text; recode_desc)\n\nhow I will recode each item (in R; recode)\n\nwhether item is reverse coded (reverse)\n\nscale minimum (mini)\n\nscale maximum (maxi)\n\ntimeline of variable collection (year or wave)\n\nmeta name / never changing name (meta)\n\n\n\n\n\ndataset: this column indexes the name of the dataset that you will be pulling the data from. This is important because we will use this info later on (see purrr tutorial) to load and clean specific data files. Even if you don’t have multiple data sets, I believe consistency is more important and suggest using this.\ncategory: broad categories that different variables can be put into. I’m a fan of naming them things like “outcome”, “predictor”, “moderator”, “demographic”, “procedural”, etc. but sometimes use more descriptive labels like “Big 5” to indicate the model from which the measures are derived.\nname: label is basically one level lower than category. So if the category is Big 5, the label would be, or example, “A” for Agreeableness, “SWB” for subjective well-being, etc. This column is most important and useful when you have multiple items in a scales, so I’ll typically leave this blank when something is a standalone variable (e.g. sex, single-item scales, etc.).\nitem_name: This is the lowest level and most descriptive variable. It indicates which item in scale something is. So it may be “kind” for Agreebleness or “sex” for the demographic biological sex variable.\nold_name: this column is the name of the variable in the data you are pulling it from. This should be exact. The goal of this column is that it will allow us to select() variables from the original data file and rename them something that is more useful to us.\nitem_text: this column is the original text that participants saw or a description of the item.\nscale: this column tells you what the scale of the variable is. Is it a numeric variable, a text variable, etc. This is helpful for knowing the plausible range.\nrecode_text: sometimes, we want to recode variables for analyses (e.g. for categorical variables with many levels where sample sizes for some levels are too small to actually do anything with it). I use this column to note the kind of recoding I’ll do to a variable for transparency.\nrecode: I write the R code I’ll parse by reading my codebook into R into this column.\n\nHere are additional columns that will make our lives easier or are applicable to some but not all data sets:\n\n\nreverse: this column tells you whether items in a scale need to be reverse coded. I recommend coding this as 1 (leave alone) and -1 (reverse) for reasons that will become clear later.\n\n\nmini: this column represents the minimum value of scales that are numeric. Leave blank otherwise.\n\n\nmaxi: this column represents the maximum value of scales that are numeric. Leave blank otherwise.\n\n\nyear: for longitudinal data, we have several waves of data and the name of the same item across waves is often different, so it’s important to note to which wave an item belongs. You can do this by noting the wave (e.g. 1, 2, 3), but I prefer the actual year the data were collected (e.g. 2005, 2009, etc.)\n\n\nmeta: Some datasets have a meta name, which essentially means a name that variable has across all waves to make it clear which variables are the same. They are not always useful as some data sets have meta names but no great way of extracting variables using them. But they’re still typically useful to include in your codebook regardless."
  },
  {
    "objectID": "04-week4-workbook.html#download-example-codebook",
    "href": "04-week4-workbook.html#download-example-codebook",
    "title": "Week 4 Workbook",
    "section": "Download Example Codebook",
    "text": "Download Example Codebook\nBelow, let’s download the codebook we will use for this study, which will include all of the above columns. We’ll load it in later. For now, let’s explore it.\n\nCode# set the path\nwd <- \"https://github.com/emoriebeck/psc290-data-FQ23/raw/main/04-workshops/04-week4-readr\"\n\ndownload.file(\n  url      = sprintf(\"%s//codebook.xlsx\", wd), \n  destfile = \"codebook.xlsx\"\n  )\n\n\nCodebook Tab\nThe resulting codebook looks something like this:\n\n\nIn addition, to the codebook, I also document other overarching info three other tabs\nOverview Tab\n\n\nOverview just lists what variables I’m considering as serving different functions (e.g., demographics, covariates, moderators, predictors, outcomes, indepenedent variables, dependent variables, etc.)\n\n\nKey Tab\n\n\nKey helps me create tables that I’ll be able to use in R to help me rename things. This is super helpful for making final tables and figures!!\n\n\nSample Tab\n\n\nSample helps other people understand how you’re using the columns. This is generally good practice and also helpful if you have research assistants or collaborators helping you out!"
  },
  {
    "objectID": "04-week4-workbook.html#workspace",
    "href": "04-week4-workbook.html#workspace",
    "title": "Week 4 Workbook",
    "section": "Workspace",
    "text": "Workspace\n\nDownload the following .zip file with an R project\n\nWe’re going to walk through this script, and then you will spend the rest of class working on your problem set, which basically does the same with your own data.\n\nPackages\n\nCodelibrary(psych)\nlibrary(plyr)\nlibrary(tidyverse)\n\n\nCodebook\n\nCodeloadWorkbook_url <- function(url, sheet) {\n    temp_file <- tempfile(fileext = \".xlsx\")\n    download.file(url = url, destfile = temp_file, mode = \"wb\", quiet = TRUE)\n    readxl::read_excel(temp_file, sheet = sheet)\n}\n\nurl <- \"https://github.com/emoriebeck/psc290-data-FQ23/raw/main/04-workshops/04-week4-readr/codebook.xlsx\"\ncodebook <- loadWorkbook_url(url, sheet = \"codebook\") %>%\n  mutate(old_name = str_to_lower(old_name))\n\nkey <- loadWorkbook_url(url, sheet = \"Key\")\ntraits   <- key %>% filter(category == \"Big 5\")\noutcomes <- key %>% filter(category == \"out\")\ncovars   <- key %>% filter(category == \"dem\")\n\n\nLoad in Data\n\nCodevars <- codebook$old_name\nsoep <- read_csv(file = \"https://github.com/emoriebeck/psc290-data-FQ23/raw/main/04-workshops/04-week4-readr/soep.csv\") %>%\n  select(one_of(vars)) # keep vars from codebook\nhead(soep)"
  },
  {
    "objectID": "04-week4-workbook.html#data-cleaning",
    "href": "04-week4-workbook.html#data-cleaning",
    "title": "Week 4 Workbook",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nOnce I have my workspace setup, it’s time to clean the data\nFor the sake of time, I’m going to skip the descriptives we talked about last week, but we should be doing those in a real setting!\nI often have the following sections in my data cleaning section:\n\nRename Variables\n\nChange to Long\nBring in Codebook\n\n\nRecode Variables\nReverse Scoring\nPredictors (In this case, personality)\nOutcomes (In this case, life events)\nCovariates / Demographics\n[any other variable categories you have]\n\n\nI like to clean different categories of variables separately because I often clean them relatively similarly within categories. Differences within categories are generally captured via columns in my codebook\n\nRename Variables\nChange Data to Long\n\nTo get our codebook to play nice with the data since our data are in wide form (including across years), we need to make the data long with at least one name that corresponds to the codebook\nIn this case, we’ll all the variables but the participant and household ID’s long, and change the item name to old_name since it contains the original variable names.\n\n\nCode## change data to long format\nsoep_long <- soep %>%\n  pivot_longer(\n    cols = c(-persnr, -hhnr)\n    , names_to = \"old_name\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) %>%\n  rename(SID = persnr, HHID = hhnr)\nsoep_long\n\n\n\n  \n\n\n\nMerge in Codebook\n\nNow, let’s actually merge in the codebook. We’ll use left_join() here becuase we want to keep all observations in our raw soep_long data frame.\n\n\nCode# merge in codebook\nsoep_long <- soep_long %>% # long data\n  left_join( # keep all rows in long data\n    codebook %>% # merge in the following variables from the codebook\n      select(old_name, category, name, item_name, year, recode, \n             reverse, mini, maxi, comp_rule, long_rule)\n  ) %>%\n  select(-old_name) # get rid of old_name because we're done with it\n\n\nRecode Variables\n\nNow that we’ve merged our codebook and raw data, we’re ready to use the information in the codebook to:\n\nrecode (recode)\nreverse score (reverse, mini, maxi)\ncomposite (comp_rule, long_rule)\n\n\nI find the easiest way to recode variables, especially in projects where I may need to differently recode hundreds or thousands of different variables differently, is to write a little function that takes code chunks from my codebook and runs them.\nThe function looks like this:\n\n\nCoderecode_fun <- function(rule, y){\n  x <- y$value\n  if(!is.na(rule)){y$value <- eval(parse(text = rule))}\n  return(y)\n}\n\n\n\nNow we’re going to apply each rule to each chunk of the data that uses the same one\nTo do this, we’re going to use some functions from the purrr package that we won’t talk about it this week.\nDon’t worry too much about this. This is code you can copy paste or add to an R script you source across projects!\n\n\nCodesoep_recode <- soep_long %>%\n  group_by(recode) %>% # group by the r code rule\n  nest() %>% # create a nested data frame\n  ungroup() %>% # ungroup() \n  # apply the recode function\n  mutate(data = pmap(list(recode, data), recode_fun)) %>%\n  unnest(data) %>% # unnest the data to get back to a normal df\n  # change any negative, nan, or Inf values to NA\n  mutate(value = ifelse(value < 0 | is.nan(value) | is.infinite(value), NA, value)) %>%\n  select(-recode) # we're done with the recode column, so remove it\nsoep_recode\n\n\n\n  \n\n\n\nReverse Coding\n\nNow we can reverse code our data. We’ll use similar code to what I showed last week, but we’re working on long-format data, so we’ll use the reverse column to tell us which rows to reverse and what the mini and maxi scale values are.\n\n\nCodesoep_recode <- soep_recode %>%\n  mutate(value = ifelse(reverse == \"no\", value, \n         as.numeric(reverse.code(-1, value, mini = mini, maxi = maxi)))) %>%\n  select(-reverse, -mini, -maxi)\nsoep_recode\n\n\n\n  \n\n\n\nComposite Items\n\nNow that our data are recoded and reverse scored, we can clean each category of data:\n\nPersonality\nOutcomes / Life Events\nDemographics Covariates\n\n\nThen we’ll merge them together\nPersonality\n\nFor the Big Five, we want to get composites within years (2005, 2009, 2013)\nthe comp_rule is average, so want to get the mean\nthe long_rule is select because we’ll choose what to do with each year\n\n\nCodesoep_big5 <- soep_recode %>%\n  # keep Big Five & drop missings\n  filter(category == \"Big 5\" & !is.na(value)) %>%\n  # \"split\" the data by category, person, household, trait, item, and year\n  group_by(category, SID, HHID, name, item_name, year) %>%\n  # \"apply\" the mean function, collapsing within splits\n  summarize(value = mean(value)) %>%\n  # \"split\" the data by category, person, household, trait, and item\n  group_by(category, SID, HHID, name, year) %>%\n  # \"apply\" the mean function, collapsing within splits\n  summarize(value = mean(value)) %>%\n  # \"combine\" the data back together\n  ungroup()\nsoep_big5\n\n\n\n  \n\n\n\n\nBelow, you’ll see an alternate way to do this that uses a function\n\nWe haven’t learned purrr and functions yet, so we’re not quite ready for this yet\nBut keep this code and see if you can figure it out because it’s much more flexible and super useful in the case that you need to apply different rules to different variables (especially useful for covariates and moderators!!)\n\n\n\n\nCodeMode <- function(x) {\n  ux <- unique(x)\n  ux <- ux[!is.na(ux)]\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\nfun_call <- function(x, rule){\n    switch(rule,\n           average = mean(x, na.rm = T),\n           mode = Mode(x)[1],\n           sum = sum(x, na.rm = T),\n           skip = unique(x)[1],\n           select = unique(x)[1],\n           max = max(x, na.rm = T),\n           min = min(x, na.rm = T))\n}\n\n# compositing within years\nyear_comp_fun <- function(df, rule){\n  df %>%\n    # group by person and item (collapse across age)\n    group_by(SID, HHID, long_rule, name, item_name, year) %>% \n    summarize(value = fun_call(value, rule)) %>%\n    group_by(SID, HHID, long_rule, name, year) %>% \n    summarize(value = fun_call(value, rule)) %>%\n    ungroup() %>% \n    mutate(value = ifelse(is.infinite(value) | is.nan(value), NA, value))\n}\n\nsoep_big5 <- soep_recode %>%\n  filter(category == \"Big 5\" & !is.na(value)) %>%\n  group_by(category, comp_rule) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(data = map2(data, comp_rule, year_comp_fun)) %>%\n  unnest(data) %>%\n  select(-comp_rule, -long_rule)\nsoep_big5\n\n\nOutcomes\n\nNow onto life events\nFor these data we want to get a single composite for each life event across all years (i.e. did they ever experience each event)\nBoth the comp_rule and the long_rule are max\n\n\nCodesoep_out <- soep_recode %>%\n  # keep Life events & drop missings\n  filter(category == \"Life Event\" & !is.na(value)) %>%\n  # \"split\" the data by category, person, household, event, and year\n  group_by(SID, HHID, category, name, year) %>% \n  # \"apply\" the max function, collapsing within splits\n  summarize(value = max(value)) %>%\n  # \"split\" the data by category, person, household, event\n  group_by(SID, HHID, category, name) %>% \n  # \"apply\" the max function, collapsing within splits\n  summarize(value = max(value)) %>%\n  # \"combine\" the data back together\n  ungroup()\n\n\n\nAs before, the more flexible way is below.\n\n\nCodesoep_out <- soep_recode %>%\n  filter(category == \"Life Event\" & !is.na(value)) %>%\n  group_by(category, comp_rule) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(data = map2(data, comp_rule, year_comp_fun)) %>%\n  unnest(data) %>%\n  select(-comp_rule)\n\n\ncomp_fun <- function(data, rule){\n  data %>%\n    group_by(SID, HHID, name) %>%\n    summarize(value = fun_call(value, rule)) %>%\n    ungroup()\n}\n\nsoep_out <- soep_out %>%\n  group_by(long_rule) %>%\n  nest() %>%\n  mutate(data = map2(data, long_rule, comp_fun)) %>%\n  unnest(data) %>%\n  select(-long_rule)\n\n\nCovariates\n\nNow let’s do the covariates\nThe comp_rule and long_rule columns tell us that the two variables actually have the same rule (mode), which make it easy to clean\nAs before, the more flexible way is in the workbook, which is particularly useful for demographics and covariates that may be on super different scales\n\n\nCodeMode <- function(x) {\n  ux <- unique(x)\n  ux <- ux[!is.na(ux)]\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\nsoep_cov <- soep_recode %>%\n  # keep demographics & drop missings\n  filter(category == \"Demographic\" & !is.na(value)) %>%\n  # \"split\" the data by category, person, household, covariate, and year\n  group_by(category, SID, HHID, name, year) %>%\n  # \"apply\" the Mode function, collapsing within splits\n  summarize(value = Mode(value)) %>%\n  # \"split\" the data by category, person, household, and covariate\n  group_by(SID, HHID, name) %>%\n  # \"apply\" the Mode function, collapsing within splits\n  summarize(value = Mode(value)) %>%\n  # \"combine\" the data back together\n  ungroup() %>%\n  # pivot data wider so there are separate columns for each covariate\n  pivot_wider(\n    names_from = \"name\"\n    , values_from = \"value\"\n    )\nsoep_cov\n\n\n\n  \n\n\n\nMerge Data\n\nLastly, let’s re-merge the data to bring the information back together\nBecause we want the crossings of traits and life events, we’ll need to change the name and value columns to be specific to the variable categories\nWe want the data to look like this:\n\n\n\nSID\nHHID\nyear\nevent\no_value\ntrait\np_value\nsex\nDOB\n\n\n\n\nCodesoep_clean <- soep_big5 %>%\n  # select key variables and rename for personality\n  select(SID, HHID, year, trait = name, p_value = value) %>%\n  # bring in matching rows (by SID and HHID)\n  inner_join(\n    soep_out %>%\n      # select key variables and rename for outcomes\n      select(SID, HHID, event = name, o_value = value)\n    ) %>%\n  # bring the covariates\n  left_join(soep_cov)\nsoep_clean\n\n\n\n  \n\n\n\nSave Your Data\n\nCodewrite.csv(\n  x = soep_clean\n  , file = sprintf(\"clean_data_%s.csv\", Sys.Date())\n  , row.names = F\n  )"
  },
  {
    "objectID": "04-week4-workbook.html#note-why-did-we-do-the-keys",
    "href": "04-week4-workbook.html#note-why-did-we-do-the-keys",
    "title": "Week 4 Workbook",
    "section": "Note: Why did we do the keys?",
    "text": "Note: Why did we do the keys?\n\nCodesoep_clean %>%\n  mutate(trait = factor(trait, levels = traits$name, labels = traits$long_name)\n         , event = factor(event, levels = outcomes$name, labels = outcomes$long_name))\n\n\n\n  \n\n\n\n\nCodesoep_clean %>%\n  mutate(trait = factor(trait, levels = traits$name, labels = traits$long_name)\n         , event = factor(event, levels = outcomes$name, labels = outcomes$long_name)\n         , o_value = factor(o_value, levels = c(0,1), labels = c(\"No Event\", \"Event\"))) %>%\n  group_by(trait, event, o_value) %>%\n  summarize_at(vars(p_value), lst(mean, sd)) %>%\n  ungroup() %>%\n  ggplot(aes(x = mean, y = event, shape = o_value)) + \n    geom_errorbar(\n      aes(xmin = mean - sd, xmax = mean + sd)\n      , width = .1\n      , position = position_dodge(width=.8)\n      ) + \n  geom_point(position = position_dodge(width=.8)) + \n  facet_grid(~trait) + \n  theme_classic()"
  },
  {
    "objectID": "01-week1-slides.html",
    "href": "01-week1-slides.html",
    "title": "Week 1 Slides",
    "section": "",
    "text": "Welcome to Week 1! This week, we’re just getting started. You can download the slides code and slides here."
  },
  {
    "objectID": "08-week8-slides.html",
    "href": "08-week8-slides.html",
    "title": "Week 8 Slides",
    "section": "",
    "text": "Welcome to Week 8! This week, we’ll talk about building a functional workflow for tables and figures."
  },
  {
    "objectID": "02-week2-slides.html",
    "href": "02-week2-slides.html",
    "title": "Week 2 Slides",
    "section": "",
    "text": "Welcome to Week 2! This week, we’ll talk about R Projects, Reproducibility, and dplyr."
  },
  {
    "objectID": "ps8-week8.html",
    "href": "ps8-week8.html",
    "title": "Problem Set Week 8",
    "section": "",
    "text": "Due Date: Monday, November 27, 12:01 AM PST.\nDownload your problem set for week 8 below or on Canvas.\nAnswers will be posted after the due date."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Course Schedule\nNote: Course schedule is subject to change without notice.\n\n\n\nDay\nDate\nTopic\nDue Today\n\n\n\n\nFirst Day of Classes September 27\n\n\n\n1\n10/02/2023\nLecture: Basics of Workflow\nWorkshop: Introduction to R & Workflow Basics; Quarto\nReadings:\n-       r4ds: Ch. 3, 5, 7, 29\nZoom Recording\n\n\n\n2\n10/09/2023\nLecture: Reproducibility and Workflow Values\nWorkshop: Data Transformation: Introduction to dplyr\nReadings:\n-       r4ds: Ch. 4\nZoom Recording\nProblem Set 1 Due\n\n\n3\n10/16/2023\nLecture: Understanding and Assessing Data Quality\nWorkshop: Reshaping and Joining: Introduction to tidyr\nReadings:\n-       r4ds: Ch. 6\nZoom Recording\nProblem Set 2 Due\n\n\n4\n10/23/2023\nLecture: Documenting Data and Procedures\nWorkshop: Using Codebooks to Aid Data Import\nReadings:\n-       r4ds: Ch. 8, 21, 24\nZoom Recording in 2 parts due to technical issues (Part 1; Part 2)\nProblem Set 3 Due\n\n\n5\n10/30/2023\nLecture: Functions\nWorkshop: Iteration: Introduction to purrr\nReadings:\n-       r4ds: Ch. 26, 27\nZoom Recording\nProblem Set 4 Due\n\n\n6\n11/06/2023\nLecture: Review – Putting the Pieces of Your Workflow Together\nWorkshop: Review – tidyverse: Using codebooks, functions, and iteration within a tidyverse framework (a series of in-class activities)\nReadings: None\nZoom Recording\nProblem Set 5 Due\n\n\n7\n11/13/2023\nLecture: Data Structures in R\nWorkshop: Data Transformation: Dates, Strings, regex, and Other Tricky Classes\nReadings:\n-       r4ds: Ch. 13-19\nZoom Recording\nProblem Set 6 Due\n\n\n8\n11/20/2023\nLecture: GitHub and Versioning\nWorkshop: (Functional) Tables & Figures\nReadings:\n-       TBD\nZoom Recording\nProblem Set 7 Due\nPROPOSALS DUE\n\n\n\n\nThanksgiving Break\n\n\n\n9\n11/27/2023\nLecture: Using R resources efficiently\nWorkshop: Parallelization: Introduction to future and furrr\nReadings:\n-       https://dcgerard.github.io/advancedr/09_future.html\nZoom Recording\nProblem Set 8 Due\n\n\n10\n12/04/2023\nReview + Code hacks\n\n\n\n\n12/15/2023\nFinal Project Due Date"
  },
  {
    "objectID": "06-week6-workbook.html",
    "href": "06-week6-workbook.html",
    "title": "Week 6 - Review",
    "section": "",
    "text": "Loading required package: Matrix\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()      masks psych::%+%()\n✖ ggplot2::alpha()    masks psych::alpha()\n✖ dplyr::arrange()    masks plyr::arrange()\n✖ purrr::compact()    masks plyr::compact()\n✖ dplyr::count()      masks plyr::count()\n✖ dplyr::desc()       masks plyr::desc()\n✖ tidyr::expand()     masks Matrix::expand()\n✖ dplyr::failwith()   masks plyr::failwith()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::group_rows() masks kableExtra::group_rows()\n✖ dplyr::id()         masks plyr::id()\n✖ dplyr::lag()        masks stats::lag()\n✖ dplyr::mutate()     masks plyr::mutate()\n✖ tidyr::pack()       masks Matrix::pack()\n✖ dplyr::rename()     masks plyr::rename()\n✖ dplyr::summarise()  masks plyr::summarise()\n✖ dplyr::summarize()  masks plyr::summarize()\n✖ tidyr::unpack()     masks Matrix::unpack()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
  },
  {
    "objectID": "06-week6-workbook.html#select",
    "href": "06-week6-workbook.html#select",
    "title": "Week 6 - Review",
    "section": "1. select()\n",
    "text": "1. select()\n\nAdd or remove using select() helper functions.\n\n\nstarts_with()\n\nends_with()\ncontains()\nmatches()\nnum_range()\none_of()\nall_of()\n\n\nCodebfi |>\n  select(starts_with(\"C\"))"
  },
  {
    "objectID": "06-week6-workbook.html#filter",
    "href": "06-week6-workbook.html#filter",
    "title": "Week 6 - Review",
    "section": "2. filter()\n",
    "text": "2. filter()\n\n\nOften times, when conducting research (experiments or otherwise), there are observations (people, specific trials, etc.) that you don’t want to include.\nWe can use filter() with logical statements to include only rows that match certain conditions\n\nWe can refer to both bare quoted columns and objects in the global environment\n\n\n== or !=\n\n\n< or <=\n\n\n> or >=\n\n%in%\nall_of()\none_of()\n!\n\n| and &"
  },
  {
    "objectID": "06-week6-workbook.html#pivot_longer",
    "href": "06-week6-workbook.html#pivot_longer",
    "title": "Week 6 - Review",
    "section": "1. pivot_longer()\n",
    "text": "1. pivot_longer()\n\n\n(Formerly gather()) Makes wide data long, based on a key \n\nCore arguments:\n\n\ndata: the data, blank if piped\n\ncols: columns to be made long, selected via select() calls\n\nnames_to: name(s) of key column(s) in new long data frame (string or string vector)\n\nvalues_to: name of values in new long data frame (string)\n\nnames_sep: separator in column headers, if multiple keys\n\nvalues_drop_na: drop missing cells (similar to na.rm = T) \n\n\n\nWhy would I make my data longer?\n\nMain reason: Columns names sometimes contain data.\nExample: Billboard data has time information in column names\n\n\nCodedata(billboard)\nstr(billboard)\n\ntibble [317 × 79] (S3: tbl_df/tbl/data.frame)\n $ artist      : chr [1:317] \"2 Pac\" \"2Ge+her\" \"3 Doors Down\" \"3 Doors Down\" ...\n $ track       : chr [1:317] \"Baby Don't Cry (Keep...\" \"The Hardest Part Of ...\" \"Kryptonite\" \"Loser\" ...\n $ date.entered: Date[1:317], format: \"2000-02-26\" \"2000-09-02\" ...\n $ wk1         : num [1:317] 87 91 81 76 57 51 97 84 59 76 ...\n $ wk2         : num [1:317] 82 87 70 76 34 39 97 62 53 76 ...\n $ wk3         : num [1:317] 72 92 68 72 25 34 96 51 38 74 ...\n $ wk4         : num [1:317] 77 NA 67 69 17 26 95 41 28 69 ...\n $ wk5         : num [1:317] 87 NA 66 67 17 26 100 38 21 68 ...\n $ wk6         : num [1:317] 94 NA 57 65 31 19 NA 35 18 67 ...\n $ wk7         : num [1:317] 99 NA 54 55 36 2 NA 35 16 61 ...\n $ wk8         : num [1:317] NA NA 53 59 49 2 NA 38 14 58 ...\n $ wk9         : num [1:317] NA NA 51 62 53 3 NA 38 12 57 ...\n $ wk10        : num [1:317] NA NA 51 61 57 6 NA 36 10 59 ...\n $ wk11        : num [1:317] NA NA 51 61 64 7 NA 37 9 66 ...\n $ wk12        : num [1:317] NA NA 51 59 70 22 NA 37 8 68 ...\n $ wk13        : num [1:317] NA NA 47 61 75 29 NA 38 6 61 ...\n $ wk14        : num [1:317] NA NA 44 66 76 36 NA 49 1 67 ...\n $ wk15        : num [1:317] NA NA 38 72 78 47 NA 61 2 59 ...\n $ wk16        : num [1:317] NA NA 28 76 85 67 NA 63 2 63 ...\n $ wk17        : num [1:317] NA NA 22 75 92 66 NA 62 2 67 ...\n $ wk18        : num [1:317] NA NA 18 67 96 84 NA 67 2 71 ...\n $ wk19        : num [1:317] NA NA 18 73 NA 93 NA 83 3 79 ...\n $ wk20        : num [1:317] NA NA 14 70 NA 94 NA 86 4 89 ...\n $ wk21        : num [1:317] NA NA 12 NA NA NA NA NA 5 NA ...\n $ wk22        : num [1:317] NA NA 7 NA NA NA NA NA 5 NA ...\n $ wk23        : num [1:317] NA NA 6 NA NA NA NA NA 6 NA ...\n $ wk24        : num [1:317] NA NA 6 NA NA NA NA NA 9 NA ...\n $ wk25        : num [1:317] NA NA 6 NA NA NA NA NA 13 NA ...\n $ wk26        : num [1:317] NA NA 5 NA NA NA NA NA 14 NA ...\n $ wk27        : num [1:317] NA NA 5 NA NA NA NA NA 16 NA ...\n $ wk28        : num [1:317] NA NA 4 NA NA NA NA NA 23 NA ...\n $ wk29        : num [1:317] NA NA 4 NA NA NA NA NA 22 NA ...\n $ wk30        : num [1:317] NA NA 4 NA NA NA NA NA 33 NA ...\n $ wk31        : num [1:317] NA NA 4 NA NA NA NA NA 36 NA ...\n $ wk32        : num [1:317] NA NA 3 NA NA NA NA NA 43 NA ...\n $ wk33        : num [1:317] NA NA 3 NA NA NA NA NA NA NA ...\n $ wk34        : num [1:317] NA NA 3 NA NA NA NA NA NA NA ...\n $ wk35        : num [1:317] NA NA 4 NA NA NA NA NA NA NA ...\n $ wk36        : num [1:317] NA NA 5 NA NA NA NA NA NA NA ...\n $ wk37        : num [1:317] NA NA 5 NA NA NA NA NA NA NA ...\n $ wk38        : num [1:317] NA NA 9 NA NA NA NA NA NA NA ...\n $ wk39        : num [1:317] NA NA 9 NA NA NA NA NA NA NA ...\n $ wk40        : num [1:317] NA NA 15 NA NA NA NA NA NA NA ...\n $ wk41        : num [1:317] NA NA 14 NA NA NA NA NA NA NA ...\n $ wk42        : num [1:317] NA NA 13 NA NA NA NA NA NA NA ...\n $ wk43        : num [1:317] NA NA 14 NA NA NA NA NA NA NA ...\n $ wk44        : num [1:317] NA NA 16 NA NA NA NA NA NA NA ...\n $ wk45        : num [1:317] NA NA 17 NA NA NA NA NA NA NA ...\n $ wk46        : num [1:317] NA NA 21 NA NA NA NA NA NA NA ...\n $ wk47        : num [1:317] NA NA 22 NA NA NA NA NA NA NA ...\n $ wk48        : num [1:317] NA NA 24 NA NA NA NA NA NA NA ...\n $ wk49        : num [1:317] NA NA 28 NA NA NA NA NA NA NA ...\n $ wk50        : num [1:317] NA NA 33 NA NA NA NA NA NA NA ...\n $ wk51        : num [1:317] NA NA 42 NA NA NA NA NA NA NA ...\n $ wk52        : num [1:317] NA NA 42 NA NA NA NA NA NA NA ...\n $ wk53        : num [1:317] NA NA 49 NA NA NA NA NA NA NA ...\n $ wk54        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk55        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk56        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk57        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk58        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk59        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk60        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk61        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk62        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk63        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk64        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk65        : num [1:317] NA NA NA NA NA NA NA NA NA NA ...\n $ wk66        : logi [1:317] NA NA NA NA NA NA ...\n $ wk67        : logi [1:317] NA NA NA NA NA NA ...\n $ wk68        : logi [1:317] NA NA NA NA NA NA ...\n $ wk69        : logi [1:317] NA NA NA NA NA NA ...\n $ wk70        : logi [1:317] NA NA NA NA NA NA ...\n $ wk71        : logi [1:317] NA NA NA NA NA NA ...\n $ wk72        : logi [1:317] NA NA NA NA NA NA ...\n $ wk73        : logi [1:317] NA NA NA NA NA NA ...\n $ wk74        : logi [1:317] NA NA NA NA NA NA ...\n $ wk75        : logi [1:317] NA NA NA NA NA NA ...\n $ wk76        : logi [1:317] NA NA NA NA NA NA ...\n\n\n\nCodebillboard |>\n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    names_prefix = \"wk\",\n    names_transform = as.numeric,\n    values_to = \"rank\"\n  )\n\n\n\n  \n\n\n\n\nThis doesn’t just apply to longitudinal data. This is also important when thinking about iteration.\nFor example, if you variables can be grouped into different categories (covariates, IVs/predictors, DVs/outcomes, moderators, etc.), then your column names contain implicit data\nThe data below contain both time, variable, and category information\n\n\nCode# load the codebook\n(codebook <- read_csv(\"week6-codebook.csv\") |>\n    mutate(old_name = str_to_lower(old_name)))\n\n\n\n  \n\n\nCodeold.names <- codebook$old_name # get old column names\nnew.names <- codebook$new_name # get new column names\nsoep <- read_csv(\"week6-data.csv\") |>\n    select(all_of(old.names))\n\n\nExercise\n\nBy pivoting our data longer, we can more easily extract information from the column names\nPivot the data below longer. Some hints:\n\ndon’t make the procedural or demographic variables long\nsplit the column names you make long into four chunks:\n\n“category”, “label”, “item_name”, “year”\n\n\ndrop NA values\n\n\n\nSolution:\n\nCodesoep_long <- soep |>\n  setNames(new.names) |>\n  pivot_longer(\n    cols = c(-starts_with(\"Proc\"), -starts_with(\"Dem\"))\n    , names_to = c(\"category\",  \"label\",    \"item_name\",    \"year\")\n    , names_pattern = \"(.*)_(.*)_(.*)_(.*)\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) |> mutate(year = as.numeric(year))\nsoep_long\n\n\n\n  \n\n\n\n\nLong format data are easier to clean (we’ll come back to this but we’ll create the cleaned data frame to use for merging practice)\n\n\nCodesoep_big5 <- soep_long |>\n  filter(category == \"big5\") |>\n  mutate(value = mapvalues(value, seq(-8,0), rep(NA, 9))) |>\n  drop_na(value) |>\n  group_by(Proc_SID, label, year) |>\n  summarize(value = mean(value)) |>\n  ungroup()\n\nsoep_le <- soep_long |>\n  filter(category == \"le\") |>\n  mutate(value = mapvalues(value, seq(-8,1), c(rep(NA, 6), 0, NA, NA, 1))) |>\n  drop_na(value) |>\n  group_by(Proc_SID, label) |>\n  summarize(value = sum(value)) |>\n  ungroup()\n\nsoep_clean <- soep_big5 |>\n  rename(trait = label, p_value = value) |>\n  inner_join(\n    soep_le |>\n      rename(le = label, le_value = value)\n  )"
  },
  {
    "objectID": "06-week6-workbook.html#pivot_wider",
    "href": "06-week6-workbook.html#pivot_wider",
    "title": "Week 6 - Review",
    "section": "2. pivot_wider()\n",
    "text": "2. pivot_wider()\n\n\n(Formerly spread()) Makes wide data long, based on a key \n\nCore arguments:\n\n\ndata: the data, blank if piped\n\nnames_from: name(s) of key column(s) in new long data frame (string or string vector)\n\nnames_sep: separator in column headers, if multiple keys\n\nnames_glue: specify multiple or custom separators of multiple keys\n\nvalues_from: name of values in new long data frame (string)\n\nvalues_fn: function applied to data with duplicate labels \n\n\n\nWhy would I pivot wider?\n\nSome analyses require wide format data\nFor example, SEM in lavaan in R requires that both indicators and time are wide format.\nThe code below uses the codebook to rename items according to a numbered format that isn’t specific to any trait\n\n\nCodebig5 <- codebook |> filter(category == \"big5\")\nsoep_lavaan <- soep_long |>\n  filter(category == \"big5\") |>\n  mutate(item_name = mapvalues(item_name, big5$item_name, big5$lavaan_name, warn_missing = F))\nsoep_lavaan\n\n\n\n  \n\n\n\nExercise\n\nChange the soep_lavaan data frame to be in wide format using pivot_wider():\n\npull the names from two sources: item_name() and year\n\n\n\n\n\nCodesoep_lavaan |>\n  pivot_wider(\n    names_from = c(\"item_name\", \"year\")\n    , values_from = \"value\"\n  )"
  },
  {
    "objectID": "06-week6-workbook.html#the-_join-functions",
    "href": "06-week6-workbook.html#the-_join-functions",
    "title": "Week 6 - Review",
    "section": "The _join() Functions",
    "text": "The _join() Functions\n\nOften we may need to pull different data from different sources\nThere are lots of reasons to need to do this\nWe don’t have time to get into all the use cases here, so we’ll talk about them in high level terms\nWe’ll focus on:\n\nfull_join()\ninner_join()\nleft_join()\nright_join()"
  },
  {
    "objectID": "06-week6-workbook.html#full_join",
    "href": "06-week6-workbook.html#full_join",
    "title": "Week 6 - Review",
    "section": "3. full_join()\n",
    "text": "3. full_join()\n\n\nMost simply, we can put those back together keeping all observations.\nPro: sometimes we want to maintain missing data (i.e. some people are randomly missing variables and we don’t want to drop them completely)\nCon: can leave you with lots of NAs\nJoin the codebook to the data below using full_join()\nLook at the data. What’s going on here\n\n\nCodesoep_long |>\n  filter(!category == \"big5\") |> \n  full_join(\n    # your code here\n  ) |> \n    View()\n\n\nHere’s the solution:\n\nCodesoep_long |>\n  filter(!category == \"big5\") |>\n  full_join(codebook |> select(category, label, item_name, year, item_text)) \n\n\n\n  \n\n\n\n\nnote we have lots of missing data because the Big Five portions of the codebook were joined even though we removed that data"
  },
  {
    "objectID": "06-week6-workbook.html#inner_join",
    "href": "06-week6-workbook.html#inner_join",
    "title": "Week 6 - Review",
    "section": "4. inner_join()\n",
    "text": "4. inner_join()\n\n\nWe can also keep all rows present in both data frames\nPro: Won’t add rows with missing values in key variables\nCon: will drop observations that you want want for counts, correlations, etc.\nJoin the codebook to the data below using inner_join()\nLook at the data. What’s going on here\n\n\nCodesoep_long |>\n  filter(!category == \"big5\")\n  full_join(\n    # your code here\n  ) |> \n    View()\n\n\n\nNote that filtering, renaming/selecting, and joining is a common workflow\n\n\nCodesoep_long |>\n  filter(category == \"big5\") |>\n  select(Proc_SID, trait = label, item_name, year, p_value = value) |>\n  inner_join(\n    soep_long |>\n    filter(category == \"le\") |>\n    select(Proc_SID, le = label, year, le_value = value)\n  )"
  },
  {
    "objectID": "06-week6-workbook.html#left_join",
    "href": "06-week6-workbook.html#left_join",
    "title": "Week 6 - Review",
    "section": "5. left_join()\n",
    "text": "5. left_join()\n\n\nOr all rows present in the left (first) data frame, perhaps if it’s a subset of people with complete data\n\n\nCodesoep_long |>\n  filter(category == \"big5\") |>\n  select(Proc_SID, trait = label, item_name, year, p_value = value) |>\n  left_join(\n    soep_long |>\n    filter(category == \"le\") |>\n    select(Proc_SID, le = label, year, le_value = value)\n  )"
  },
  {
    "objectID": "06-week6-workbook.html#right_join",
    "href": "06-week6-workbook.html#right_join",
    "title": "Week 6 - Review",
    "section": "6. right_join()\n",
    "text": "6. right_join()\n\n\nOr all rows present in the right (second) data frame, such as I do when I join a codebook with raw data\n\n\nCodesoep_long |>\n  filter(category == \"big5\") |>\n  select(Proc_SID, trait = label, item_name, year, p_value = value) |>\n  right_join(\n    soep_long |>\n    filter(category == \"le\") |>\n    select(Proc_SID, le = label, year, le_value = value)\n  )"
  },
  {
    "objectID": "06-week6-workbook.html#your-turn",
    "href": "06-week6-workbook.html#your-turn",
    "title": "Week 6 - Review",
    "section": "Your Turn",
    "text": "Your Turn\nIn small groups, discuss what’s happening when you use full_join(), left_join(), right_join(), inner_join(), and anti_join() with the code below. Which is correct in this use case?\n\nCodesoep_long |>\n  filter(category == \"big5\") |>\n  select(Proc_SID, trait = label, item_name, year, p_value = value) |>\n  [x]_join(\n    soep_long |>\n    filter(category == \"le\") |>\n    select(Proc_SID, le = label, year, le_value = value)\n  )"
  },
  {
    "objectID": "06-week6-workbook.html#bringing-it-all-together-split-apply-combine",
    "href": "06-week6-workbook.html#bringing-it-all-together-split-apply-combine",
    "title": "Week 6 - Review",
    "section": "Bringing it all together: Split-Apply-Combine",
    "text": "Bringing it all together: Split-Apply-Combine\n\nMuch of the power of dplyr functions lay in the split-apply-combine method\n\nA given set of of data are:\n\n\nsplit into smaller chunks\nthen a function or series of functions are applied to each chunk\nand then the chunks are combined back together"
  },
  {
    "objectID": "06-week6-workbook.html#group_by",
    "href": "06-week6-workbook.html#group_by",
    "title": "Week 6 - Review",
    "section": "3. group_by()\n",
    "text": "3. group_by()\n\n\nThe group_by() function is the “split” of the method\nIt basically implicitly breaks the data set into chunks by whatever bare quoted column(s)/variable(s) are supplied as arguments."
  },
  {
    "objectID": "06-week6-workbook.html#mutate",
    "href": "06-week6-workbook.html#mutate",
    "title": "Week 6 - Review",
    "section": "4. mutate()\n",
    "text": "4. mutate()\n\n\n\nmutate() is one of your “apply” functions\nWhen you use mutate(), the resulting data frame will have the same number of rows you started with\nYou are directly mutating the existing data frame, either modifying existing columns or creating new ones"
  },
  {
    "objectID": "06-week6-workbook.html#summarize-summarise",
    "href": "06-week6-workbook.html#summarize-summarise",
    "title": "Week 6 - Review",
    "section": "5. summarize() / summarise()\n",
    "text": "5. summarize() / summarise()\n\n\n\nsummarize() is one of your “apply” functions\nThe resulting data frame will have the same number of rows as your grouping variable\nYou number of groups is 1 for ungrouped data frames"
  },
  {
    "objectID": "06-week6-workbook.html#question-1",
    "href": "06-week6-workbook.html#question-1",
    "title": "Week 6 - Review",
    "section": "Question 1:",
    "text": "Question 1:\n\nLet’s start with the Big Five data:\n\n\n\nfilter() out only Big Five rows\n\nmutate() each observation so that values less than one are changed to NA\n\nRemove any missing values using filter() or drop_na()\n\nGroup (split) the data so that you have a “group” for each person x trait x year combination\n\nsummarize() the values to get a composite score for each Big Five trait for each person in each year:\n\nSolution\nRemember when I said that long format data are easier to clean. Let’s do that now.\n\nCodesoep_big5 <- soep_long |>\n  filter(category == \"big5\") |>\n  mutate(value = mapvalues(value, seq(-8,0), rep(NA, 9))) |>\n  drop_na(value) |>\n  group_by(Proc_SID, label, year) |>\n  summarize(value = mean(value)) |>\n  ungroup()\nsoep_big5"
  },
  {
    "objectID": "06-week6-workbook.html#question-2",
    "href": "06-week6-workbook.html#question-2",
    "title": "Week 6 - Review",
    "section": "Question 2:",
    "text": "Question 2:\nNow let’s take care of the life event data:\n1. filter() out only life event rows\n2. mutate() each observation so that\n\n-2 = 0\n1 = 1\neverything else is NA\n\n\n\nRemove any missing values using filter() or drop_na()\n\nGroup (split) the data so that you have a “group” for each person x event combination\n\nsummarize() the values to get a sum score for each event for each person across all years:\n\nSolution\n\nCodesoep_le <- soep_long |>\n  filter(category == \"le\") |>\n  mutate(value = mapvalues(value, seq(-8,1), c(rep(NA, 6), 0, NA, NA, 1))) |>\n  drop_na(value) |>\n  group_by(Proc_SID, label) |>\n  summarize(value = sum(value)) |>\n  ungroup()\nsoep_le"
  },
  {
    "objectID": "06-week6-workbook.html#question-3",
    "href": "06-week6-workbook.html#question-3",
    "title": "Week 6 - Review",
    "section": "Question 3:",
    "text": "Question 3:\nJust for practice, now make your Big Five data frame wide, leaving the time variable (year) long\nSolution\n\nCodesoep_big5 |>\n  pivot_wider(\n    names_from = \"label\"\n    , values_from = \"value\"\n  )"
  },
  {
    "objectID": "06-week6-workbook.html#question-4",
    "href": "06-week6-workbook.html#question-4",
    "title": "Week 6 - Review",
    "section": "Question 4:",
    "text": "Question 4:\n\nNow, let’s join the data frames back together.\nWhich join function do you think is most appropriate?\nHint: You will need to rename the label and value columns to reflect the category of the data\n\nSolution\n\nCodesoep_clean <- soep_big5 |>\n  rename(trait = label, p_value = value) |>\n  inner_join(\n    soep_le |>\n      rename(le = label, le_value = value)\n  )\nsoep_clean"
  },
  {
    "objectID": "06-week6-workbook.html#functions-1",
    "href": "06-week6-workbook.html#functions-1",
    "title": "Week 6 - Review",
    "section": "Functions",
    "text": "Functions\nHow to approach writing functions? (broad recipe)\n\nExperiment with performing the task outside of a function\n\nExperiment with performing task with different sets of inputs\nOften, you must revise this code, when an approach that worked outside a function does not work within a function\n\n\nWrite the function\nTest the function\n\nTry to “break” it\n\n\n\nContinual improvement. As you use the function, make continual improvements going back-and-forth between steps 1-3"
  },
  {
    "objectID": "06-week6-workbook.html#basics-of-writing-functions",
    "href": "06-week6-workbook.html#basics-of-writing-functions",
    "title": "Week 6 - Review",
    "section": "Basics of writing functions",
    "text": "Basics of writing functions\nThree components of a function:\n\n\nFunction name\n\nDefine a function using function() and give it a name using the assignment operator <-\n\n\n\n\nFunction arguments (sometimes called “inputs”)\n\nInputs that the function takes; they go inside the parentheses of function()\n\nCan be vectors, data frames, logical statements, strings, etc.\n\n\nIn the above hypothetical code, the function took three inputs arg1, arg2, arg3, but we could have written:\n\n\nfunction(x, y, z) or function(Larry, Curly, Moe)\n\n\n\nIn the “function call,” you specify values to assign to these function arguments\n\n\n\nFunction body\n\nWhat the function does to the inputs\nFunction body goes inside the pair of curly brackets ({}) that follows function()\n\nAbove hypothetical function doesn’t do anything, but your function can return a value (covered in later section)"
  },
  {
    "objectID": "06-week6-workbook.html#num_negative-function",
    "href": "06-week6-workbook.html#num_negative-function",
    "title": "Week 6 - Review",
    "section": "\nnum_negative() function",
    "text": "num_negative() function\nTask: Write function called num_negative()\n\nWrite a function that counts the number of observations with negative values for a specific variable\nApply this function to variables from dataframe df (created below)\nAdapted from Ben Skinner’s Programming 1 R Workshop HERE\n\n\n\nCode# Sample dataframe `df` that contains some negative values\ndf"
  },
  {
    "objectID": "06-week6-workbook.html#steps",
    "href": "06-week6-workbook.html#steps",
    "title": "Week 6 - Review",
    "section": "Steps:",
    "text": "Steps:\nRecommended steps:\n\nPerform task outside of function\n\nHINT: sum(data_frame_name$var_name<0)\n\n\n\nWrite function\nApply/test function on variables\n\nStep 1: Perform task outside of function\n\nCodenames(df) # identify variable names\n\n[1] \"id\"     \"age\"    \"sibage\" \"parage\"\n\nCodedf$age # print observations for a variable\n\n  [1]  17  15 -97  13 -97  12 -99 -97  16  16 -98  20 -99  20  11  20  12  17\n [19]  19  17 -97 -99  12  13  11  15  20  14 -99  11  20 -98  11 -98  12  16\n [37]  12  18  12  19  12 -97  20  17  11  19  19  12 -98  11  15  18  15 -98\n [55]  15  19 -97  13 -98  16  13  12  16  19 -99  19 -98  13 -97  20  15  19\n [73]  15  12  18 -99  18 -98 -98 -98 -97  12  14  19 -97  11  20  18  14 -99\n [91]  15  20 -97  14  14  19  18  17  20  15\n\nCode#BaseR\nsum(df$age<0) # count number of obs w/ negative values for variable \"age\"\n\n[1] 27\n\n\nStep 2: Write function\n\nCodenum_missing <- function(x){\n  sum(x<0)\n}\n\n\nStep 3: Apply function\n\nCodenum_missing(df$age)\n\n[1] 27\n\nCodenum_missing(df$sibage)\n\n[1] 22"
  },
  {
    "objectID": "06-week6-workbook.html#num_missing-function",
    "href": "06-week6-workbook.html#num_missing-function",
    "title": "Week 6 - Review",
    "section": "\nnum_missing() function",
    "text": "num_missing() function\nTask: Write function called num_negative()\n\nWrite a function that counts number of missing observations for a variable and allows you to specify which values are associated with missing for that variable. This function will take two arguments:\n\n\nx: The variable (e.g., df$sibage)\n\nmiss_vals: Vector of values you want to associate with “missing” variable\n\nValues to associate with missing for df$age: -97,-98,-99\n\nValues to associate with missing for df$sibage: -97,-98,-99\n\nValues to associate with missing for df$parage: -4,-7,-8"
  },
  {
    "objectID": "06-week6-workbook.html#steps-1",
    "href": "06-week6-workbook.html#steps-1",
    "title": "Week 6 - Review",
    "section": "Steps",
    "text": "Steps\nRecommended steps:\n\nPerform task outside of function\n\nHINT: sum(data_frame_name$var_name %in% c(-4,-5))\n\n\n\nWrite function\nApply/test function on variables\n\nStep 1: Perform task outside of function\n\nCodesum(df$age %in% c(-97,-98,-99))\n\n[1] 27\n\n\nStep 2: Write function\n\nCodenum_missing <- function(x, miss_vals){\n\n  sum(x %in% miss_vals)\n}\n\n\nStep 3: Apply function\n\nCodenum_missing(df$age,c(-97,-98,-99))\n\n[1] 27\n\nCodenum_missing(df$sibage,c(-97,-98,-99))\n\n[1] 22\n\nCodenum_missing(df$parage,c(-4,-7,-8))\n\n[1] 17"
  },
  {
    "objectID": "06-week6-workbook.html#purrrmap",
    "href": "06-week6-workbook.html#purrrmap",
    "title": "Week 6 - Review",
    "section": "purrr::map()\n",
    "text": "purrr::map()\n\n\n\nmap() functions are the tidyverse alternative to for loops and chaotic lists with deep nesting structures\n\nmap() functions, unlike _apply() functions can take any number of inputs, which mimics nested for loops\n\nmap() functions can return any output type, including heterogeneous outputs (at least if you return it as a list)\n\nInputs\n\nYou control how many inputs using the following:\n\n\nmap(): one input, arguments are map(.x, .f)\n\n\nmap2(): two inputs, arguments are map2(.x, y., .f)\n\n\npmap(): any number of inputs, arguments are pmap(.l, .f)\n\nNote the .l becuase this means we have to wrap inputs in a list()\n\n\n\n\n\nOuputs\n\nYou can also control the output of purrr::map():\n\n\nmap(): outputs a list\n\nmap_chr(): outputs a character vector\n\n\nmap_dbl(): outputs a numeric vector\n\n\nmap_lgl(): outputs a logical vector\n\n\nmap_int(): outputs a integer vector\n\n\nmap_vec(): outputs essentially any type of vector\n\n\nNote that if one input combination fails, all will fail and nothing will be outputted\nError handling\n\nHaving everything fail because one thing went wrong is really frustrating\nThere are a number of functions in purrr to help with that:\n\n\npossibly(.f, otherwise): returns whatever you ask it return with otherwise when a .f call fails\n\nsafely(.f): returns a list with the output, if successful, and errors, if unsuccessful\nOthers: see documentation.\n\n\nList columns\n\nOne of the easiest ways to work with purrr is using list columns in nested data frames\n\n\n\n\nYou can create a nested data frame using tidyr::nest() or tibble() (where one column is a list itself)\n\n\nCodesoep_clean |>\n  group_by(trait, year, le) |>\n  nest() |>\n  ungroup()\n\n\n\n  \n\n\n\n\n\nYou can then call map() within a mutate call to modify the list column or create new columns in your data frame\n\n\nCodetibble(\n  x = c(1,2,3)\n  , y = list(letters[1:5], letters[6:10], letters[11:15])\n)"
  },
  {
    "objectID": "06-week6-workbook.html#question-1-1",
    "href": "06-week6-workbook.html#question-1-1",
    "title": "Week 6 - Review",
    "section": "Question 1:",
    "text": "Question 1:\n\nCreate a data frame called soep_nested that creates a list column of the data split by trait and life event.\n\nSolution\n\nCodesoep_nested <- soep_clean |>\n  group_by(trait, le) |>\n  nest() |>\n  ungroup()"
  },
  {
    "objectID": "06-week6-workbook.html#question-2-1",
    "href": "06-week6-workbook.html#question-2-1",
    "title": "Week 6 - Review",
    "section": "Question 2:",
    "text": "Question 2:\n\nUsing mutate(), create a new list column called model that runs the following function\n\n\nCodelmer_fun <- function(d){\n  d <- d |> \n    mutate(wave = year - 2005) |>\n    group_by(Proc_SID) |>\n    filter(n() > 1)\n  m <- lmer(p_value ~ wave + le_value + le_value:wave + (1 + wave | Proc_SID), data = d)\n  return(m)\n}\n\n\nSolution\n\nCodesoep_nested <- soep_nested |>\n  mutate(model = map(data, lmer_fun))\nsoep_nested"
  },
  {
    "objectID": "06-week6-workbook.html#question-3-1",
    "href": "06-week6-workbook.html#question-3-1",
    "title": "Week 6 - Review",
    "section": "Question 3:",
    "text": "Question 3:\n\nUse the following function to extract the number of people we estimated slopes for in this model. Output the result as an integer to a new column called npeople\n\n\n\nCodenslopes_fun <- function(m) summary(m)$ngrps\n\n\nSolution\n\nCodesoep_nested <- soep_nested |>\n  mutate(npeople = map_int(model, nslopes_fun))\nsoep_nested"
  },
  {
    "objectID": "06-week6-workbook.html#question-4-1",
    "href": "06-week6-workbook.html#question-4-1",
    "title": "Week 6 - Review",
    "section": "Question 4:",
    "text": "Question 4:\n\nUse the tidy() function from the broom.mixed package to extract the coefficients from the model and their confidence intervals. Save it to the column “tidy”\nHints:\n\nUse the argument conf.int = T to get the confidence intervals\nAdditional arguments to the .f function called in map() can be just included as addition arguments (e.g., map(.x, .f, conf.int = T))\n\n\n\nSolution\n\nCodesoep_nested <- soep_nested |>\n  mutate(tidy = map(model, broom.mixed::tidy, conf.int = T))"
  },
  {
    "objectID": "06-week6-workbook.html#question-5",
    "href": "06-week6-workbook.html#question-5",
    "title": "Week 6 - Review",
    "section": "Question 5:",
    "text": "Question 5:\n\nLet’s practice making a super simple table. Do the following:\n\n\nremove the data and model columns from the data frame\n\nunnest() the tidy column\nKeep only fixed effects (effect == \"fixed\")\nWe only care about the interaction, so only keep the interaction term\n\nround the estimate, conf.low, and conf.high columns to 2 decimal places\nKeep the trait, le, estimate, conf.low, and conf.high columns only\n\npivot_wider() by trait for estimate, conf.low, and conf.high\n\n\nSolution\n\nCodesoep_tab <- soep_nested |>\n  select(-data, -model) |>\n  unnest(tidy) |>\n  filter(effect == \"fixed\" & grepl(\":\", term)) |>\n  mutate(across(c(estimate, conf.low, conf.high), \\(x) round(x, 2))) |>\n  select(trait, le, estimate, conf.low, conf.high) |>\n  pivot_wider(\n    names_from = \"trait\"\n    , names_glue = \"{trait}_{.value}\"\n    , values_from = c(estimate, conf.low, conf.high)\n  )\nsoep_tab"
  },
  {
    "objectID": "06-week6-workbook.html#question-6",
    "href": "06-week6-workbook.html#question-6",
    "title": "Week 6 - Review",
    "section": "Question 6:",
    "text": "Question 6:\nUse the function below to get model predictions\n\nCodepred_fun <- function(m){\n  d <- m@frame |>\n      select(-p_value) |>\n      distinct()\n  bind_cols(d, pred = predict(m, newdata = d))\n}\n\n\nSolution\n\nCodesoep_nested <- soep_nested |>\n  mutate(pred = map(model, pred_fun))\nsoep_nested"
  },
  {
    "objectID": "06-week6-workbook.html#question-7",
    "href": "06-week6-workbook.html#question-7",
    "title": "Week 6 - Review",
    "section": "Question 7:",
    "text": "Question 7:\n\nLet’s practice making a super simple table. Do the following:\n\n\nremove the data, model, and tidy columns from the data frame\n\nunnest() the tidy column\n\ngroup_by() life event and nest() + ungroup()\n\nsave this as soep_pred\n\n\nSolution\n\nCodesoep_pred <- soep_nested |>\n  select(-data, -model, -tidy) |>\n  unnest(pred) |>\n  group_by(trait) |>\n  nest() |>\n  ungroup()\nsoep_pred"
  },
  {
    "objectID": "06-week6-workbook.html#question-8",
    "href": "06-week6-workbook.html#question-8",
    "title": "Week 6 - Review",
    "section": "Question 8:",
    "text": "Question 8:\n\nUse the following function to create a new column p that contains spaghetti plots\nNote that the function takes two inputs!\n\n\nCodespag_plot_fun <- function(d, trait){\n  set.seed(6)\n  d |>\n    group_by(le) |>\n    nest() |>\n    mutate(data = map(data, ~filter(., Proc_SID %in% sample(unique(.$Proc_SID), 100)))) |>\n    unnest(data) |>\n    ungroup() |>\n    mutate(le_value = ifelse(le_value > 1, 1, le_value)) |>\n    ggplot(aes(x = wave, y = pred)) + \n      geom_line(aes(group = Proc_SID, color = factor(le_value)), alpha = .3) + \n      geom_smooth(method = \"lm\", se = F, color = \"darkblue\") + \n      scale_color_manual(values = c(\"grey\", \"blue\"), labels = c(\"No Event\", \"Event\")) + \n      labs(x = \"Wave\", y = \"Predicted Trait Levels\", color = \"Life Event\", title = trait) + \n      facet_wrap(~le) + \n      theme_classic() + \n      theme(legend.position = c(.7, .1))\n}\n\n\nSolution\n\nCodesoep_pred <- soep_pred |>\n  mutate(p = map2(data, trait, spag_plot_fun))\n\nsoep_pred$p[[1]]"
  },
  {
    "objectID": "06-week6-workbook.html#wrap-up-1",
    "href": "06-week6-workbook.html#wrap-up-1",
    "title": "Week 6 - Review",
    "section": "Wrap-Up",
    "text": "Wrap-Up\n\nToday’s goal was to review the coding concepts we’ve used so far and ask you to apply them using a series of guided examples\nThe biggest takeaway I wanted you to have is chaining, or how you can use tidyverse functions in chains to accomplish a bunch of goals simultaneously\nWe cleaned, composited, and ran 50 models across thousands of people, including predictions and tables in less than 100 lines of code. Just doing the models, tidy(), and predict() parts of that alone would have been 150 lines of code and introduced huge opportunities for errors!"
  },
  {
    "objectID": "06-week6-workbook.html#full-code",
    "href": "06-week6-workbook.html#full-code",
    "title": "Week 6 - Review",
    "section": "Full Code",
    "text": "Full Code\nData\n\nCode# load the codebook\n(codebook <- read_csv(\"week6-codebook.csv\") |>\n    mutate(old_name = str_to_lower(old_name)))\n\nold.names <- codebook$old_name # get old column names\nnew.names <- codebook$new_name # get new column names\nsoep <- read_csv(\"week6-data.csv\") |>\n    select(all_of(old.names))\n\n\nPivot Long\n\nCodesoep_long <- soep |>\n  setNames(new.names) |>\n  pivot_longer(\n    cols = c(-starts_with(\"Proc\"), -starts_with(\"Dem\"))\n    , names_to = c(\"category\",  \"label\",    \"item_name\",    \"year\")\n    , names_pattern = \"(.*)_(.*)_(.*)_(.*)\"\n    , values_to = \"value\"\n    , values_drop_na = T\n  ) |> mutate(year = as.numeric(year))\nsoep_long\n\n\nRecode and Composite\n\nCodesoep_big5 <- soep_long |>\n  filter(category == \"big5\") |>\n  mutate(value = mapvalues(value, seq(-8,0), rep(NA, 9))) |>\n  drop_na(value) |>\n  group_by(Proc_SID, label, year) |>\n  summarize(value = mean(value)) |>\n  ungroup()\n\nsoep_le <- soep_long |>\n  filter(category == \"le\") |>\n  mutate(value = mapvalues(value, seq(-8,1), c(rep(NA, 6), 0, NA, NA, 1))) |>\n  drop_na(value) |>\n  group_by(Proc_SID, label) |>\n  summarize(value = sum(value)) |>\n  ungroup()\n\nsoep_clean <- soep_big5 |>\n  rename(trait = label, p_value = value) |>\n  inner_join(\n    soep_le |>\n      rename(le = label, le_value = value)\n  )\n\n\nModels\n\nCodesoep_nested <- soep_clean |>\n  group_by(trait, le) |>\n  nest() |>\n  ungroup()\n\nlmer_fun <- function(d){\n  d <- d |> \n    mutate(wave = year - 2005) |>\n    group_by(Proc_SID) |>\n    filter(n() > 1)\n  m <- lmer(p_value ~ wave + le_value + le_value:wave + (1 + wave | Proc_SID), data = d)\n  return(m)\n}\n\nsoep_nested <- soep_nested |>\n  mutate(model = map(data, lmer_fun))\nsoep_nested\n\n\nResults\nTables\n\nCodenslopes_fun <- function(m) summary(m)$ngrps\n\nsoep_nested <- soep_nested |>\n  mutate(npeople = map_int(model, nslopes_fun))\nsoep_nested\n\nsoep_nested <- soep_nested |>\n  mutate(tidy = map(model, broom.mixed::tidy, conf.int = T))\n\nsoep_tab <- soep_nested |>\n  select(-data, -model) |>\n  unnest(tidy) |>\n  filter(effect == \"fixed\" & grepl(\":\", term)) |>\n  mutate(across(c(estimate, conf.low, conf.high), \\(x) round(x, 2))) |>\n  select(trait, le, estimate, conf.low, conf.high) |>\n  pivot_wider(\n    names_from = \"trait\"\n    , names_glue = \"{trait}_{.value}\"\n    , values_from = c(estimate, conf.low, conf.high)\n  )\nsoep_tab\n\n\nModel Predictions\n\nCodepred_fun <- function(m){\n  d <- m@frame |>\n      select(-p_value) |>\n      distinct()\n  bind_cols(d, pred = predict(m, newdata = d))\n}\n\nsoep_nested <- soep_nested |>\n  mutate(pred = map(model, pred_fun))\nsoep_nested\n\nsoep_pred <- soep_nested |>\n  select(-data, -model, -tidy) |>\n  unnest(pred) |>\n  group_by(trait) |>\n  nest() |>\n  ungroup()\nsoep_pred\n\nspag_plot_fun <- function(d, trait){\n  set.seed(6)\n  d |>\n    group_by(le) |>\n    nest() |>\n    mutate(data = map(data, ~filter(., Proc_SID %in% sample(unique(.$Proc_SID), 100)))) |>\n    unnest(data) |>\n    ungroup() |>\n    mutate(le_value = ifelse(le_value > 1, 1, le_value)) |>\n    ggplot(aes(x = wave, y = pred)) + \n      geom_line(aes(group = Proc_SID, color = factor(le_value)), alpha = .3) + \n      geom_smooth(method = \"lm\", se = F, color = \"darkblue\") + \n      scale_color_manual(values = c(\"grey\", \"blue\"), labels = c(\"No Event\", \"Event\")) + \n      labs(x = \"Wave\", y = \"Predicted Trait Levels\", color = \"Life Event\", title = trait) + \n      facet_wrap(~le) + \n      theme_classic() + \n      theme(legend.position = c(.7, .1))\n}\n\nsoep_pred <- soep_pred |>\n  mutate(p = map2(data, trait, spag_plot_fun))\n\nsoep_pred$p[[1]]"
  }
]